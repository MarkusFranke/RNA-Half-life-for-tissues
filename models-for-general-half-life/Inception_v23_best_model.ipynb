{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inception v23 - best model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarkusFranke/RNA-Half-life-for-tissues/blob/main/models-for-general-half-life/Inception_v23_best_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKkQxNHexGtb"
      },
      "outputs": [],
      "source": [
        "from Bio import SeqIO #for parsing Fasta Files\n",
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "from kipoiseq.transforms.functional import one_hot, fixed_len\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from sklearn.metrics import explained_variance_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import backend as k\n",
        "from keras.callbacks import EarlyStopping, History\n",
        "from keras.models import Model\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow as tf\n",
        "import keras.layers as kl\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import subprocess\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we can't run this in google colab due to RAM limitations, I'm importing the data from disk. All the data should be available on google drive though, under the same filenames, either to be downloaded and run with a path to where the user saved them, or to be directly accessible by mounting the google drive (and setting a shortcut to our google drive data path in google drive)\n"
      ],
      "metadata": {
        "id": "ku26rqE7x06J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hl = pd.read_excel(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\kelley_et_al_corrected_hl.xlsx', skiprows=[0, 1])\n",
        "hl['zscore'] = zscore(hl['half-life (PC1)'])\n",
        "halflife = hl[[\"Ensembl Gene Id\", \"zscore\"]]\n",
        "hl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "7vRfq7bIxy4J",
        "outputId": "a68f04ff-1f56-4479-bef7-ca4be3329f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Ensembl Gene Id  Gene name  half-life (PC1)  Bazzini_ActD_HEK293_1  \\\n",
              "0      ENSG00000000003     TSPAN6         8.660955               0.763166   \n",
              "1      ENSG00000000419       DPM1         2.241221               0.529938   \n",
              "2      ENSG00000000457      SCYL3        -6.929173              -0.798471   \n",
              "3      ENSG00000000460   C1orf112         0.440909               0.461228   \n",
              "4      ENSG00000000938        FGR        -0.943680               0.164310   \n",
              "...                ...        ...              ...                    ...   \n",
              "13916  ENSG00000284770       TBCE         2.218664               0.100281   \n",
              "13917  ENSG00000285077  ARHGAP11B        -3.262964              -0.733980   \n",
              "13918  ENSG00000288596    C8orf44         2.118850              -0.485957   \n",
              "13919  ENSG00000288701     PRRC2B         0.133147               0.133770   \n",
              "13920  ENSG00000288722       F8A1        -3.485607              -0.607463   \n",
              "\n",
              "       Bazzini_ActD_HeLa_1  Bazzini_ActD_RPE_1  Bazzini_4sU_K562_1  \\\n",
              "0                 0.258448            0.106486            1.019072   \n",
              "1                 0.222678           -0.040666           -0.284952   \n",
              "2                -0.894854           -1.039150           -1.444532   \n",
              "3                 0.195794           -0.739672           -0.123925   \n",
              "4                 0.112064            0.095773            0.045345   \n",
              "...                    ...                 ...                 ...   \n",
              "13916            -0.187624           -0.143422            0.201024   \n",
              "13917            -0.934478           -0.952570           -0.461339   \n",
              "13918            -0.320560           -0.332189            0.582473   \n",
              "13919             0.214899            0.144491            0.090991   \n",
              "13920            -0.419258           -0.378650           -0.775774   \n",
              "\n",
              "       Akimitsu_BrU_HeLa_1  Rinn_ActD_K562_1  Rinn_ActD_K562_2  ...  \\\n",
              "0                 2.022504          1.744745          1.783356  ...   \n",
              "1                 0.145097          0.866919          0.832768  ...   \n",
              "2                -1.287191         -1.006317         -1.062201  ...   \n",
              "3                 0.162538         -0.023056         -0.008479  ...   \n",
              "4                 0.024136         -0.209157         -0.223700  ...   \n",
              "...                    ...               ...               ...  ...   \n",
              "13916             0.933352          0.465403          0.451758  ...   \n",
              "13917            -0.940048          0.225157          0.161858  ...   \n",
              "13918            -0.614775          0.305297          0.336730  ...   \n",
              "13919             0.131956         -0.040831         -0.035576  ...   \n",
              "13920             0.400149         -1.060379         -1.033201  ...   \n",
              "\n",
              "       Gejman_4sU_GM12812_1  Gejman_4sU_GM12814_1  Gejman_4sU_GM12815_1  \\\n",
              "0                  1.037361              0.969166              1.209562   \n",
              "1                 -0.212424             -0.747989              0.371214   \n",
              "2                 -1.155096             -1.421651             -1.568912   \n",
              "3                  1.365208              1.017193             -0.239569   \n",
              "4                  0.722198              1.024313              1.484018   \n",
              "...                     ...                   ...                   ...   \n",
              "13916              1.082013              0.917651              0.897480   \n",
              "13917              0.256969              0.033071             -0.249782   \n",
              "13918              1.023526             -0.155662             -0.950892   \n",
              "13919             -0.091563             -0.021327             -0.017414   \n",
              "13920              0.433892              0.446224              1.344124   \n",
              "\n",
              "       Simon_4sU_K562_1  Simon_4sU_K562_2  Rissland_4sU_HEK293_1  \\\n",
              "0              2.080643          2.095154               0.674058   \n",
              "1              0.772595          0.712843              -0.350914   \n",
              "2             -1.308978         -1.311572              -0.387172   \n",
              "3              0.373929          0.380154              -0.063720   \n",
              "4             -0.375671         -0.384504               0.232924   \n",
              "...                 ...               ...                    ...   \n",
              "13916          0.866792          0.868080              -0.104068   \n",
              "13917         -1.116405         -1.078302              -1.495412   \n",
              "13918         -0.175193         -0.166760               2.505294   \n",
              "13919          0.088581          0.107441               0.345123   \n",
              "13920         -0.631782         -0.629741              -0.413725   \n",
              "\n",
              "       Rissland_4sU_HEK293_2  Rissland_4sU_HEK293_3  Rissland_4sU_HEK293_4  \\\n",
              "0                   1.120375               1.456258               1.791769   \n",
              "1                  -0.879247              -0.825603               0.109861   \n",
              "2                  -1.229226              -1.122749              -0.570002   \n",
              "3                  -0.450610              -0.805719               0.453957   \n",
              "4                   0.347933               0.266619               0.063565   \n",
              "...                      ...                    ...                    ...   \n",
              "13916              -0.169414              -0.404886              -0.165188   \n",
              "13917              -1.006317              -1.240272              -1.440629   \n",
              "13918               1.068162               1.307039               1.563308   \n",
              "13919              -0.188176              -0.119128               0.001050   \n",
              "13920              -0.071227              -0.105036              -0.197549   \n",
              "\n",
              "         zscore  \n",
              "0      1.807620  \n",
              "1      0.467763  \n",
              "2     -1.446182  \n",
              "3      0.092022  \n",
              "4     -0.196955  \n",
              "...         ...  \n",
              "13916  0.463055  \n",
              "13917 -0.681011  \n",
              "13918  0.442223  \n",
              "13919  0.027789  \n",
              "13920 -0.727478  \n",
              "\n",
              "[13921 rows x 58 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ensembl Gene Id</th>\n",
              "      <th>Gene name</th>\n",
              "      <th>half-life (PC1)</th>\n",
              "      <th>Bazzini_ActD_HEK293_1</th>\n",
              "      <th>Bazzini_ActD_HeLa_1</th>\n",
              "      <th>Bazzini_ActD_RPE_1</th>\n",
              "      <th>Bazzini_4sU_K562_1</th>\n",
              "      <th>Akimitsu_BrU_HeLa_1</th>\n",
              "      <th>Rinn_ActD_K562_1</th>\n",
              "      <th>Rinn_ActD_K562_2</th>\n",
              "      <th>...</th>\n",
              "      <th>Gejman_4sU_GM12812_1</th>\n",
              "      <th>Gejman_4sU_GM12814_1</th>\n",
              "      <th>Gejman_4sU_GM12815_1</th>\n",
              "      <th>Simon_4sU_K562_1</th>\n",
              "      <th>Simon_4sU_K562_2</th>\n",
              "      <th>Rissland_4sU_HEK293_1</th>\n",
              "      <th>Rissland_4sU_HEK293_2</th>\n",
              "      <th>Rissland_4sU_HEK293_3</th>\n",
              "      <th>Rissland_4sU_HEK293_4</th>\n",
              "      <th>zscore</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ENSG00000000003</td>\n",
              "      <td>TSPAN6</td>\n",
              "      <td>8.660955</td>\n",
              "      <td>0.763166</td>\n",
              "      <td>0.258448</td>\n",
              "      <td>0.106486</td>\n",
              "      <td>1.019072</td>\n",
              "      <td>2.022504</td>\n",
              "      <td>1.744745</td>\n",
              "      <td>1.783356</td>\n",
              "      <td>...</td>\n",
              "      <td>1.037361</td>\n",
              "      <td>0.969166</td>\n",
              "      <td>1.209562</td>\n",
              "      <td>2.080643</td>\n",
              "      <td>2.095154</td>\n",
              "      <td>0.674058</td>\n",
              "      <td>1.120375</td>\n",
              "      <td>1.456258</td>\n",
              "      <td>1.791769</td>\n",
              "      <td>1.807620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENSG00000000419</td>\n",
              "      <td>DPM1</td>\n",
              "      <td>2.241221</td>\n",
              "      <td>0.529938</td>\n",
              "      <td>0.222678</td>\n",
              "      <td>-0.040666</td>\n",
              "      <td>-0.284952</td>\n",
              "      <td>0.145097</td>\n",
              "      <td>0.866919</td>\n",
              "      <td>0.832768</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.212424</td>\n",
              "      <td>-0.747989</td>\n",
              "      <td>0.371214</td>\n",
              "      <td>0.772595</td>\n",
              "      <td>0.712843</td>\n",
              "      <td>-0.350914</td>\n",
              "      <td>-0.879247</td>\n",
              "      <td>-0.825603</td>\n",
              "      <td>0.109861</td>\n",
              "      <td>0.467763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENSG00000000457</td>\n",
              "      <td>SCYL3</td>\n",
              "      <td>-6.929173</td>\n",
              "      <td>-0.798471</td>\n",
              "      <td>-0.894854</td>\n",
              "      <td>-1.039150</td>\n",
              "      <td>-1.444532</td>\n",
              "      <td>-1.287191</td>\n",
              "      <td>-1.006317</td>\n",
              "      <td>-1.062201</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.155096</td>\n",
              "      <td>-1.421651</td>\n",
              "      <td>-1.568912</td>\n",
              "      <td>-1.308978</td>\n",
              "      <td>-1.311572</td>\n",
              "      <td>-0.387172</td>\n",
              "      <td>-1.229226</td>\n",
              "      <td>-1.122749</td>\n",
              "      <td>-0.570002</td>\n",
              "      <td>-1.446182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ENSG00000000460</td>\n",
              "      <td>C1orf112</td>\n",
              "      <td>0.440909</td>\n",
              "      <td>0.461228</td>\n",
              "      <td>0.195794</td>\n",
              "      <td>-0.739672</td>\n",
              "      <td>-0.123925</td>\n",
              "      <td>0.162538</td>\n",
              "      <td>-0.023056</td>\n",
              "      <td>-0.008479</td>\n",
              "      <td>...</td>\n",
              "      <td>1.365208</td>\n",
              "      <td>1.017193</td>\n",
              "      <td>-0.239569</td>\n",
              "      <td>0.373929</td>\n",
              "      <td>0.380154</td>\n",
              "      <td>-0.063720</td>\n",
              "      <td>-0.450610</td>\n",
              "      <td>-0.805719</td>\n",
              "      <td>0.453957</td>\n",
              "      <td>0.092022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENSG00000000938</td>\n",
              "      <td>FGR</td>\n",
              "      <td>-0.943680</td>\n",
              "      <td>0.164310</td>\n",
              "      <td>0.112064</td>\n",
              "      <td>0.095773</td>\n",
              "      <td>0.045345</td>\n",
              "      <td>0.024136</td>\n",
              "      <td>-0.209157</td>\n",
              "      <td>-0.223700</td>\n",
              "      <td>...</td>\n",
              "      <td>0.722198</td>\n",
              "      <td>1.024313</td>\n",
              "      <td>1.484018</td>\n",
              "      <td>-0.375671</td>\n",
              "      <td>-0.384504</td>\n",
              "      <td>0.232924</td>\n",
              "      <td>0.347933</td>\n",
              "      <td>0.266619</td>\n",
              "      <td>0.063565</td>\n",
              "      <td>-0.196955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13916</th>\n",
              "      <td>ENSG00000284770</td>\n",
              "      <td>TBCE</td>\n",
              "      <td>2.218664</td>\n",
              "      <td>0.100281</td>\n",
              "      <td>-0.187624</td>\n",
              "      <td>-0.143422</td>\n",
              "      <td>0.201024</td>\n",
              "      <td>0.933352</td>\n",
              "      <td>0.465403</td>\n",
              "      <td>0.451758</td>\n",
              "      <td>...</td>\n",
              "      <td>1.082013</td>\n",
              "      <td>0.917651</td>\n",
              "      <td>0.897480</td>\n",
              "      <td>0.866792</td>\n",
              "      <td>0.868080</td>\n",
              "      <td>-0.104068</td>\n",
              "      <td>-0.169414</td>\n",
              "      <td>-0.404886</td>\n",
              "      <td>-0.165188</td>\n",
              "      <td>0.463055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13917</th>\n",
              "      <td>ENSG00000285077</td>\n",
              "      <td>ARHGAP11B</td>\n",
              "      <td>-3.262964</td>\n",
              "      <td>-0.733980</td>\n",
              "      <td>-0.934478</td>\n",
              "      <td>-0.952570</td>\n",
              "      <td>-0.461339</td>\n",
              "      <td>-0.940048</td>\n",
              "      <td>0.225157</td>\n",
              "      <td>0.161858</td>\n",
              "      <td>...</td>\n",
              "      <td>0.256969</td>\n",
              "      <td>0.033071</td>\n",
              "      <td>-0.249782</td>\n",
              "      <td>-1.116405</td>\n",
              "      <td>-1.078302</td>\n",
              "      <td>-1.495412</td>\n",
              "      <td>-1.006317</td>\n",
              "      <td>-1.240272</td>\n",
              "      <td>-1.440629</td>\n",
              "      <td>-0.681011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13918</th>\n",
              "      <td>ENSG00000288596</td>\n",
              "      <td>C8orf44</td>\n",
              "      <td>2.118850</td>\n",
              "      <td>-0.485957</td>\n",
              "      <td>-0.320560</td>\n",
              "      <td>-0.332189</td>\n",
              "      <td>0.582473</td>\n",
              "      <td>-0.614775</td>\n",
              "      <td>0.305297</td>\n",
              "      <td>0.336730</td>\n",
              "      <td>...</td>\n",
              "      <td>1.023526</td>\n",
              "      <td>-0.155662</td>\n",
              "      <td>-0.950892</td>\n",
              "      <td>-0.175193</td>\n",
              "      <td>-0.166760</td>\n",
              "      <td>2.505294</td>\n",
              "      <td>1.068162</td>\n",
              "      <td>1.307039</td>\n",
              "      <td>1.563308</td>\n",
              "      <td>0.442223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13919</th>\n",
              "      <td>ENSG00000288701</td>\n",
              "      <td>PRRC2B</td>\n",
              "      <td>0.133147</td>\n",
              "      <td>0.133770</td>\n",
              "      <td>0.214899</td>\n",
              "      <td>0.144491</td>\n",
              "      <td>0.090991</td>\n",
              "      <td>0.131956</td>\n",
              "      <td>-0.040831</td>\n",
              "      <td>-0.035576</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.091563</td>\n",
              "      <td>-0.021327</td>\n",
              "      <td>-0.017414</td>\n",
              "      <td>0.088581</td>\n",
              "      <td>0.107441</td>\n",
              "      <td>0.345123</td>\n",
              "      <td>-0.188176</td>\n",
              "      <td>-0.119128</td>\n",
              "      <td>0.001050</td>\n",
              "      <td>0.027789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13920</th>\n",
              "      <td>ENSG00000288722</td>\n",
              "      <td>F8A1</td>\n",
              "      <td>-3.485607</td>\n",
              "      <td>-0.607463</td>\n",
              "      <td>-0.419258</td>\n",
              "      <td>-0.378650</td>\n",
              "      <td>-0.775774</td>\n",
              "      <td>0.400149</td>\n",
              "      <td>-1.060379</td>\n",
              "      <td>-1.033201</td>\n",
              "      <td>...</td>\n",
              "      <td>0.433892</td>\n",
              "      <td>0.446224</td>\n",
              "      <td>1.344124</td>\n",
              "      <td>-0.631782</td>\n",
              "      <td>-0.629741</td>\n",
              "      <td>-0.413725</td>\n",
              "      <td>-0.071227</td>\n",
              "      <td>-0.105036</td>\n",
              "      <td>-0.197549</td>\n",
              "      <td>-0.727478</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13921 rows × 58 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\Homo_sapiens.GRCh38.83.chosenTranscript.3pUTRs.fa') as fasta_file:  # Will close handle cleanly\n",
        "    UTR3_identifiers = []\n",
        "    UTR3_seqs = []\n",
        "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
        "        UTR3_identifiers.append(seq_record.id)\n",
        "        UTR3_seqs.append(seq_record.seq)\n",
        "\n",
        "with open(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\Homo_sapiens.GRCh38.83.chosenTranscript.5pUTRs.fa') as fasta_file:  # Will close handle cleanly\n",
        "    UTR5_identifiers = []\n",
        "    UTR5_seqs = []\n",
        "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
        "        UTR5_identifiers.append(seq_record.id)\n",
        "        UTR5_seqs.append(seq_record.seq)\n",
        "\n",
        "with open(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\Homo_sapiens.GRCh38.83.chosenTranscript.ORFs.fa') as fasta_file:  # Will close handle cleanly\n",
        "    ORF_identifiers = []\n",
        "    ORF_seqs = []\n",
        "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
        "        ORF_identifiers.append(seq_record.id)\n",
        "        ORF_seqs.append(seq_record.seq)\n",
        "\n",
        "print(UTR3_seqs[0])\n",
        "print(UTR5_seqs[0])\n",
        "print(ORF_seqs[0])\n",
        "print(UTR3_identifiers[0])\n",
        "print(UTR5_identifiers[0])\n",
        "print(ORF_identifiers[0])\n",
        "print(len(UTR3_seqs))\n",
        "print(len(UTR5_seqs))\n",
        "print(len(ORF_seqs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFU1gCykz-M7",
        "outputId": "33925934-55fe-44fd-8a85-3f26f8b101de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AATATTATGTATGCAGCAATATTTGAGTAACAAGAAGCAAATATCCAAGTTCCAAAATTATAAAAGAAATTCTTATCCAAATAGTAATGTTCTAATTGATCATATAAGAAAGCAAAGCATAGACATTAGAATTATAAGTCAGCAGTGGTCTGTTCAAGAACAATCAACATTTTTAGAAAATAGTAGGACAAAATTAGGAAATAATTATCACCAAGAGGATCTAGTTCATGACTTTCTATTATCTCAATTAGATTGCTCAATCATCAGCCTTCCTATACTAAACTCTGATTCAGGACCAAGAAAGGCATAGTCTGACTCTGGAAATGCGCTGTTGGAAGCCAAATAACATCAATACTCTTGTTCTATAATTGAATATCAAATAAGACAAATTACCATTAATTTAATGACTGTGGAGTTAATTGTATACCAGCATTTCAGCAAATCATCATCAATAGTATTACATTAGCAATTTATGCAATTAAAAGGGCTTTGTAAAACTTTGAATAGATTTTATTGTCATTAGTAGCTGTTGGAACTTCATTATTATATAATGTTTTTGCAAACTTTAACTTTTTTCTAAATTGTTAAATAAAAGAATAACTATCCTTAATCTAAATAATTTTGGTAGCAAATCCTATAAGGTATTAAACATTTTAAGGTATATTATTACATTGCTATTTTACTGTTTCTCATTAACCCAAACAGTTTAAAGGCAGAATTCCACTTAGAAACAAGTTGCATTTTGAAAGTTTATTTGTAATCCATTTGTTTGGAATTCAGAAATGTATTTCACATAAAAATAATCTTGGAAGTAATAAATTCCAAAATTAACTAACAAAA\n",
            "AGATGAGATTTCATCATGTTGGCCAGCCTGGTCTCAAACTCCTGACCTCAAGTGACCCGCCTGCCTCAGCCTCCCAAAGTGCTGGGATTACAGGAATTTAGTGATTGACA\n",
            "ATGGCAGAAAAAATCCTAGAGAAGTTGGATGTCCTTGATAAGCAAGCAGAGATAATCTTGGCCAGAAGAACAAAGATAAACAGGCTTCAGAGTGAAGGAAGAAAAACAACTATGGCTATACCCCTGACATTTGATTTTCAGTTGGAATTTGAAGAAGCTCTTGCTACATCCGCGTCTAAGGCAATATCAAAGATCAAAGAAGACAAGTCATGCAGCATTACAAAATCAAAAATGCATGTCTCTTTCAAATGTGAGCCTGAACCTAGAAAGAGTAATTTTGAAAAGTCAAATTTAAGACCATTCTTTATTCAAACAAATGTAAAAAATAAAGAAAGTGAGTCAACAGCTCAAATTGAAAAAAAACCTAGGAAACCATTGGATTCTGTTGGTCTCTTAGAAGGTGATAGAAATAAAAGAAAAAAATCTCCACAGATGAACGATTTTAATATAAAAGAAAACAAATCGGTCAGAAATTATCAATTAAGTAAGTATAGGTCAGTAAGAAAGAAAAGCTTGCTCCCGTTGTGCTTTGAGGATGAATTGAAAAATCCACATGCCAAGATAGTCAACGTTAGTCCAACAAAGACAGTAACTTCTCACATGGAACAAAAGGACACAAATCCCATAATTTTCCATGACACAGAATATGTACGAATGTTACTTTTGACAAAAAATAGATTTTCTTCTCATCCTTTGGAAAATGAAAACATTTACCCACATAAAAGAACAAATTTCATTTTAGAAAGAAATTGTGAAATCCTCAAATCTATAATTGGCAATCAATCTATTTCTCTTTTCAAACCCCAAAAAACTATGCCTACAGTACAGAGAAAAGATATACAGATCCCTATGTCTTTTAAAGCGGGCCACACAACTGTAGATGATAAACTAAAGAAGAAAACTAATAAGCAGACACTAGAAAACAGATCTTGGAATACACTCTATAATTTCTCACAGAATTTTTCTAGCCTAACAAAACAATTTGTGGGTTACCTTGATAAAGCTGTTATTCATGAAATGAGTGCCCAAACTGGAAAATTTGAAAGAATGTTTTCTGCAGGAAAACCAACGAGCATACCCACATCCAGTGCCTTACCTGTCAAATGTTACTCAAAGCCTTTTAAATATATATATGAACTAAATAATGTAACGCCACTGGATAATTTGTTAAACTTATCAAATGAAATTTTAAATGCCTCA\n",
            "ENSG00000203963\n",
            "ENSG00000203963\n",
            "ENSG00000203963\n",
            "19094\n",
            "18642\n",
            "20253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exons = pd.read_csv(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\Kelley_et_al_exon_junctions.txt', delimiter = \"\\t\")\n",
        "exons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "HrhPuo0w0di6",
        "outputId": "9b91e12c-f3ca-4cd6-ce38-c4fcff267c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                GeneID  UTR5_len  \\\n",
              "0      ENSG00000000003       112   \n",
              "1      ENSG00000000457       222   \n",
              "2      ENSG00000000460       700   \n",
              "3      ENSG00000000938       289   \n",
              "4      ENSG00000000971       240   \n",
              "...                ...       ...   \n",
              "13225  ENSG00000278615        48   \n",
              "13226  ENSG00000278619       239   \n",
              "13227  ENSG00000278845       161   \n",
              "13228  ENSG00000280789       574   \n",
              "13229  ENSG00000281991       330   \n",
              "\n",
              "                         Exon_Junctions_In_Full_Sequence  \n",
              "0                                199,388,463,562,697,781  \n",
              "1      387,573,687,744,847,959,1037,1177,1362,1534,16...  \n",
              "2      766,871,1012,1178,1263,1402,1483,1548,1697,182...  \n",
              "3           515,618,717,821,971,1127,1307,1384,1538,1670  \n",
              "4      298,484,590,667,859,1030,1204,1399,1576,1759,1...  \n",
              "...                                                  ...  \n",
              "13225                                         87,212,310  \n",
              "13226                                  781,875,1008,1128  \n",
              "13227                        227,405,523,622,671,821,995  \n",
              "13228                                          1056,1139  \n",
              "13229                                                495  \n",
              "\n",
              "[13230 rows x 3 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GeneID</th>\n",
              "      <th>UTR5_len</th>\n",
              "      <th>Exon_Junctions_In_Full_Sequence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ENSG00000000003</td>\n",
              "      <td>112</td>\n",
              "      <td>199,388,463,562,697,781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENSG00000000457</td>\n",
              "      <td>222</td>\n",
              "      <td>387,573,687,744,847,959,1037,1177,1362,1534,16...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENSG00000000460</td>\n",
              "      <td>700</td>\n",
              "      <td>766,871,1012,1178,1263,1402,1483,1548,1697,182...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ENSG00000000938</td>\n",
              "      <td>289</td>\n",
              "      <td>515,618,717,821,971,1127,1307,1384,1538,1670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENSG00000000971</td>\n",
              "      <td>240</td>\n",
              "      <td>298,484,590,667,859,1030,1204,1399,1576,1759,1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13225</th>\n",
              "      <td>ENSG00000278615</td>\n",
              "      <td>48</td>\n",
              "      <td>87,212,310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13226</th>\n",
              "      <td>ENSG00000278619</td>\n",
              "      <td>239</td>\n",
              "      <td>781,875,1008,1128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13227</th>\n",
              "      <td>ENSG00000278845</td>\n",
              "      <td>161</td>\n",
              "      <td>227,405,523,622,671,821,995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13228</th>\n",
              "      <td>ENSG00000280789</td>\n",
              "      <td>574</td>\n",
              "      <td>1056,1139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13229</th>\n",
              "      <td>ENSG00000281991</td>\n",
              "      <td>330</td>\n",
              "      <td>495</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13230 rows × 3 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chromosomes = pd.read_csv(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\Kelley_et_al_chromosomes.txt', delimiter = \"\\t\")\n",
        "chromosomes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "N_zoThwx0u0w",
        "outputId": "e2e3986e-b4ec-4479-e703-5b88e4179c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                GeneID  Chromosome\n",
              "0      ENSG00000186092           1\n",
              "1      ENSG00000279928           1\n",
              "2      ENSG00000279457           1\n",
              "3      ENSG00000278566           1\n",
              "4      ENSG00000273547           1\n",
              "...                ...         ...\n",
              "20290  ENSG00000277856  KI270726.1\n",
              "20291  ENSG00000275063  KI270726.1\n",
              "20292  ENSG00000271254  KI270711.1\n",
              "20293  ENSG00000277475  KI270713.1\n",
              "20294  ENSG00000268674  KI270713.1\n",
              "\n",
              "[20295 rows x 2 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GeneID</th>\n",
              "      <th>Chromosome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ENSG00000186092</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENSG00000279928</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENSG00000279457</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ENSG00000278566</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENSG00000273547</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20290</th>\n",
              "      <td>ENSG00000277856</td>\n",
              "      <td>KI270726.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20291</th>\n",
              "      <td>ENSG00000275063</td>\n",
              "      <td>KI270726.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20292</th>\n",
              "      <td>ENSG00000271254</td>\n",
              "      <td>KI270711.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20293</th>\n",
              "      <td>ENSG00000277475</td>\n",
              "      <td>KI270713.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20294</th>\n",
              "      <td>ENSG00000268674</td>\n",
              "      <td>KI270713.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20295 rows × 2 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chromosomes['Chromosome'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7fzASja1JI6",
        "outputId": "75f2cc68-901f-4849-c807-e51114259934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1             2053\n",
              "19            1458\n",
              "11            1316\n",
              "2             1298\n",
              "17            1185\n",
              "3             1070\n",
              "6             1045\n",
              "12            1033\n",
              "7              980\n",
              "5              868\n",
              "16             865\n",
              "X              824\n",
              "14             824\n",
              "9              772\n",
              "4              747\n",
              "10             730\n",
              "8              670\n",
              "15             609\n",
              "20             541\n",
              "22             489\n",
              "13             320\n",
              "18             269\n",
              "21             233\n",
              "Y               54\n",
              "MT              13\n",
              "KI270728.1       6\n",
              "KI270727.1       4\n",
              "KI270734.1       3\n",
              "GL000194.1       2\n",
              "GL000195.1       2\n",
              "KI270726.1       2\n",
              "KI270713.1       2\n",
              "GL000009.2       1\n",
              "GL000205.2       1\n",
              "GL000219.1       1\n",
              "GL000213.1       1\n",
              "GL000218.1       1\n",
              "KI270731.1       1\n",
              "KI270721.1       1\n",
              "KI270711.1       1\n",
              "Name: Chromosome, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XRaMXVFv1YvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rbp_k = np.load(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\RBP_k.npy')\n",
        "rbp_k.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNwe4p0T4_u7",
        "outputId": "4e0de028-3741-496d-d965-dc809f62be42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13230, 59)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So not only do we have much more chromosomes, we the data comes from Pedro (df), Saluki (the sequences), from Pauline (chromosomes, exon junctions, via Saluki-chosen transcript), and from Yasmine (RBPs using Deepripe).\n",
        "Thus we should take good care that we merge the tables correctly."
      ],
      "metadata": {
        "id": "ITVKHQAf5c5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d = {'geneID': UTR5_identifiers, 'UTR5_seqs': UTR5_seqs}\n",
        "UTR5 = pd.DataFrame(data=d)\n",
        "d = {'geneID': ORF_identifiers, 'ORF_seqs': ORF_seqs}\n",
        "ORF = pd.DataFrame(data=d)\n",
        "d = {'geneID': UTR3_identifiers, 'UTR3_seqs': UTR3_seqs}\n",
        "UTR3 = pd.DataFrame(data=d)\n",
        "\n",
        "#merge every data frame to sequences\n",
        "halflife = hl[[\"Ensembl Gene Id\", \"zscore\"]]\n",
        "seqs = pd.merge(pd.merge(UTR5, ORF, on ='geneID'), UTR3, on = 'geneID')\n",
        "sequences = pd.merge(halflife, seqs, right_on = 'geneID', left_on = 'Ensembl Gene Id')\n",
        "sequences = sequences.drop(columns=[\"geneID\"])\n",
        "sequences = sequences.rename(columns={\"Ensembl Gene Id\": \"geneID\"})\n",
        "sequences = pd.merge(sequences, chromosomes, left_on='geneID', right_on='GeneID')\n",
        "sequences = sequences.drop(columns=[\"GeneID\"])\n",
        "sequences = pd.merge(sequences, exons, left_on='geneID', right_on='GeneID')\n",
        "sequences = sequences.drop(columns=[\"GeneID\"])\n",
        "\n",
        "#transform seqs into strings:\n",
        "sequences[\"UTR5_seqs\"] = sequences[\"UTR5_seqs\"].apply(str)\n",
        "sequences[\"UTR3_seqs\"] = sequences[\"UTR3_seqs\"].apply(str)\n",
        "sequences[\"ORF_seqs\"] = sequences[\"ORF_seqs\"].apply(str)\n",
        "\n",
        "rubbish = [d, UTR3, ORF, UTR5, UTR5_seqs, UTR3_seqs, ORF_seqs, UTR3_identifiers, UTR5_identifiers, ORF_identifiers, hl, halflife]\n",
        "del rubbish\n",
        "\n",
        "sequences.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "M7rLipk85dwm",
        "outputId": "64c65ac5-e5ad-41b9-bf80-520cc2c56f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            geneID    zscore  \\\n",
              "0  ENSG00000000003  1.807620   \n",
              "1  ENSG00000000457 -1.446182   \n",
              "2  ENSG00000000460  0.092022   \n",
              "3  ENSG00000000938 -0.196955   \n",
              "4  ENSG00000000971  1.611324   \n",
              "\n",
              "                                           UTR5_seqs  \\\n",
              "0  AGTTGTGGACGCTCGTAAGTTTTCGGCAGTTTCCGGGGAGACTCGG...   \n",
              "1  TGTCCCGTTTCCGGACCCGTCTCTATGGTGTAGGAGAAACCCGGCC...   \n",
              "2  GGCTTTGGCCCTGGAAAGCCTCGCGGACGTGTTCTGACCCAAGGTT...   \n",
              "3  GGCTTGGGGCTAGGGCGTGACTGTCTCCCTGCCACCATCACCGCCC...   \n",
              "4  ACAGCATTAACATTTAGTGGGAGTGCAGTGAGAATTGGGTTTAACT...   \n",
              "\n",
              "                                            ORF_seqs  \\\n",
              "0  ATGGCGTCCCCGTCTCGGAGACTGCAGACTAAACCAGTCATTACTT...   \n",
              "1  ATGGGATCAGAGAACAGTGCTTTAAAGAGCTATACACTGAGAGAAC...   \n",
              "2  ATGTTTTTACCTCATATGAACCACCTGACATTGGAACAGACTTTCT...   \n",
              "3  ATGGGCTGTGTGTTCTGCAAGAAATTGGAGCCGGTGGCCACGGCCA...   \n",
              "4  ATGAGACTTCTAGCAAAGATTATTTGCCTTATGTTATGGGCTATTT...   \n",
              "\n",
              "                                           UTR3_seqs Chromosome  UTR5_len  \\\n",
              "0  CCCAATGTATCTGTGGGCCTATTCCTCTCTACCTTTAAGGACATTT...          X       112   \n",
              "1  CAATAGATGTGAGTTAAACTTTAGGAAAAAGGATTCCCTTTTTTTA...          1       222   \n",
              "2  AACTTATCACTAGGCAGAACTGGGTTTGATGCTTTGTCAACTGAAA...          1       700   \n",
              "3  CCTGTCCGGGCATCAACCCTCTCTGGCGGTGGCCACCAGTCCTTGC...          1       289   \n",
              "4  AATCAATCATAAAGTGCACACCTTTATTCAGAACTTTAGTATTAAA...          1       240   \n",
              "\n",
              "                     Exon_Junctions_In_Full_Sequence  \n",
              "0                            199,388,463,562,697,781  \n",
              "1  387,573,687,744,847,959,1037,1177,1362,1534,16...  \n",
              "2  766,871,1012,1178,1263,1402,1483,1548,1697,182...  \n",
              "3       515,618,717,821,971,1127,1307,1384,1538,1670  \n",
              "4  298,484,590,667,859,1030,1204,1399,1576,1759,1...  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>geneID</th>\n",
              "      <th>zscore</th>\n",
              "      <th>UTR5_seqs</th>\n",
              "      <th>ORF_seqs</th>\n",
              "      <th>UTR3_seqs</th>\n",
              "      <th>Chromosome</th>\n",
              "      <th>UTR5_len</th>\n",
              "      <th>Exon_Junctions_In_Full_Sequence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ENSG00000000003</td>\n",
              "      <td>1.807620</td>\n",
              "      <td>AGTTGTGGACGCTCGTAAGTTTTCGGCAGTTTCCGGGGAGACTCGG...</td>\n",
              "      <td>ATGGCGTCCCCGTCTCGGAGACTGCAGACTAAACCAGTCATTACTT...</td>\n",
              "      <td>CCCAATGTATCTGTGGGCCTATTCCTCTCTACCTTTAAGGACATTT...</td>\n",
              "      <td>X</td>\n",
              "      <td>112</td>\n",
              "      <td>199,388,463,562,697,781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENSG00000000457</td>\n",
              "      <td>-1.446182</td>\n",
              "      <td>TGTCCCGTTTCCGGACCCGTCTCTATGGTGTAGGAGAAACCCGGCC...</td>\n",
              "      <td>ATGGGATCAGAGAACAGTGCTTTAAAGAGCTATACACTGAGAGAAC...</td>\n",
              "      <td>CAATAGATGTGAGTTAAACTTTAGGAAAAAGGATTCCCTTTTTTTA...</td>\n",
              "      <td>1</td>\n",
              "      <td>222</td>\n",
              "      <td>387,573,687,744,847,959,1037,1177,1362,1534,16...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENSG00000000460</td>\n",
              "      <td>0.092022</td>\n",
              "      <td>GGCTTTGGCCCTGGAAAGCCTCGCGGACGTGTTCTGACCCAAGGTT...</td>\n",
              "      <td>ATGTTTTTACCTCATATGAACCACCTGACATTGGAACAGACTTTCT...</td>\n",
              "      <td>AACTTATCACTAGGCAGAACTGGGTTTGATGCTTTGTCAACTGAAA...</td>\n",
              "      <td>1</td>\n",
              "      <td>700</td>\n",
              "      <td>766,871,1012,1178,1263,1402,1483,1548,1697,182...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ENSG00000000938</td>\n",
              "      <td>-0.196955</td>\n",
              "      <td>GGCTTGGGGCTAGGGCGTGACTGTCTCCCTGCCACCATCACCGCCC...</td>\n",
              "      <td>ATGGGCTGTGTGTTCTGCAAGAAATTGGAGCCGGTGGCCACGGCCA...</td>\n",
              "      <td>CCTGTCCGGGCATCAACCCTCTCTGGCGGTGGCCACCAGTCCTTGC...</td>\n",
              "      <td>1</td>\n",
              "      <td>289</td>\n",
              "      <td>515,618,717,821,971,1127,1307,1384,1538,1670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENSG00000000971</td>\n",
              "      <td>1.611324</td>\n",
              "      <td>ACAGCATTAACATTTAGTGGGAGTGCAGTGAGAATTGGGTTTAACT...</td>\n",
              "      <td>ATGAGACTTCTAGCAAAGATTATTTGCCTTATGTTATGGGCTATTT...</td>\n",
              "      <td>AATCAATCATAAAGTGCACACCTTTATTCAGAACTTTAGTATTAAA...</td>\n",
              "      <td>1</td>\n",
              "      <td>240</td>\n",
              "      <td>298,484,590,667,859,1030,1204,1399,1576,1759,1...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 10000 #this is slightly longer than the 95% quantile, but lower than Saluki's implementation\n",
        "\n",
        "seqs = sequences['UTR5_seqs'] + sequences['ORF_seqs'] + sequences['UTR3_seqs']\n",
        "def pad_sequence(seqs, max_len, anchor='start', value='N'):\n",
        "  padded_seqs = [fixed_len(seq, max_len, anchor=anchor) for seq in seqs.astype(\"string\")]\n",
        "  return padded_seqs\n",
        "fixed_len_seqs = np.array(pad_sequence(seqs, max_len))\n",
        "print(fixed_len_seqs[0:4])\n",
        "del seqs\n",
        "print(fixed_len_seqs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8oUTj9w-XQg",
        "outputId": "c673ba5f-3122-446a-cd0c-9ff0fc027ff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AGTTGTGGACGCTCGTAAGTTTTCGGCAGTTTCCGGGGAGACTCGGGGACTCCGCGTCTCGCTCTCTGTGTTCCAATCGCCCGGTGCGGTGGTGCAGGGTCTCGGGCTAGTCATGGCGTCCCCGTCTCGGAGACTGCAGACTAAACCAGTCATTACTTGTTTCAAGAGCGTTCTGCTAATCTACACTTTTATTTTCTGGATCACTGGCGTTATCCTTCTTGCAGTTGGCATTTGGGGCAAGGTGAGCCTGGAGAATTACTTTTCTCTTTTAAATGAGAAGGCCACCAATGTCCCCTTCGTGCTCATTGCTACTGGTACCGTCATTATTCTTTTGGGCACCTTTGGTTGTTTTGCTACCTGCCGAGCTTCTGCATGGATGCTAAAACTGTATGCAATGTTTCTGACTCTCGTTTTTTTGGTCGAACTGGTCGCTGCCATCGTAGGATTTGTTTTCAGACATGAGATTAAGAACAGCTTTAAGAATAATTATGAGAAGGCTTTGAAGCAGTATAACTCTACAGGAGATTATAGAAGCCATGCAGTAGACAAGATCCAAAATACGTTGCATTGTTGTGGTGTCACCGATTATAGAGATTGGACAGATACTAATTATTACTCAGAAAAAGGATTTCCTAAGAGTTGCTGTAAACTTGAAGATTGTACTCCACAGAGAGATGCAGACAAAGTAAACAATGAAGGTTGTTTTATAAAGGTGATGACCATTATAGAGTCAGAAATGGGAGTCGTTGCAGGAATTTCCTTTGGAGTTGCTTGCTTCCAACTGATTGGAATCTTTCTCGCCTACTGCCTCTCTCGTGCCATAACAAATAACCAGTATGAGATAGTGCCCAATGTATCTGTGGGCCTATTCCTCTCTACCTTTAAGGACATTTAGGGTCCCCCCTGTGAATTAGAAAGTTGCTTGGCTGGAGAACTGACAACACTACTTACTGATAGACCAAAAAACTACACCAGTAGGTTGATTCAATCAAGATGTATGTAGACCTAAAACTACACCAATAGGCTGATTCAATCAAGATCCGTGCTCGCAGTGGGCTGATTCAATCAAGATGTATGTTTGCTATGTTCTAAGTCCACCTTCTATCCCATTCATGTTAGATCGTTGAAACCCTGTATCCCTCTGAAACACTGGAAGAGCTAGTAAATTGTAAATGAAGTAATACTGTGTTCCTCTTGACTGTTATTTTTCTTAGTAGGGGGCCTTTGGAAGGCACTGTGAATTTGCTATTTTGATGTAGTGTTACAAGATGGAAAATTGATTCCTCTGACTTTGCTATTGATGTAGTGTGATAGAAAATTCACCCCTCTGAACTGGCTCCTTCCCAGTCAAGGTTATCTGGTTTGATTGTATAATTTGCACCAAGAAGTTAAAATGTTTTATGACTCTCTGTTCTGCTGACAGGCAGAGAGTCACATTGTGTAATTTAATTTCAGTCAGTCAATAGATGGCATCCCTCATCAGGGTTGCCAGATGGTGATAACAGTGTAAGGCCTTGGGTCTAAGGCATCCACGACTGGAAGGGACTACTGATGTTCTGTGATACATCAGGTTTCAGCACACAACTTACATTTCTTTGCCTCCAAATTGAGGCATTTATTATGATGTTCATACTTTCCCTCTTGTTTGAAAGTTTCTAATTATTAAATGGTGTCGGAATTGTTGTATTTTCCTTAGGAATTCAGTGGAACTTATCTTCATTAAATTTAGCTGGTACCAGGTTGATATGACTTGTCAATATTATGGTCAACTTTAAGTCTTAGTTTTCGTTTGTGCCTTTGATTAATAAGTATAACTCTTATACAATAAATACTGCTTTCCTCTAAAAAGATCGTGTTTAAATTAACTTGTAGAAAATCTGCTGGAATGGTTGTTGTTTTCCACTGAGAAAGCTAAGCCCTACATTTCTATTCAGAGTACTGTTTTTAGATGTGAAATATAAGCCTGCGGCCTTAACTCTGTATTAAAAAAAATGTTTTTGTTTAAAAAAAACTGTTCCCATAGGTGCAGCAAACCACCATGGCACATGTATACCTATGTAACAAACCTGCACATTCTGCACATGTATCCCAGAACTTAATGTAAACAAAAAAATCTTAAAGTGCAAATATTAAAAAAAACTGTTCTCTGTGAAAAAAATTATATTCCATGTTATAAAGTAGCATATGACTAGTGTTCTCCTAGNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN'\n",
            " 'TGTCCCGTTTCCGGACCCGTCTCTATGGTGTAGGAGAAACCCGGCCCCCAGAAGATTGTGGGTGTAGTGGCCACAGCCTTACAGGCAGGCAGGGGTGGTTGGTGTCAACAGGGGGGCCAACAGGGTACCAGAGCCAAGACCCTCGGCCTCCTCCCCCGCCGCCTTCCTGCAGATCTGCTTGGCTTTGAGGAAGAGTGGCAGTACTGCCTCACTGCATAAGGGATGGGATCAGAGAACAGTGCTTTAAAGAGCTATACACTGAGAGAACCACCATTTACCTTACCCTCTGGACTTGCTGTTTATCCCGCTGTACTGCAAGATGGCAAATTTGCTTCAGTTTTTGTGTATAAGAGAGAAAATGAAGACAAGGTTAATAAAGCTGCCAAGCATTTGAAGACACTTCGTCACCCTTGCTTGCTAAGATTTTTATCTTGTACTGTGGAAGCGGATGGCATTCATCTTGTCACTGAGCGAGTACAGCCCCTGGAAGTGGCTTTGGAAACATTGTCTTCTGCAGAGGTCTGTGCTGGGATCTATGACATATTGCTGGCTCTTATCTTCCTTCATGACAGAGGACACCTAACACACAATAATGTCTGTTTATCATCTGTGTTTGTGAGTGAAGATGGACACTGGAAGCTAGGAGGAATGGAAACTGTTTGTAAAGTTTCTCAGGCCACACCAGAGTTTCTGAGGAGTATTCAGTCAATAAGAGACCCAGCATCTATCCCTCCTGAAGAGATGTCTCCAGAATTCACAACTCTCCCAGAGTGTCATGGACATGCCCGGGATGCCTTTTCATTTGGAACATTGGTGGAAAGTTTGCTCACAATCTTAAATGAACAGGTTTCAGCGGATGTTCTCTCCAGCTTTCAACAGACCTTGCACTCAACTTTGCTGAATCCCATTCCAAAATGTCGGCCAGCGCTCTGCACCTTACTATCTCATGACTTCTTCAGAAATGATTTTCTGGAAGTTGTGAATTTCTTGAAAAGTTTAACATTGAAGAGTGAAGAGGAGAAAACGGAATTCTTTAAATTTCTGCTGGACAGAGTCAGCTGCTTGTCAGAGGAATTGATAGCTTCAAGGTTGGTGCCTCTTCTGCTTAATCAGTTGGTGTTTGCAGAGCCAGTGGCTGTTAAGAGTTTTCTTCCTTATCTGCTTGGCCCCAAAAAAGATCATGCGCAGGGAGAAACTCCTTGCTTGCTCTCACCAGCCCTGTTCCAGTCACGGGTGATCCCCGTGCTTCTCCAGTTGTTTGAAGTTCATGAAGAGCATGTGCGGATGGTGCTGCTGTCTCACATCGAGGCCTACGTGGAGCACTTCACTCAGGAGCAGCTGAAGAAAGTCATCTTGCCACAGGTTTTGCTGGGCCTGCGTGATACTAGCGATTCCATTGTGGCAATTACTCTGCATAGCCTAGCAGTGCTGGTCTCTCTGCTTGGACCAGAGGTGGTTGTGGGAGGAGAACGAACCAAGATCTTCAAACGCACTGCCCCAAGTTTTACTAAAAATACTGACCTTTCTCTAGAAGATTCTCCTATGTGTGTCGTCTGCAGCCATCACAGTCAGATCTCGCCAATCTTGGAGAACCCCTTCTCTAGCATATTCCCTAAATGTTTCTTTTCTGGCAGCACGCCCATCAACAGCAAGAAGCACATACAGCGAGATTACTACAATACTCTTTTACAGACAGGCGATCCATTTTCTCAGCCTATTAAATTTCCCATAAATGGACTCTCAGATGTAAAAAATACTTCGGAGGACAGTGAAAACTTCCCATCAAGTTCTAAAAAGTCTGAGGAGTGGCCTGACTGGAGTGAACCTGAGGAGCCTGAAAATCAAACTGTCAACATACAGATTTGGCCTAGAGAACCTTGTGATGATGTCAAGTCCCAGTGCACTACCTTGGATGTGGAAGAGTCATCTTGGGATGACTGCGAGCCCAGCAGCTTAGATACTAAAGTAAACCCAGGAGGTGGAATCACTGCTACAAAACCTGTTACCTCAGGGGAGCAGAAGCCTATTCCTGCTTTGCTTTCACTCACTGAAGAGTCTATGCCTTGGAAATCAAGCTTACCCCAAAAGATTAGCCTTGTACAAAGGGGGGATGACGCAGACCAAATCGAGCCGCCAAAAGTGTCATCACAAGAAAGGCCCCTTAAGGTTCCATCAGAACTTGGTTTAGGAGAGGAATTCACCATTCAAGTAAAAAAGAAGCCAGTAAAAGATCCTGAGATGGATTGGTTTGCTGATATGATCCCAGAAATTAAGCCTTCTGCTGCTTTTCTTATATTACCTGAACTGAGGACAGAAATGGTCCCAAAAAAGGATGATGTCTCCCCAGTGATGCAGTTTTCCTCAAAATTTGCTGCAGCAGAAATTACTGAGGGAGAGGCTGAAGGCTGGGAAGAAGAAGGGGAGCTGAACTGGGAAGATAATAACTGGCAATAGATGTGAGTTAAACTTTAGGAAAAAGGATTCCCTTTTTTTAAAAAAAATCAATACCTCAAAAGCAGGCTTTGGGACAAGAAAACCCCAAAGTGGCCTGCTTTTCCCATCCCAGGAGCTCATTATCCAGTCTGTGCCAACTGAAGTAGGAGACTGACTGTGAGTGCTGGCTAAAAGCCCTGGGTGGTGAGGCTCACAGTACTGGTTTCCAGGAGGAAGAGCCTTTGTGCATTTGACTGAGGCCAGTTTCTATGAAGAGCAAGTAGCTGAGGAGAGGTCGAATTTACTGCTTTTTCCAGGACAATTCTGGAAGTAAAGAAAATGTAATTCAAGCTGGTTAGCTTAATTTTGTGCCATTCTTTAACATAAGAGTAAGCTCTATTATGAAATACAACTTTAAAAAATTTTAGCTATAAATTATATAAATGATTTTAAATTGCTGAGGTTTCCTTAGGCAGCTTATTTATTTGTTTACAGTTAGACTATCTGAGTAAATGGTTCTTTGTGGACCTAGGCAGTTCCTGACTGTTCCACATGTAGTACATTGTACCAAAGTTCTTAATAAGAATATTCCCCACAATCCTGTTCTCTAAATGTCAAATAAAGATTATTTTCACTAGATTCAACTTTACAAAANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN'\n",
            " 'GGCTTTGGCCCTGGAAAGCCTCGCGGACGTGTTCTGACCCAAGGTTTTAGCAGTGGATGTGGCGTTTTCTTCCATTCCTTCTTTCAGTTTTTCTGTACTCGTTGCTTGCAATTAAGTGTAAATACTTTTGCTAGTGGATAATGGGGGAGGCAAGGACTGAGACCTGCGGTATGACGATAGCTCTGGCTCTTAATAGTTTGAGGTAAAGCGAGATACTCTGAGCTTTTGTCTCCCGTAAAAAGGGTGGTGAATATGAATAAGGGCTTTCTTAGCGTTATAAGAATTAAAGGGCATAGTTCTGTGGTGTGAAATCTTTAAAAGATGTTCAGTAAATAAAAATGATTTTCCTCCTTCCCCTCTCAGACCTCTTTTTCTTCTTTCTTTCTTTTTTTTTGACAAGTTCTCACTCCTCTCACCCAGGCTGGAGTCTTTCTGAAAGAGTTCTTCCGCTTGTTGTTGGCTTTCAACTGTTGGATTTGAGGCGCTTAGCGCCTTCTTCGTCCGGGTGCAGCACATTCTTGATTGGTCTCATGCCTTTGTGGTTGTAAATGTGCCTGGAATCCTAGCCTTTCATGGTTTGTTCTGAGTAATGAATACCCTATTACTATGATACTAGTATCTTCCTTAATTATCCTACTCATTGTCTCAACATTCTGACAGTTGGATTGAGCATATTCAAATTTTGAAAATTATTGTAGAAATGTTTTTACCTCATATGAACCACCTGACATTGGAACAGACTTTCTTTTCACAAGTGTTACCAAAGACTGTGAAATTATTCGATGACATGATGTATGAATTAACCAGTCAAGCCAGAGGACTGTCAAGCCAAAATTTGGAAATCCAGACCACTCTAAGGAATATTTTACAAACAATGGTGCAGCTCTTAGGAGCTCTCACAGGATGTGTTCAGCATATCTGTGCCACACAGGAATCCATCATTTTGGAAAATATTCAGAGTCTCCCCTCCTCAGTCCTTCATATAATTAAAAGCACATTTGTGCATTGTAAGAATAGTGAATCTGTGTATTCTGGGTGTTTACACCTAGTTTCAGACCTTCTCCAGGCTCTTTTCAAGGAGGCCTATTCTCTTCAAAAGCAGTTAATGGAACTGCTGGACATGGTTTGCATGGACCCTTTAGTAGATGACAATGATGATATTTTGAATATGGTAATAGTTATTCATTCTTTATTGGATATCTGCTCTGTTATTTCCAGTATGGACCATGCATTTCATGCCAATACTTGGAAGTTTATAATTAAGCAGAGCCTTAAGCACCAGTCCATAATAAAAAGCCAGTTGAAACACAAAGATATAATTACTAGCTTGTGTGAAGACATTCTTTTCTCCTTCCATTCTTGTTTACAGTTAGCTGAGCAGATGACACAGTCAGATGCACAGGATAATGCTGACTACAGATTATTTCAGAAAACACTCAAATTGTGTCGTTTTTTTGCCAACTCCCTTTTGCACTACGCTAAGGAATTTCTTCCTTTCCTCTCTGATTCTTGCTGTACTTTGCACCAACTGTATCTTCAGATACACAGCAAGTTTCCTCCAAGCCTTTATGCTACCAGGATTTCTAAAGCACACCAAGAGGAAATAGCAGGTGCTTTCCTAGTGACACTGGATCCACTTATCAGTCAGCTGCTCACATTTCAGCCTTTCATGCAGGTGGTTTTGGACAGTAAATTAGACCTGCCATGTGAACTGCAGTTTCCACAATGTCTTCTTCTGGTTGTTGTCATGGATAAGCTGCCATCTCAGCCTAAGGAAGTGCAAACCCTGTGGTGCACAGACAGCCAGGTCTCAGAAACGACAACCAGGATATCTCTACTCAAAGCCGTTTTCTACAGTTTTGAGCAGTGTTCTGGTGAACTCTCTCTACCTGTTCATTTACAGGGATTAAAGAGTAAGGGGAAAGCTGAGGTGGCTGTCACCTTGTATCAGCATGTTTGTGTTCATCTGTGTACATTTATTACTTCCTTTCATCCCTCACTGTTTGCTGAACTGGATGCTGCTCTGCTGAATGCTGTACTTAGTGCTAATATGATCACCTCTTTGTTAGCTATGGATGCATGGTGCTTCCTTGCTCGATATGGGACTGCTGAACTGTGTGCACACCATGTCACCATAGTGGCTCATCTGATAAAGTCATGCCCTGGAGAATGTTATCAACTCATCAACCTATCAATACTGTTGAAGCGTCTCTTTTTCTTCATGGCACCACCCCATCAGCTGGAGTTTATCCAGAAATTTTCCCCAAAAGAAGCAGAAAATCTGCCTCTGTGGCAACATATTTCCTTCCAGGCGTTACCTCCTGAGCTTAGGGAACAAACTGTCCATGAGGTCACCACAGTAGGCACTGCAGAATGCAGGAAATGGCTGAGCAGGAGTCGTACTTTGGGAGAACTAGAATCTCTGAACACAGTACTGTCTGCTTTGCTTGCAGTATGTAATTCTGCTGGTGAAGCTTTGGATACAGGAAAACAAACTGCAATTATCGAAGTTGTGAGTCAGCTTTGGGCTTTTTTAAACATTAAACAGGTAGCAGATCAACCTTATGTTCAACAGACATTCAGCCTTTTACTTCCACTGTTGGGATTTTTCATTCAAACTCTAGATCCTAAACTGATACTTCAGGCAGTAACTTTGCAGACCTCGCTACTTAAATTAGAGCTTCCTGACTATGTTCGTTTGGCAATGTTGGATTTTGTATCTTCTTTAGGAAAACTTTTTATACCTGAAGCTATCCAGGACAGAATTCTGCCCAACCTGTCCTGTATGTTTGCCTTACTGCTAGCTGACAGGAGTTGGCTGCTAGAACAACATACCTTGGAGGCGTTTACTCAGTTCGCTGAGGGAACAAATCATGAAGAGATAGTTCCACAGTGTCTCAGTTCTGAAGAAACTAAGAACAAAGTTGTATCCTTTCTGGAGAAGACTGGGTTTGTAGATGAAACTGAAGCTGCCAAAGTGGAACGTGTGAAACAGGAAAAAGGTATTTTCTGGGAACCCTTTGCTAATGTGACTGTAGAAGAAGCAAAGAGGTCATCTTTACAGCCTTATGCAAAAAGAGCTCGTCAGGAGTTCCCCTGGGAAGAAGAGTACAGGTCAGCGCTGCATACAATAGCAGGGGCTTTGGAAGCAACTGAGTCACTACTCCAAAAGGGTCCTGCTCCAGCCTGGCTTTCAATGGAAATGGAGGCGCTCCAAGAAAGGATGGATAAGCTAAAACGTTACATACATACTCTAGGGAACTTATCACTAGGCAGAACTGGGTTTGATGCTTTGTCAACTGAAAATACTTATGTCTGTACATTTTCTAACAGATATAAAACAAATTTTGTAAAGTTGAATCTAGTGAAAATAATCTTTATTTGACATTTAGAGAACAGGATTGTGGGGAATATTCTTATTAAGAACTTTGGTACAATGTACTACATGTGGAACAGTCAGGAACTGCCTAGGTCCACAAAGAACCATTTACTCAGATAGTCTAACTGTAAACAAATAAATAAGCTGCCTAAGGAAACCTCAGCAATTTAAAATCATTTATATAATTTATAGCTAAAATTTTTTAAAGTTGTATTTCATAATAGAGCTTACTCTTATGTTAAAGAATGGCACAAAATTAAGCTAACCAGCTTGAATTACATTTTCTTTACTTCCAGAATTGTCCTGGAAAAAGCAGTAAATTCGACCTCTCCTCAGCTACTTGCTCTTCATAGAAACTGGCCTCAGTCAAATGCACAAAGGCTCTTCCTCCTGGAAACCAGTACTGTGAGCCTCACCACCCAGGGCTTTTAGCCAGCACTCACAGTCAGTCTCCTACTTCAGTTGGCACAGACTGGATAATGAGCTCCTGGGATGGGAAAAGCAGGCCACTTTGGGGTTTTCTTGTCCCAAAGCCTGCTTTTGAGGTATTGATTTTTTTTAAAAAAAGGGAATCCTTTTTCCTAAAGTTTAACTCACATCTATTGTCACCAGTTATTATCTTCCCAGTTCAGCTCCCCTTCTTCTTCCCAGCCTTCAGCCTCTCCCTGCAACAAAATAAAGCACACCAAGAACCCACTGAAACAAATCATATGCAAAAATCATACGCAAATTTGAAAAAGCAGGAATTTAAAATTTATCTTTTGATGCCAGAAACACTACCTCGTACTAAGTAAAATAACTTAGAGCTCTAACAGAAAGTTGAAAAGTAGGATTACAAAACCATGGCACTGGAAAATAGCTTTTCAAAAACCATAAAATCCAGTAAAAATCAGTGTGGATGACACCAAATCCTTATTTTAATCCCTTTTTTTTCTTTTTCAAATAAAAAGGTTACAATAGCTCATTAAACAAANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN'\n",
            " 'GGCTTGGGGCTAGGGCGTGACTGTCTCCCTGCCACCATCACCGCCCGCCGGCCGTGACTGCAATAAGAGAAGTCCGAGGCGGCTTCCTCCTCCCTGCCCAGCAGGGGCGGCGGTCAGAGGCGGGCAGCACCCCAGTTCTCCCCGCACGCCGGCACTCGCGGCTGCTGGAGCCCCGGCTGGCTCACCCCGGGGCCGGGCAGAATTGGGCTCCAGGTCTCTGACCCCTCCCAAGGATCATGCCGCAGCCCCACTGACCCAGGAGTAGGGGCCTAAGGGCAGGGAACCTGGAATGGGCTGTGTGTTCTGCAAGAAATTGGAGCCGGTGGCCACGGCCAAGGAGGATGCTGGCCTGGAAGGGGACTTCAGAAGCTACGGGGCAGCAGACCACTATGGGCCTGACCCCACTAAGGCCCGGCCTGCATCCTCATTTGCCCACATCCCCAACTACAGCAACTTCTCCTCTCAGGCCATCAACCCTGGCTTCCTTGATAGTGGCACCATCAGGGGTGTGTCAGGGATTGGGGTGACCCTGTTCATTGCCCTGTATGACTATGAGGCTCGAACTGAGGATGACCTCACCTTCACCAAGGGCGAGAAGTTCCACATCCTGAACAATACTGAAGGTGACTGGTGGGAGGCTCGGTCTCTCAGCTCCGGAAAAACTGGCTGCATTCCCAGCAACTACGTGGCCCCTGTTGACTCAATCCAAGCTGAAGAGTGGTACTTTGGAAAGATTGGGAGAAAGGATGCAGAGAGGCAGCTGCTTTCACCAGGCAACCCCCAGGGGGCCTTTCTCATTCGGGAAAGCGAGACCACCAAAGGTGCCTACTCCCTGTCCATCCGGGACTGGGATCAGACCAGAGGCGATCATGTGAAGCATTACAAGATCCGCAAACTGGACATGGGCGGCTACTACATCACCACACGGGTTCAGTTCAACTCGGTGCAGGAGCTGGTGCAGCACTACATGGAGGTGAATGACGGGCTGTGCAACCTGCTCATCGCGCCCTGCACCATCATGAAGCCGCAGACGCTGGGCCTGGCCAAGGACGCCTGGGAGATCAGCCGCAGCTCCATCACGCTGGAGCGCCGGCTGGGCACCGGCTGCTTCGGGGATGTGTGGCTGGGCACGTGGAACGGCAGCACTAAGGTGGCGGTGAAGACGCTGAAGCCGGGCACCATGTCCCCGAAGGCCTTCCTGGAGGAGGCGCAGGTCATGAAGCTGCTGCGGCACGACAAGCTGGTGCAGCTGTACGCCGTGGTGTCGGAGGAGCCCATCTACATCGTGACCGAGTTCATGTGTCACGGCAGCTTGCTGGATTTTCTCAAGAACCCAGAGGGCCAGGATTTGAGGCTGCCCCAATTGGTGGACATGGCAGCCCAGGTAGCTGAGGGCATGGCCTACATGGAACGCATGAACTACATTCACCGCGACCTGAGGGCAGCCAACATCCTGGTTGGGGAGCGGCTGGCGTGCAAGATCGCAGACTTTGGCTTGGCGCGTCTCATCAAGGACGATGAGTACAACCCCTGCCAAGGTTCCAAGTTCCCCATCAAGTGGACAGCCCCAGAAGCTGCCCTCTTTGGCAGATTCACCATCAAGTCAGACGTGTGGTCCTTTGGGATCCTGCTCACTGAGCTCATCACCAAGGGCCGAATCCCCTACCCAGGCATGAATAAACGGGAAGTGTTGGAACAGGTGGAGCAGGGCTACCACATGCCGTGCCCTCCAGGCTGCCCAGCATCCCTGTACGAGGCCATGGAACAGACCTGGCGTCTGGACCCGGAGGAGAGGCCTACCTTCGAGTACCTGCAGTCCTTCCTGGAGGACTACTTCACCTCCGCTGAACCACAGTACCAGCCCGGGGATCAGACACCTGTCCGGGCATCAACCCTCTCTGGCGGTGGCCACCAGTCCTTGCCAATCCCCAGAGCTGTTCTTCCAAAGCCCCCAGGCTGGCTTAGAACCCCATAGAGTCCTAGCATCACCGAGGACGTGGCTGCTCTGACACCACCTAGGGCAACCTACTTGTTTTACAGATGGGGCAAAAGGAGGCCCAGAGCTGATCTCTCATCCGCTCTGGCCCCAAGCACTATTTCTTCCTTTTCCACTTAGGCCCCTACATGCCTGTAGCCTTTCTCACTCCATCCCCACCCAAAGTGCTCAGACCTTGTCTAGTTATTTATAAAACTGTATGTACCTCCCTCACTTCTCTCCTATCACTGCTTTCCTACTCTCCTTTTATCTCACTCTAGTCCAGGTGCCAAGAATTTCCCTTCTACCCTCTATTCTCTTGTGTCTGTAAGTTACAAAGTCAGGAAAAGTCTTGGCTGGACCCCTTTCCTGCTGGGTGGATGCAGTGGTCCAGGACTGGGGTCTGGGCCCAGGTTTGAGGGAGAAGGTTGCAGAGCACTTCCCACCTCTCTGAATAGTGTGTATGTGTTGGTTTATTGATTCTGTAAATAAGTAAAATGACAATATGAATCCTCAAACCATGAAATACCCTTGAACCTTCCTTTGGGAGCGGGGGTGGTCAATAGGGGGTGAACGGACAGATATGGCTACAGGCAGCAGCAGGGGAAGCTGGAGAGGGCCCTAATGCCTACCAAGCACGGGGCATCCAAGGTGTGGAGTTTTAGAACACCCAGAGTCCCACTGCTCATCTGCACGTGAGTTTAGAAGACAAGCAGCTGAAGATACATTAAAATGTCCCCTTCGTTGCTGANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN']\n",
            "(13230,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot for track 1-4: the nucleotides \n",
        "\n",
        "one_hot_seqs = np.array([one_hot(seq, neutral_value=0) for seq in fixed_len_seqs])\n",
        "print(one_hot_seqs[0:2])\n",
        "print(one_hot_seqs.shape)\n",
        "rubbish = [fixed_len_seqs]\n",
        "del rubbish"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdyuPKVL_0QT",
        "outputId": "7c00c967-22a1-405a-9db0-94ef8cc04a2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[1. 0. 0. 0.]\n",
            "  [0. 0. 1. 0.]\n",
            "  [0. 0. 0. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 1.]\n",
            "  [0. 0. 1. 0.]\n",
            "  [0. 0. 0. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]]]\n",
            "(13230, 10000, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot for track 5: the exon binding sites\n",
        "# dunno why, I guess I still suck at python, but this took me over an hours to code and bugfix\n",
        "# lol this is future me from the next day, this was wrong and I had redo all the training\n",
        "\n",
        "exons = []\n",
        "\n",
        "for i in range(len(sequences)):\n",
        "  onehot = np.repeat(0, repeats = max_len)\n",
        "  if(isinstance(sequences[\"Exon_Junctions_In_Full_Sequence\"][i], str)):\n",
        "    current_exons = list(map(int, sequences[\"Exon_Junctions_In_Full_Sequence\"][i].split(\",\")))\n",
        "    assert len(current_exons) > 0\n",
        "    positions_capped = [x for x in current_exons if x <= 10000] # delete all exon junctions after 10000 since we're capping the sequence there\n",
        "    onehot[positions_capped] = 1\n",
        "    '''\n",
        "    for j in current_exons:\n",
        "      positions = [x+len(sequences['UTR5_seqs'][i]) for x in current_exons] # have to add UTR5 length to indices\n",
        "      positions_capped = [x for x in positions if x <= 10000] # delete all exon junctions after 10000 since we're capping the sequence there\n",
        "      onehot[positions_capped] = 1 #exon junctions are 1 now\n",
        "      \n",
        "  if(isinstance(sequences[\"Exon_Junctions_In_Full_Sequence\"][i], float)):\n",
        "    if(not(math.isnan(sequences[\"Exon_Junctions_In_Full_Sequence\"][i]))):\n",
        "      onehot[int(sequences[\"Exon_Junctions_In_Full_Sequence\"][i])+len(sequences['UTR5_seqs'][i])] = 1\n",
        "    '''\n",
        "  exons.append(onehot)"
      ],
      "metadata": {
        "id": "27vK8R2SAD9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(map(int, sequences[\"Exon_Junctions_In_Full_Sequence\"][1].split(\",\"))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDsHa06rCEQq",
        "outputId": "3d8f0bc4-83bd-45fb-c9a4-b6d8291d06fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[387, 573, 687, 744, 847, 959, 1037, 1177, 1362, 1534, 1696, 2391]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#one hot for track 6: Marking the beginning of each codon with 1\n",
        "starts = []\n",
        "for i in range(len(sequences)):\n",
        "  #assert len(sequences['ORF_seqs'].astype(\"string\")[i]) % 3 == 0 \n",
        "  lst = list(range(len(sequences['ORF_seqs'].astype(\"string\")[i])))\n",
        "  onehot = np.repeat(0, repeats = len(sequences['ORF_seqs'].astype(\"string\")[i]))\n",
        "  onehot[lst[0::3]] = 1\n",
        "  full = np.concatenate((np.repeat([0], repeats = len(sequences['UTR5_seqs'].astype(\"string\")[i])),\n",
        "                         onehot,\n",
        "                         np.repeat([0], repeats = len(sequences['UTR3_seqs'].astype(\"string\")[i]))), axis=None)\n",
        "  if (len(full) > max_len):\n",
        "    full = full[:max_len]\n",
        "  elif (len(full) < max_len):\n",
        "    full = np.concatenate((full, np.repeat(0, repeats = max_len - len(full))),axis = None)\n",
        "  starts.append(full)"
      ],
      "metadata": {
        "id": "pRuNTsz4AHLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rubbish = [fixed_len_seqs]\n",
        "\n",
        "rubbish = [fixed_len_seqs, d, UTR3, ORF, UTR5, UTR5_seqs, UTR3_seqs, ORF_seqs, hl,\n",
        "           UTR3_identifiers, UTR5_identifiers, ORF_identifiers, halflife, max_len]\n",
        "del rubbish"
      ],
      "metadata": {
        "id": "lt-QVEBiP5Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This takes about 18 GB, so beware of that\n",
        "onehot = np.concatenate((one_hot_seqs,np.array(exons)[:, :, None], np.array(starts)[:, :, None]), axis = 2)\n",
        "print(onehot.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZpZoaGuPp3c",
        "outputId": "f4b5b52f-df39-4142-c41e-b512590dfb72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13230, 10000, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del exons\n",
        "del starts"
      ],
      "metadata": {
        "id": "8qpC7Tr5RZae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for The Train-Val-Test split we split as recommended on Chromosomes:"
      ],
      "metadata": {
        "id": "oM3U9VTxSy0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chrom_val = ['2', '3', '4']\n",
        "chrom_test = ['1', '8', '9']"
      ],
      "metadata": {
        "id": "Ih4OOJxISyJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_test = np.where(sequences.Chromosome.isin(chrom_test))[0]\n",
        "idx_val = np.where(sequences.Chromosome.isin(chrom_val))[0]\n",
        "idx_train = np.where(~(sequences.Chromosome.isin(chrom_test)| sequences.Chromosome.isin(chrom_val)))[0]"
      ],
      "metadata": {
        "id": "FQcQCI4BTFSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(array, idx_train, idx_val, idx_test):\n",
        "  return array[idx_train], array[idx_val], array[idx_test]"
      ],
      "metadata": {
        "id": "k7GWuUtuTJxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(idx_test))\n",
        "print(len(idx_val))\n",
        "print(len(idx_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOrm43s0Z8M2",
        "outputId": "4a7a0b4b-ef09-4f8b-fe7e-01305c4474db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2334\n",
            "2118\n",
            "8778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, X_test = train_test_split(onehot, idx_train, idx_val, idx_test)\n",
        "y_train, y_val, y_test = train_test_split(sequences['zscore'].values, idx_train, idx_val, idx_test)"
      ],
      "metadata": {
        "id": "mmpWExHRTfMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExplainedVariance(keras.callbacks.Callback):\n",
        "    def __init__(self, validation_data=(), interval=10):\n",
        "        super(keras.callbacks.Callback, self).__init__()\n",
        "\n",
        "        self.interval = interval\n",
        "        self.X_val, self.y_val = validation_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.interval == 0:\n",
        "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "            var_score = explained_variance_score(self.y_val, y_pred)\n",
        "            #r2 = r2_score(self.y_val, y_pred)\n",
        "            del y_pred\n",
        "            #print(\" - interval evaluation - epoch: {:d} - explained variance: {:.6f} - R2: {:.6f}\".format(epoch, var_score, r2))\n",
        "            print(\"interval evaluation - epoch: {:d} - explained variance: {:.6f}\".format(epoch, var_score))\n",
        "        gc.collect()"
      ],
      "metadata": {
        "id": "xf5ZZA63U-qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saluki-Type model\n",
        "input = kl.Input((X_train.shape[1:]))\n",
        "\n",
        "x = kl.Conv1D(64, kernel_size=5, activation=None, kernel_regularizer=tf.keras.regularizers.l2(l=0.0015))(input)\n",
        "x = kl.BatchNormalization()(x)\n",
        "x = kl.Activation(\"relu\")(x)\n",
        "\n",
        "for i in range(6):\n",
        "  x1 = kl.Conv1D(32, kernel_size=5, padding=\"same\", activation=None, kernel_regularizer=tf.keras.regularizers.l2(l=0.0015))(x)\n",
        "  x1 = kl.MaxPooling1D(pool_size=2)(x1)\n",
        "  x1 = kl.BatchNormalization()(x1)\n",
        "  x1 = kl.Activation(\"relu\")(x1)\n",
        "  x1 = kl.Dropout(0.33)(x1)\n",
        "\n",
        "  x2 = kl.Conv1D(32, kernel_size=3, padding = \"same\", activation=None, kernel_regularizer=tf.keras.regularizers.l2(l=0.0015))(x)\n",
        "  x2 = kl.MaxPooling1D(pool_size=2)(x2)\n",
        "  x2 = kl.BatchNormalization()(x2)\n",
        "  x2 = kl.Activation(\"relu\")(x2)\n",
        "  x2 = kl.Dropout(0.33)(x2)\n",
        "\n",
        "  x3 = kl.MaxPooling1D(pool_size=2)(x)\n",
        "  x3 = kl.Conv1D(16, kernel_size=1, padding='same', activation=None, kernel_regularizer=tf.keras.regularizers.l2(l=0.0015))(x3)\n",
        "  x3 = kl.BatchNormalization()(x3)\n",
        "  x3 = kl.Activation(\"relu\")(x3)\n",
        "  x3 = kl.Dropout(0.33)(x3)\n",
        "\n",
        "  x4 = kl.Conv1D(16, kernel_size=1, padding='same', activation=None, kernel_regularizer=tf.keras.regularizers.l2(l=0.0015))(x)\n",
        "  x4 = kl.MaxPooling1D(pool_size=2)(x4)\n",
        "  x4 = kl.BatchNormalization()(x4)\n",
        "  x4 = kl.Activation(\"relu\")(x4)\n",
        "  x4 = kl.Dropout(0.33)(x4)\n",
        "\n",
        "  x = kl.concatenate([x1, x2, x3, x4], axis = 2)\n",
        "\n",
        "x = kl.GRU(80, go_backwards=True, kernel_regularizer=tf.keras.regularizers.l2(l=0.001))(x)\n",
        "x = kl.Dropout(0.33)(x)\n",
        "x = kl.BatchNormalization()(x)\n",
        "x = kl.Activation(\"relu\")(x)\n",
        "\n",
        "x = kl.Dense(96, kernel_regularizer=tf.keras.regularizers.l2(l=0.001))(x) #backwards so it encounters padding first\n",
        "#x = kl.Dropout(0.3)(x)\n",
        "#x = kl.BatchNormalization()(x) #batch is fine here, no padding to consider anymore\n",
        "x = kl.Activation(\"relu\")(x)\n",
        "\n",
        "output = kl.Dense(units=1)(x)\n",
        "\n",
        "my_saluki = Model(inputs=input, outputs=output)\n",
        "my_saluki.summary()\n",
        "\n",
        "#Saluki: We chose layer normalization over batch normalization because most of the 3′ positions are zero padded and would confuse the batch statistics.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0vIgaHIVpyI",
        "outputId": "2cc52f67-9b94-4bf0-afb1-95560a8b85b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 10000, 6)]   0           []                               \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 9996, 64)     1984        ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 9996, 64)    256         ['conv1d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 9996, 64)     0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 9996, 32)     10272       ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 9996, 32)     6176        ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 4998, 64)    0           ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 9996, 16)     1040        ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 4998, 32)     0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 4998, 32)    0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 4998, 16)     1040        ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " max_pooling1d_3 (MaxPooling1D)  (None, 4998, 16)    0           ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 4998, 32)    128         ['max_pooling1d[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 4998, 32)    128         ['max_pooling1d_1[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 4998, 16)    64          ['conv1d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 4998, 16)    64          ['max_pooling1d_3[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 4998, 32)     0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 4998, 32)     0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 4998, 16)     0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 4998, 16)     0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 4998, 32)     0           ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 4998, 32)     0           ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 4998, 16)     0           ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 4998, 16)     0           ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 4998, 96)     0           ['dropout[0][0]',                \n",
            "                                                                  'dropout_1[0][0]',              \n",
            "                                                                  'dropout_2[0][0]',              \n",
            "                                                                  'dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 4998, 32)     15392       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 4998, 32)     9248        ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling1d_6 (MaxPooling1D)  (None, 2499, 96)    0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 4998, 16)     1552        ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling1d_4 (MaxPooling1D)  (None, 2499, 32)    0           ['conv1d_5[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_5 (MaxPooling1D)  (None, 2499, 32)    0           ['conv1d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 2499, 16)     1552        ['max_pooling1d_6[0][0]']        \n",
            "                                                                                                  \n",
            " max_pooling1d_7 (MaxPooling1D)  (None, 2499, 16)    0           ['conv1d_8[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 2499, 32)    128         ['max_pooling1d_4[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 2499, 32)    128         ['max_pooling1d_5[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 2499, 16)    64          ['conv1d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 2499, 16)    64          ['max_pooling1d_7[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 2499, 32)     0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 2499, 32)     0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 2499, 16)     0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 2499, 16)     0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 2499, 32)     0           ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 2499, 32)     0           ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 2499, 16)     0           ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 2499, 16)     0           ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 2499, 96)     0           ['dropout_4[0][0]',              \n",
            "                                                                  'dropout_5[0][0]',              \n",
            "                                                                  'dropout_6[0][0]',              \n",
            "                                                                  'dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 2499, 32)     15392       ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 2499, 32)     9248        ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_10 (MaxPooling1D  (None, 1249, 96)    0           ['concatenate_1[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 2499, 16)     1552        ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_8 (MaxPooling1D)  (None, 1249, 32)    0           ['conv1d_9[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_9 (MaxPooling1D)  (None, 1249, 32)    0           ['conv1d_10[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 1249, 16)     1552        ['max_pooling1d_10[0][0]']       \n",
            "                                                                                                  \n",
            " max_pooling1d_11 (MaxPooling1D  (None, 1249, 16)    0           ['conv1d_12[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 1249, 32)    128         ['max_pooling1d_8[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 1249, 32)    128         ['max_pooling1d_9[0][0]']        \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 1249, 16)    64          ['conv1d_11[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 1249, 16)    64          ['max_pooling1d_11[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1249, 32)     0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1249, 32)     0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1249, 16)     0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1249, 16)     0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 1249, 32)     0           ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 1249, 32)     0           ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 1249, 16)     0           ['activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 1249, 16)     0           ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 1249, 96)     0           ['dropout_8[0][0]',              \n",
            "                                                                  'dropout_9[0][0]',              \n",
            "                                                                  'dropout_10[0][0]',             \n",
            "                                                                  'dropout_11[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 1249, 32)     15392       ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 1249, 32)     9248        ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_14 (MaxPooling1D  (None, 624, 96)     0           ['concatenate_2[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)             (None, 1249, 16)     1552        ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_12 (MaxPooling1D  (None, 624, 32)     0           ['conv1d_13[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling1d_13 (MaxPooling1D  (None, 624, 32)     0           ['conv1d_14[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 624, 16)      1552        ['max_pooling1d_14[0][0]']       \n",
            "                                                                                                  \n",
            " max_pooling1d_15 (MaxPooling1D  (None, 624, 16)     0           ['conv1d_16[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 624, 32)     128         ['max_pooling1d_12[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 624, 32)     128         ['max_pooling1d_13[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 624, 16)     64          ['conv1d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 624, 16)     64          ['max_pooling1d_15[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 624, 32)      0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 624, 32)      0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 624, 16)      0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 624, 16)      0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 624, 32)      0           ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 624, 32)      0           ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)           (None, 624, 16)      0           ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 624, 16)      0           ['activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 624, 96)      0           ['dropout_12[0][0]',             \n",
            "                                                                  'dropout_13[0][0]',             \n",
            "                                                                  'dropout_14[0][0]',             \n",
            "                                                                  'dropout_15[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_17 (Conv1D)             (None, 624, 32)      15392       ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_18 (Conv1D)             (None, 624, 32)      9248        ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_18 (MaxPooling1D  (None, 312, 96)     0           ['concatenate_3[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_20 (Conv1D)             (None, 624, 16)      1552        ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_16 (MaxPooling1D  (None, 312, 32)     0           ['conv1d_17[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling1d_17 (MaxPooling1D  (None, 312, 32)     0           ['conv1d_18[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_19 (Conv1D)             (None, 312, 16)      1552        ['max_pooling1d_18[0][0]']       \n",
            "                                                                                                  \n",
            " max_pooling1d_19 (MaxPooling1D  (None, 312, 16)     0           ['conv1d_20[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 312, 32)     128         ['max_pooling1d_16[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 312, 32)     128         ['max_pooling1d_17[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 312, 16)     64          ['conv1d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 312, 16)     64          ['max_pooling1d_19[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 312, 32)      0           ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 312, 32)      0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 312, 16)      0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 312, 16)      0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 312, 32)      0           ['activation_17[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)           (None, 312, 32)      0           ['activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)           (None, 312, 16)      0           ['activation_19[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 312, 16)      0           ['activation_20[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 312, 96)      0           ['dropout_16[0][0]',             \n",
            "                                                                  'dropout_17[0][0]',             \n",
            "                                                                  'dropout_18[0][0]',             \n",
            "                                                                  'dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_21 (Conv1D)             (None, 312, 32)      15392       ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_22 (Conv1D)             (None, 312, 32)      9248        ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_22 (MaxPooling1D  (None, 156, 96)     0           ['concatenate_4[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_24 (Conv1D)             (None, 312, 16)      1552        ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_20 (MaxPooling1D  (None, 156, 32)     0           ['conv1d_21[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling1d_21 (MaxPooling1D  (None, 156, 32)     0           ['conv1d_22[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_23 (Conv1D)             (None, 156, 16)      1552        ['max_pooling1d_22[0][0]']       \n",
            "                                                                                                  \n",
            " max_pooling1d_23 (MaxPooling1D  (None, 156, 16)     0           ['conv1d_24[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 156, 32)     128         ['max_pooling1d_20[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 156, 32)     128         ['max_pooling1d_21[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 156, 16)     64          ['conv1d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 156, 16)     64          ['max_pooling1d_23[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 156, 32)      0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 156, 32)      0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 156, 16)      0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 156, 16)      0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_20 (Dropout)           (None, 156, 32)      0           ['activation_21[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_21 (Dropout)           (None, 156, 32)      0           ['activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_22 (Dropout)           (None, 156, 16)      0           ['activation_23[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_23 (Dropout)           (None, 156, 16)      0           ['activation_24[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 156, 96)      0           ['dropout_20[0][0]',             \n",
            "                                                                  'dropout_21[0][0]',             \n",
            "                                                                  'dropout_22[0][0]',             \n",
            "                                                                  'dropout_23[0][0]']             \n",
            "                                                                                                  \n",
            " gru (GRU)                      (None, 80)           42720       ['concatenate_5[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_24 (Dropout)           (None, 80)           0           ['gru[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 80)          320         ['dropout_24[0][0]']             \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 80)           0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 96)           7776        ['activation_25[0][0]']          \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 96)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            97          ['activation_26[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 212,705\n",
            "Trainable params: 211,265\n",
            "Non-trainable params: 1,440\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUAbPQFOV3mD",
        "outputId": "15daf1e7-19cf-49e4-8ae6-59d4268ae8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "import datetime"
      ],
      "metadata": {
        "id": "aIP6jd9lV5o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# So my GPU-memory is somewhat unstable and crashes for batchsize 16 or bigger"
      ],
      "metadata": {
        "id": "mo9tczmEapVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"best_weights23\",\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)"
      ],
      "metadata": {
        "id": "5W32fVNzHOgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "my_saluki.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.99, clipnorm=0.5),\n",
        "              loss=\"mse\")\n",
        "\n",
        "logdir = os.path.join(os.path.join(os.getcwd(), \"logs\"), datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1, write_graph=True)\n",
        "\n",
        "# Train the model\n",
        "history = my_saluki.fit(X_train, y_train,\n",
        "                        #X_train[0:200,:], y_train[0:200], \n",
        "                        validation_data=(X_val, y_val),\n",
        "                        #validation_data=(X_val[0:100,:], y_val[0:100]),\n",
        "                        callbacks=[EarlyStopping(patience=25, restore_best_weights=True),   \n",
        "                                   History(),\n",
        "                                   ExplainedVariance(validation_data=(X_val, y_val), interval=4),\n",
        "                                   tensorboard_callback,\n",
        "                                   model_checkpoint_callback],\n",
        "                        batch_size=32,  #they used 64\n",
        "                        epochs=1000)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t13N24J-V8eq",
        "outputId": "4368e144-0859-430a-9387-ad8a340d7335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "  6/275 [..............................] - ETA: 25s - loss: 3.0442WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0602s vs `on_train_batch_end` time: 0.0713s). Check your callbacks.\n",
            "275/275 [==============================] - ETA: 0s - loss: 2.6098interval evaluation - epoch: 0 - explained variance: -0.000717\n",
            "275/275 [==============================] - 51s 134ms/step - loss: 2.6098 - val_loss: 2.3271\n",
            "Epoch 2/1000\n",
            "275/275 [==============================] - 32s 115ms/step - loss: 2.2962 - val_loss: 2.2409\n",
            "Epoch 3/1000\n",
            "275/275 [==============================] - 37s 133ms/step - loss: 1.8260 - val_loss: 1.7609\n",
            "Epoch 4/1000\n",
            "275/275 [==============================] - 63s 230ms/step - loss: 1.3652 - val_loss: 1.9021\n",
            "Epoch 5/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 1.1057interval evaluation - epoch: 4 - explained variance: 0.296397\n",
            "275/275 [==============================] - 83s 303ms/step - loss: 1.1057 - val_loss: 1.0995\n",
            "Epoch 6/1000\n",
            "275/275 [==============================] - 102s 371ms/step - loss: 0.9422 - val_loss: 1.6949\n",
            "Epoch 7/1000\n",
            "275/275 [==============================] - 101s 368ms/step - loss: 0.8475 - val_loss: 0.8161\n",
            "Epoch 8/1000\n",
            "275/275 [==============================] - 103s 375ms/step - loss: 0.7851 - val_loss: 1.1114\n",
            "Epoch 9/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.7426interval evaluation - epoch: 8 - explained variance: 0.214637\n",
            "275/275 [==============================] - 115s 420ms/step - loss: 0.7426 - val_loss: 1.7193\n",
            "Epoch 10/1000\n",
            "275/275 [==============================] - 111s 404ms/step - loss: 0.7203 - val_loss: 2.2733\n",
            "Epoch 11/1000\n",
            "275/275 [==============================] - 114s 415ms/step - loss: 0.7102 - val_loss: 0.9392\n",
            "Epoch 12/1000\n",
            "275/275 [==============================] - 112s 407ms/step - loss: 0.7046 - val_loss: 0.8294\n",
            "Epoch 13/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6959interval evaluation - epoch: 12 - explained variance: 0.401444\n",
            "275/275 [==============================] - 111s 404ms/step - loss: 0.6959 - val_loss: 0.7630\n",
            "Epoch 14/1000\n",
            "275/275 [==============================] - 105s 380ms/step - loss: 0.6841 - val_loss: 0.6567\n",
            "Epoch 15/1000\n",
            "275/275 [==============================] - 107s 389ms/step - loss: 0.6774 - val_loss: 0.6788\n",
            "Epoch 16/1000\n",
            "275/275 [==============================] - 105s 382ms/step - loss: 0.6671 - val_loss: 2.9788\n",
            "Epoch 17/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6638interval evaluation - epoch: 16 - explained variance: 0.357234\n",
            "275/275 [==============================] - 108s 394ms/step - loss: 0.6638 - val_loss: 0.7301\n",
            "Epoch 18/1000\n",
            "275/275 [==============================] - 83s 303ms/step - loss: 0.6680 - val_loss: 1.6846\n",
            "Epoch 19/1000\n",
            "275/275 [==============================] - 81s 293ms/step - loss: 0.6524 - val_loss: 0.7055\n",
            "Epoch 20/1000\n",
            "275/275 [==============================] - 80s 293ms/step - loss: 0.6453 - val_loss: 0.9760\n",
            "Epoch 21/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6493interval evaluation - epoch: 20 - explained variance: 0.418645\n",
            "275/275 [==============================] - 94s 341ms/step - loss: 0.6493 - val_loss: 0.6427\n",
            "Epoch 22/1000\n",
            "275/275 [==============================] - 95s 347ms/step - loss: 0.6491 - val_loss: 0.6185\n",
            "Epoch 23/1000\n",
            "275/275 [==============================] - 97s 353ms/step - loss: 0.6337 - val_loss: 0.6878\n",
            "Epoch 24/1000\n",
            "275/275 [==============================] - 96s 351ms/step - loss: 0.6458 - val_loss: 0.7411\n",
            "Epoch 25/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6399interval evaluation - epoch: 24 - explained variance: 0.437478\n",
            "275/275 [==============================] - 99s 360ms/step - loss: 0.6399 - val_loss: 0.7659\n",
            "Epoch 26/1000\n",
            "275/275 [==============================] - 103s 377ms/step - loss: 0.6280 - val_loss: 0.7187\n",
            "Epoch 27/1000\n",
            "275/275 [==============================] - 86s 313ms/step - loss: 0.6233 - val_loss: 1.1375\n",
            "Epoch 28/1000\n",
            "275/275 [==============================] - 104s 380ms/step - loss: 0.6279 - val_loss: 1.3027\n",
            "Epoch 29/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6337interval evaluation - epoch: 28 - explained variance: 0.456273\n",
            "275/275 [==============================] - 96s 351ms/step - loss: 0.6337 - val_loss: 0.6484\n",
            "Epoch 30/1000\n",
            "275/275 [==============================] - 99s 360ms/step - loss: 0.6240 - val_loss: 0.7803\n",
            "Epoch 31/1000\n",
            "275/275 [==============================] - 104s 379ms/step - loss: 0.6326 - val_loss: 0.8946\n",
            "Epoch 32/1000\n",
            "275/275 [==============================] - 104s 377ms/step - loss: 0.6255 - val_loss: 1.0335\n",
            "Epoch 33/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6200interval evaluation - epoch: 32 - explained variance: 0.405375\n",
            "275/275 [==============================] - 108s 393ms/step - loss: 0.6200 - val_loss: 0.6594\n",
            "Epoch 34/1000\n",
            "275/275 [==============================] - 83s 302ms/step - loss: 0.6113 - val_loss: 0.8148\n",
            "Epoch 35/1000\n",
            "275/275 [==============================] - 87s 317ms/step - loss: 0.6117 - val_loss: 0.6454\n",
            "Epoch 36/1000\n",
            "275/275 [==============================] - 102s 371ms/step - loss: 0.6159 - val_loss: 0.9786\n",
            "Epoch 37/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6103interval evaluation - epoch: 36 - explained variance: 0.405440\n",
            "275/275 [==============================] - 97s 354ms/step - loss: 0.6103 - val_loss: 0.8038\n",
            "Epoch 38/1000\n",
            "275/275 [==============================] - 103s 376ms/step - loss: 0.6155 - val_loss: 0.5903\n",
            "Epoch 39/1000\n",
            "275/275 [==============================] - 104s 379ms/step - loss: 0.6054 - val_loss: 0.9729\n",
            "Epoch 40/1000\n",
            "275/275 [==============================] - 102s 371ms/step - loss: 0.6054 - val_loss: 0.6225\n",
            "Epoch 41/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.5987interval evaluation - epoch: 40 - explained variance: 0.407366\n",
            "275/275 [==============================] - 105s 383ms/step - loss: 0.5987 - val_loss: 0.7494\n",
            "Epoch 42/1000\n",
            "275/275 [==============================] - 80s 292ms/step - loss: 0.6098 - val_loss: 0.6075\n",
            "Epoch 43/1000\n",
            "275/275 [==============================] - 103s 376ms/step - loss: 0.6053 - val_loss: 0.7287\n",
            "Epoch 44/1000\n",
            "275/275 [==============================] - 100s 363ms/step - loss: 0.6002 - val_loss: 0.6058\n",
            "Epoch 45/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6024interval evaluation - epoch: 44 - explained variance: 0.428370\n",
            "275/275 [==============================] - 108s 395ms/step - loss: 0.6024 - val_loss: 0.7139\n",
            "Epoch 46/1000\n",
            "275/275 [==============================] - 100s 366ms/step - loss: 0.5996 - val_loss: 0.8206\n",
            "Epoch 47/1000\n",
            "275/275 [==============================] - 103s 374ms/step - loss: 0.5972 - val_loss: 1.4961\n",
            "Epoch 48/1000\n",
            "275/275 [==============================] - 104s 378ms/step - loss: 0.5926 - val_loss: 0.7161\n",
            "Epoch 49/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.5911interval evaluation - epoch: 48 - explained variance: 0.448085\n",
            "275/275 [==============================] - 112s 408ms/step - loss: 0.5911 - val_loss: 0.7856\n",
            "Epoch 50/1000\n",
            "275/275 [==============================] - 104s 380ms/step - loss: 0.5913 - val_loss: 0.9783\n",
            "Epoch 51/1000\n",
            "275/275 [==============================] - 106s 385ms/step - loss: 0.5961 - val_loss: 0.7861\n",
            "Epoch 52/1000\n",
            "275/275 [==============================] - 106s 385ms/step - loss: 0.5889 - val_loss: 0.5862\n",
            "Epoch 53/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.5909interval evaluation - epoch: 52 - explained variance: 0.434355\n",
            "275/275 [==============================] - 113s 412ms/step - loss: 0.5909 - val_loss: 0.6101\n",
            "Epoch 54/1000\n",
            "275/275 [==============================] - 106s 385ms/step - loss: 0.5968 - val_loss: 0.6792\n",
            "Epoch 55/1000\n",
            "275/275 [==============================] - 107s 390ms/step - loss: 0.5908 - val_loss: 0.9521\n",
            "Epoch 56/1000\n",
            "275/275 [==============================] - 115s 417ms/step - loss: 0.5840 - val_loss: 1.1255\n",
            "Epoch 57/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.5834interval evaluation - epoch: 56 - explained variance: 0.451995\n",
            "275/275 [==============================] - 126s 458ms/step - loss: 0.5834 - val_loss: 0.6717\n",
            "Epoch 58/1000\n",
            "275/275 [==============================] - 118s 429ms/step - loss: 0.5808 - val_loss: 0.5983\n",
            "Epoch 59/1000\n",
            "275/275 [==============================] - 119s 432ms/step - loss: 0.5843 - val_loss: 0.6352\n",
            "Epoch 60/1000\n",
            "275/275 [==============================] - 118s 429ms/step - loss: 0.5798 - val_loss: 0.6355\n",
            "Epoch 61/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.5785interval evaluation - epoch: 60 - explained variance: 0.453229\n",
            "275/275 [==============================] - 116s 424ms/step - loss: 0.5785 - val_loss: 0.6947\n",
            "Epoch 62/1000\n",
            "275/275 [==============================] - 120s 437ms/step - loss: 0.5764 - val_loss: 0.5726\n",
            "Epoch 63/1000\n",
            "275/275 [==============================] - 114s 416ms/step - loss: 0.5740 - val_loss: 0.5824\n",
            "Epoch 64/1000\n",
            "275/275 [==============================] - 116s 421ms/step - loss: 0.5816 - val_loss: 0.6053\n",
            "Epoch 65/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.5731interval evaluation - epoch: 64 - explained variance: 0.404795\n",
            "275/275 [==============================] - 112s 408ms/step - loss: 0.5731 - val_loss: 0.6855\n",
            "Epoch 66/1000\n",
            "275/275 [==============================] - 105s 381ms/step - loss: 0.5704 - val_loss: 1.4224\n",
            "Epoch 67/1000\n",
            "275/275 [==============================] - 104s 379ms/step - loss: 0.5787 - val_loss: 0.7080\n",
            "Epoch 68/1000\n",
            "275/275 [==============================] - 105s 383ms/step - loss: 0.5763 - val_loss: 0.5980\n",
            "Epoch 69/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.5718interval evaluation - epoch: 68 - explained variance: 0.407606\n",
            "275/275 [==============================] - 110s 402ms/step - loss: 0.5718 - val_loss: 0.6250\n",
            "Epoch 70/1000\n",
            "275/275 [==============================] - 104s 377ms/step - loss: 0.5716 - val_loss: 0.7384\n",
            "Epoch 71/1000\n",
            "275/275 [==============================] - 106s 384ms/step - loss: 0.5774 - val_loss: 0.5760\n",
            "Epoch 72/1000\n",
            "275/275 [==============================] - 91s 332ms/step - loss: 0.5747 - val_loss: 1.0382\n",
            "Epoch 73/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.5751interval evaluation - epoch: 72 - explained variance: 0.389717\n",
            "275/275 [==============================] - 84s 306ms/step - loss: 0.5751 - val_loss: 1.1571\n",
            "Epoch 74/1000\n",
            "275/275 [==============================] - 78s 285ms/step - loss: 0.5741 - val_loss: 0.6224\n",
            "Epoch 75/1000\n",
            "275/275 [==============================] - 75s 274ms/step - loss: 0.5731 - val_loss: 0.6354\n",
            "Epoch 76/1000\n",
            "275/275 [==============================] - 75s 272ms/step - loss: 0.5669 - val_loss: 0.7929\n",
            "Epoch 77/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.5692interval evaluation - epoch: 76 - explained variance: 0.417163\n",
            "275/275 [==============================] - 79s 288ms/step - loss: 0.5692 - val_loss: 1.6515\n",
            "Epoch 78/1000\n",
            "275/275 [==============================] - 77s 282ms/step - loss: 0.5630 - val_loss: 0.7743\n",
            "Epoch 79/1000\n",
            "275/275 [==============================] - 72s 263ms/step - loss: 0.5642 - val_loss: 1.0546\n",
            "Epoch 80/1000\n",
            "275/275 [==============================] - 75s 275ms/step - loss: 0.5578 - val_loss: 1.4016\n",
            "Epoch 81/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.5650interval evaluation - epoch: 80 - explained variance: 0.460642\n",
            "275/275 [==============================] - 79s 288ms/step - loss: 0.5650 - val_loss: 0.9301\n",
            "Epoch 82/1000\n",
            "275/275 [==============================] - 71s 260ms/step - loss: 0.5562 - val_loss: 0.6670\n",
            "Epoch 83/1000\n",
            "275/275 [==============================] - 71s 260ms/step - loss: 0.5595 - val_loss: 0.6025\n",
            "Epoch 84/1000\n",
            "275/275 [==============================] - 74s 271ms/step - loss: 0.5754 - val_loss: 0.9193\n",
            "Epoch 85/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.5617interval evaluation - epoch: 84 - explained variance: 0.457238\n",
            "275/275 [==============================] - 80s 290ms/step - loss: 0.5617 - val_loss: 0.8974\n",
            "Epoch 86/1000\n",
            "275/275 [==============================] - 77s 279ms/step - loss: 0.5646 - val_loss: 0.6294\n",
            "Epoch 87/1000\n",
            "275/275 [==============================] - 73s 267ms/step - loss: 0.5583 - val_loss: 0.6106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = my_saluki.predict(X_val)\n",
        "print(\"Explained Var Score: %.2f\" % explained_variance_score(y_val, y_pred))\n",
        "print(\"R2 Score: %.2f\" % r2_score(y_val, y_pred))"
      ],
      "metadata": {
        "id": "7hCHlQnlWPVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c65f0b3-0d11-4453-cabf-cfe844b82ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "67/67 [==============================] - 3s 41ms/step\n",
            "Explained Var Score: 0.46\n",
            "R2 Score: 0.46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_saluki.save('my_saluki5_23_v2.h5')"
      ],
      "metadata": {
        "id": "OKTg9jXZYFMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_loss(history):\n",
        "    fig, ax = plt.subplots(figsize = (5,5))\n",
        "    ax.plot(history['loss'][1:])\n",
        "    ax.plot(history['val_loss'][1:])\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('mean squared error')"
      ],
      "metadata": {
        "id": "76EPwtmyWN_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " plot_loss(history.history)"
      ],
      "metadata": {
        "id": "ser84xLLWSPq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "1428748d-0cce-4a26-8973-47b6f9f717c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}