{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN v28 overfit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarkusFranke/RNA-Half-life-for-tissues/blob/main/models-for-general-half-life/RNN_v28_overfit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKkQxNHexGtb"
      },
      "outputs": [],
      "source": [
        "from Bio import SeqIO #for parsing Fasta Files\n",
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "from kipoiseq.transforms.functional import one_hot, fixed_len\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from sklearn.metrics import explained_variance_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import backend as k\n",
        "from keras.callbacks import EarlyStopping, History\n",
        "from keras.models import Model\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow as tf\n",
        "import keras.layers as kl\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import subprocess\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we can't run this in google colab due to RAM limitations, I'm importing the data from disk. All the data should be available on google drive though, under the same filenames, either to be downloaded and run with a path to where the user saved them, or to be directly accessible by mounting the google drive (and setting a shortcut to our google drive data path in google drive)\n"
      ],
      "metadata": {
        "id": "ku26rqE7x06J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hl = pd.read_excel(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\kelley_et_al_corrected_hl.xlsx', skiprows=[0, 1])\n",
        "hl['zscore'] = zscore(hl['half-life (PC1)'])\n",
        "halflife = hl[[\"Ensembl Gene Id\", \"zscore\"]]\n",
        "hl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "7vRfq7bIxy4J",
        "outputId": "7dc50d4f-4780-462f-e492-fb3cbc080f11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Ensembl Gene Id  Gene name  half-life (PC1)  Bazzini_ActD_HEK293_1  \\\n",
              "0      ENSG00000000003     TSPAN6         8.660955               0.763166   \n",
              "1      ENSG00000000419       DPM1         2.241221               0.529938   \n",
              "2      ENSG00000000457      SCYL3        -6.929173              -0.798471   \n",
              "3      ENSG00000000460   C1orf112         0.440909               0.461228   \n",
              "4      ENSG00000000938        FGR        -0.943680               0.164310   \n",
              "...                ...        ...              ...                    ...   \n",
              "13916  ENSG00000284770       TBCE         2.218664               0.100281   \n",
              "13917  ENSG00000285077  ARHGAP11B        -3.262964              -0.733980   \n",
              "13918  ENSG00000288596    C8orf44         2.118850              -0.485957   \n",
              "13919  ENSG00000288701     PRRC2B         0.133147               0.133770   \n",
              "13920  ENSG00000288722       F8A1        -3.485607              -0.607463   \n",
              "\n",
              "       Bazzini_ActD_HeLa_1  Bazzini_ActD_RPE_1  Bazzini_4sU_K562_1  \\\n",
              "0                 0.258448            0.106486            1.019072   \n",
              "1                 0.222678           -0.040666           -0.284952   \n",
              "2                -0.894854           -1.039150           -1.444532   \n",
              "3                 0.195794           -0.739672           -0.123925   \n",
              "4                 0.112064            0.095773            0.045345   \n",
              "...                    ...                 ...                 ...   \n",
              "13916            -0.187624           -0.143422            0.201024   \n",
              "13917            -0.934478           -0.952570           -0.461339   \n",
              "13918            -0.320560           -0.332189            0.582473   \n",
              "13919             0.214899            0.144491            0.090991   \n",
              "13920            -0.419258           -0.378650           -0.775774   \n",
              "\n",
              "       Akimitsu_BrU_HeLa_1  Rinn_ActD_K562_1  Rinn_ActD_K562_2  ...  \\\n",
              "0                 2.022504          1.744745          1.783356  ...   \n",
              "1                 0.145097          0.866919          0.832768  ...   \n",
              "2                -1.287191         -1.006317         -1.062201  ...   \n",
              "3                 0.162538         -0.023056         -0.008479  ...   \n",
              "4                 0.024136         -0.209157         -0.223700  ...   \n",
              "...                    ...               ...               ...  ...   \n",
              "13916             0.933352          0.465403          0.451758  ...   \n",
              "13917            -0.940048          0.225157          0.161858  ...   \n",
              "13918            -0.614775          0.305297          0.336730  ...   \n",
              "13919             0.131956         -0.040831         -0.035576  ...   \n",
              "13920             0.400149         -1.060379         -1.033201  ...   \n",
              "\n",
              "       Gejman_4sU_GM12812_1  Gejman_4sU_GM12814_1  Gejman_4sU_GM12815_1  \\\n",
              "0                  1.037361              0.969166              1.209562   \n",
              "1                 -0.212424             -0.747989              0.371214   \n",
              "2                 -1.155096             -1.421651             -1.568912   \n",
              "3                  1.365208              1.017193             -0.239569   \n",
              "4                  0.722198              1.024313              1.484018   \n",
              "...                     ...                   ...                   ...   \n",
              "13916              1.082013              0.917651              0.897480   \n",
              "13917              0.256969              0.033071             -0.249782   \n",
              "13918              1.023526             -0.155662             -0.950892   \n",
              "13919             -0.091563             -0.021327             -0.017414   \n",
              "13920              0.433892              0.446224              1.344124   \n",
              "\n",
              "       Simon_4sU_K562_1  Simon_4sU_K562_2  Rissland_4sU_HEK293_1  \\\n",
              "0              2.080643          2.095154               0.674058   \n",
              "1              0.772595          0.712843              -0.350914   \n",
              "2             -1.308978         -1.311572              -0.387172   \n",
              "3              0.373929          0.380154              -0.063720   \n",
              "4             -0.375671         -0.384504               0.232924   \n",
              "...                 ...               ...                    ...   \n",
              "13916          0.866792          0.868080              -0.104068   \n",
              "13917         -1.116405         -1.078302              -1.495412   \n",
              "13918         -0.175193         -0.166760               2.505294   \n",
              "13919          0.088581          0.107441               0.345123   \n",
              "13920         -0.631782         -0.629741              -0.413725   \n",
              "\n",
              "       Rissland_4sU_HEK293_2  Rissland_4sU_HEK293_3  Rissland_4sU_HEK293_4  \\\n",
              "0                   1.120375               1.456258               1.791769   \n",
              "1                  -0.879247              -0.825603               0.109861   \n",
              "2                  -1.229226              -1.122749              -0.570002   \n",
              "3                  -0.450610              -0.805719               0.453957   \n",
              "4                   0.347933               0.266619               0.063565   \n",
              "...                      ...                    ...                    ...   \n",
              "13916              -0.169414              -0.404886              -0.165188   \n",
              "13917              -1.006317              -1.240272              -1.440629   \n",
              "13918               1.068162               1.307039               1.563308   \n",
              "13919              -0.188176              -0.119128               0.001050   \n",
              "13920              -0.071227              -0.105036              -0.197549   \n",
              "\n",
              "         zscore  \n",
              "0      1.807620  \n",
              "1      0.467763  \n",
              "2     -1.446182  \n",
              "3      0.092022  \n",
              "4     -0.196955  \n",
              "...         ...  \n",
              "13916  0.463055  \n",
              "13917 -0.681011  \n",
              "13918  0.442223  \n",
              "13919  0.027789  \n",
              "13920 -0.727478  \n",
              "\n",
              "[13921 rows x 58 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ensembl Gene Id</th>\n",
              "      <th>Gene name</th>\n",
              "      <th>half-life (PC1)</th>\n",
              "      <th>Bazzini_ActD_HEK293_1</th>\n",
              "      <th>Bazzini_ActD_HeLa_1</th>\n",
              "      <th>Bazzini_ActD_RPE_1</th>\n",
              "      <th>Bazzini_4sU_K562_1</th>\n",
              "      <th>Akimitsu_BrU_HeLa_1</th>\n",
              "      <th>Rinn_ActD_K562_1</th>\n",
              "      <th>Rinn_ActD_K562_2</th>\n",
              "      <th>...</th>\n",
              "      <th>Gejman_4sU_GM12812_1</th>\n",
              "      <th>Gejman_4sU_GM12814_1</th>\n",
              "      <th>Gejman_4sU_GM12815_1</th>\n",
              "      <th>Simon_4sU_K562_1</th>\n",
              "      <th>Simon_4sU_K562_2</th>\n",
              "      <th>Rissland_4sU_HEK293_1</th>\n",
              "      <th>Rissland_4sU_HEK293_2</th>\n",
              "      <th>Rissland_4sU_HEK293_3</th>\n",
              "      <th>Rissland_4sU_HEK293_4</th>\n",
              "      <th>zscore</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ENSG00000000003</td>\n",
              "      <td>TSPAN6</td>\n",
              "      <td>8.660955</td>\n",
              "      <td>0.763166</td>\n",
              "      <td>0.258448</td>\n",
              "      <td>0.106486</td>\n",
              "      <td>1.019072</td>\n",
              "      <td>2.022504</td>\n",
              "      <td>1.744745</td>\n",
              "      <td>1.783356</td>\n",
              "      <td>...</td>\n",
              "      <td>1.037361</td>\n",
              "      <td>0.969166</td>\n",
              "      <td>1.209562</td>\n",
              "      <td>2.080643</td>\n",
              "      <td>2.095154</td>\n",
              "      <td>0.674058</td>\n",
              "      <td>1.120375</td>\n",
              "      <td>1.456258</td>\n",
              "      <td>1.791769</td>\n",
              "      <td>1.807620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENSG00000000419</td>\n",
              "      <td>DPM1</td>\n",
              "      <td>2.241221</td>\n",
              "      <td>0.529938</td>\n",
              "      <td>0.222678</td>\n",
              "      <td>-0.040666</td>\n",
              "      <td>-0.284952</td>\n",
              "      <td>0.145097</td>\n",
              "      <td>0.866919</td>\n",
              "      <td>0.832768</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.212424</td>\n",
              "      <td>-0.747989</td>\n",
              "      <td>0.371214</td>\n",
              "      <td>0.772595</td>\n",
              "      <td>0.712843</td>\n",
              "      <td>-0.350914</td>\n",
              "      <td>-0.879247</td>\n",
              "      <td>-0.825603</td>\n",
              "      <td>0.109861</td>\n",
              "      <td>0.467763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENSG00000000457</td>\n",
              "      <td>SCYL3</td>\n",
              "      <td>-6.929173</td>\n",
              "      <td>-0.798471</td>\n",
              "      <td>-0.894854</td>\n",
              "      <td>-1.039150</td>\n",
              "      <td>-1.444532</td>\n",
              "      <td>-1.287191</td>\n",
              "      <td>-1.006317</td>\n",
              "      <td>-1.062201</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.155096</td>\n",
              "      <td>-1.421651</td>\n",
              "      <td>-1.568912</td>\n",
              "      <td>-1.308978</td>\n",
              "      <td>-1.311572</td>\n",
              "      <td>-0.387172</td>\n",
              "      <td>-1.229226</td>\n",
              "      <td>-1.122749</td>\n",
              "      <td>-0.570002</td>\n",
              "      <td>-1.446182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ENSG00000000460</td>\n",
              "      <td>C1orf112</td>\n",
              "      <td>0.440909</td>\n",
              "      <td>0.461228</td>\n",
              "      <td>0.195794</td>\n",
              "      <td>-0.739672</td>\n",
              "      <td>-0.123925</td>\n",
              "      <td>0.162538</td>\n",
              "      <td>-0.023056</td>\n",
              "      <td>-0.008479</td>\n",
              "      <td>...</td>\n",
              "      <td>1.365208</td>\n",
              "      <td>1.017193</td>\n",
              "      <td>-0.239569</td>\n",
              "      <td>0.373929</td>\n",
              "      <td>0.380154</td>\n",
              "      <td>-0.063720</td>\n",
              "      <td>-0.450610</td>\n",
              "      <td>-0.805719</td>\n",
              "      <td>0.453957</td>\n",
              "      <td>0.092022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENSG00000000938</td>\n",
              "      <td>FGR</td>\n",
              "      <td>-0.943680</td>\n",
              "      <td>0.164310</td>\n",
              "      <td>0.112064</td>\n",
              "      <td>0.095773</td>\n",
              "      <td>0.045345</td>\n",
              "      <td>0.024136</td>\n",
              "      <td>-0.209157</td>\n",
              "      <td>-0.223700</td>\n",
              "      <td>...</td>\n",
              "      <td>0.722198</td>\n",
              "      <td>1.024313</td>\n",
              "      <td>1.484018</td>\n",
              "      <td>-0.375671</td>\n",
              "      <td>-0.384504</td>\n",
              "      <td>0.232924</td>\n",
              "      <td>0.347933</td>\n",
              "      <td>0.266619</td>\n",
              "      <td>0.063565</td>\n",
              "      <td>-0.196955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13916</th>\n",
              "      <td>ENSG00000284770</td>\n",
              "      <td>TBCE</td>\n",
              "      <td>2.218664</td>\n",
              "      <td>0.100281</td>\n",
              "      <td>-0.187624</td>\n",
              "      <td>-0.143422</td>\n",
              "      <td>0.201024</td>\n",
              "      <td>0.933352</td>\n",
              "      <td>0.465403</td>\n",
              "      <td>0.451758</td>\n",
              "      <td>...</td>\n",
              "      <td>1.082013</td>\n",
              "      <td>0.917651</td>\n",
              "      <td>0.897480</td>\n",
              "      <td>0.866792</td>\n",
              "      <td>0.868080</td>\n",
              "      <td>-0.104068</td>\n",
              "      <td>-0.169414</td>\n",
              "      <td>-0.404886</td>\n",
              "      <td>-0.165188</td>\n",
              "      <td>0.463055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13917</th>\n",
              "      <td>ENSG00000285077</td>\n",
              "      <td>ARHGAP11B</td>\n",
              "      <td>-3.262964</td>\n",
              "      <td>-0.733980</td>\n",
              "      <td>-0.934478</td>\n",
              "      <td>-0.952570</td>\n",
              "      <td>-0.461339</td>\n",
              "      <td>-0.940048</td>\n",
              "      <td>0.225157</td>\n",
              "      <td>0.161858</td>\n",
              "      <td>...</td>\n",
              "      <td>0.256969</td>\n",
              "      <td>0.033071</td>\n",
              "      <td>-0.249782</td>\n",
              "      <td>-1.116405</td>\n",
              "      <td>-1.078302</td>\n",
              "      <td>-1.495412</td>\n",
              "      <td>-1.006317</td>\n",
              "      <td>-1.240272</td>\n",
              "      <td>-1.440629</td>\n",
              "      <td>-0.681011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13918</th>\n",
              "      <td>ENSG00000288596</td>\n",
              "      <td>C8orf44</td>\n",
              "      <td>2.118850</td>\n",
              "      <td>-0.485957</td>\n",
              "      <td>-0.320560</td>\n",
              "      <td>-0.332189</td>\n",
              "      <td>0.582473</td>\n",
              "      <td>-0.614775</td>\n",
              "      <td>0.305297</td>\n",
              "      <td>0.336730</td>\n",
              "      <td>...</td>\n",
              "      <td>1.023526</td>\n",
              "      <td>-0.155662</td>\n",
              "      <td>-0.950892</td>\n",
              "      <td>-0.175193</td>\n",
              "      <td>-0.166760</td>\n",
              "      <td>2.505294</td>\n",
              "      <td>1.068162</td>\n",
              "      <td>1.307039</td>\n",
              "      <td>1.563308</td>\n",
              "      <td>0.442223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13919</th>\n",
              "      <td>ENSG00000288701</td>\n",
              "      <td>PRRC2B</td>\n",
              "      <td>0.133147</td>\n",
              "      <td>0.133770</td>\n",
              "      <td>0.214899</td>\n",
              "      <td>0.144491</td>\n",
              "      <td>0.090991</td>\n",
              "      <td>0.131956</td>\n",
              "      <td>-0.040831</td>\n",
              "      <td>-0.035576</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.091563</td>\n",
              "      <td>-0.021327</td>\n",
              "      <td>-0.017414</td>\n",
              "      <td>0.088581</td>\n",
              "      <td>0.107441</td>\n",
              "      <td>0.345123</td>\n",
              "      <td>-0.188176</td>\n",
              "      <td>-0.119128</td>\n",
              "      <td>0.001050</td>\n",
              "      <td>0.027789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13920</th>\n",
              "      <td>ENSG00000288722</td>\n",
              "      <td>F8A1</td>\n",
              "      <td>-3.485607</td>\n",
              "      <td>-0.607463</td>\n",
              "      <td>-0.419258</td>\n",
              "      <td>-0.378650</td>\n",
              "      <td>-0.775774</td>\n",
              "      <td>0.400149</td>\n",
              "      <td>-1.060379</td>\n",
              "      <td>-1.033201</td>\n",
              "      <td>...</td>\n",
              "      <td>0.433892</td>\n",
              "      <td>0.446224</td>\n",
              "      <td>1.344124</td>\n",
              "      <td>-0.631782</td>\n",
              "      <td>-0.629741</td>\n",
              "      <td>-0.413725</td>\n",
              "      <td>-0.071227</td>\n",
              "      <td>-0.105036</td>\n",
              "      <td>-0.197549</td>\n",
              "      <td>-0.727478</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13921 rows × 58 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\Homo_sapiens.GRCh38.83.chosenTranscript.3pUTRs.fa') as fasta_file:  # Will close handle cleanly\n",
        "    UTR3_identifiers = []\n",
        "    UTR3_seqs = []\n",
        "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
        "        UTR3_identifiers.append(seq_record.id)\n",
        "        UTR3_seqs.append(seq_record.seq)\n",
        "\n",
        "with open(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\Homo_sapiens.GRCh38.83.chosenTranscript.5pUTRs.fa') as fasta_file:  # Will close handle cleanly\n",
        "    UTR5_identifiers = []\n",
        "    UTR5_seqs = []\n",
        "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
        "        UTR5_identifiers.append(seq_record.id)\n",
        "        UTR5_seqs.append(seq_record.seq)\n",
        "\n",
        "with open(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\Homo_sapiens.GRCh38.83.chosenTranscript.ORFs.fa') as fasta_file:  # Will close handle cleanly\n",
        "    ORF_identifiers = []\n",
        "    ORF_seqs = []\n",
        "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
        "        ORF_identifiers.append(seq_record.id)\n",
        "        ORF_seqs.append(seq_record.seq)\n",
        "\n",
        "print(UTR3_seqs[0])\n",
        "print(UTR5_seqs[0])\n",
        "print(ORF_seqs[0])\n",
        "print(UTR3_identifiers[0])\n",
        "print(UTR5_identifiers[0])\n",
        "print(ORF_identifiers[0])\n",
        "print(len(UTR3_seqs))\n",
        "print(len(UTR5_seqs))\n",
        "print(len(ORF_seqs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFU1gCykz-M7",
        "outputId": "fa1284a7-2097-473f-fd8a-4b831431c77c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AATATTATGTATGCAGCAATATTTGAGTAACAAGAAGCAAATATCCAAGTTCCAAAATTATAAAAGAAATTCTTATCCAAATAGTAATGTTCTAATTGATCATATAAGAAAGCAAAGCATAGACATTAGAATTATAAGTCAGCAGTGGTCTGTTCAAGAACAATCAACATTTTTAGAAAATAGTAGGACAAAATTAGGAAATAATTATCACCAAGAGGATCTAGTTCATGACTTTCTATTATCTCAATTAGATTGCTCAATCATCAGCCTTCCTATACTAAACTCTGATTCAGGACCAAGAAAGGCATAGTCTGACTCTGGAAATGCGCTGTTGGAAGCCAAATAACATCAATACTCTTGTTCTATAATTGAATATCAAATAAGACAAATTACCATTAATTTAATGACTGTGGAGTTAATTGTATACCAGCATTTCAGCAAATCATCATCAATAGTATTACATTAGCAATTTATGCAATTAAAAGGGCTTTGTAAAACTTTGAATAGATTTTATTGTCATTAGTAGCTGTTGGAACTTCATTATTATATAATGTTTTTGCAAACTTTAACTTTTTTCTAAATTGTTAAATAAAAGAATAACTATCCTTAATCTAAATAATTTTGGTAGCAAATCCTATAAGGTATTAAACATTTTAAGGTATATTATTACATTGCTATTTTACTGTTTCTCATTAACCCAAACAGTTTAAAGGCAGAATTCCACTTAGAAACAAGTTGCATTTTGAAAGTTTATTTGTAATCCATTTGTTTGGAATTCAGAAATGTATTTCACATAAAAATAATCTTGGAAGTAATAAATTCCAAAATTAACTAACAAAA\n",
            "AGATGAGATTTCATCATGTTGGCCAGCCTGGTCTCAAACTCCTGACCTCAAGTGACCCGCCTGCCTCAGCCTCCCAAAGTGCTGGGATTACAGGAATTTAGTGATTGACA\n",
            "ATGGCAGAAAAAATCCTAGAGAAGTTGGATGTCCTTGATAAGCAAGCAGAGATAATCTTGGCCAGAAGAACAAAGATAAACAGGCTTCAGAGTGAAGGAAGAAAAACAACTATGGCTATACCCCTGACATTTGATTTTCAGTTGGAATTTGAAGAAGCTCTTGCTACATCCGCGTCTAAGGCAATATCAAAGATCAAAGAAGACAAGTCATGCAGCATTACAAAATCAAAAATGCATGTCTCTTTCAAATGTGAGCCTGAACCTAGAAAGAGTAATTTTGAAAAGTCAAATTTAAGACCATTCTTTATTCAAACAAATGTAAAAAATAAAGAAAGTGAGTCAACAGCTCAAATTGAAAAAAAACCTAGGAAACCATTGGATTCTGTTGGTCTCTTAGAAGGTGATAGAAATAAAAGAAAAAAATCTCCACAGATGAACGATTTTAATATAAAAGAAAACAAATCGGTCAGAAATTATCAATTAAGTAAGTATAGGTCAGTAAGAAAGAAAAGCTTGCTCCCGTTGTGCTTTGAGGATGAATTGAAAAATCCACATGCCAAGATAGTCAACGTTAGTCCAACAAAGACAGTAACTTCTCACATGGAACAAAAGGACACAAATCCCATAATTTTCCATGACACAGAATATGTACGAATGTTACTTTTGACAAAAAATAGATTTTCTTCTCATCCTTTGGAAAATGAAAACATTTACCCACATAAAAGAACAAATTTCATTTTAGAAAGAAATTGTGAAATCCTCAAATCTATAATTGGCAATCAATCTATTTCTCTTTTCAAACCCCAAAAAACTATGCCTACAGTACAGAGAAAAGATATACAGATCCCTATGTCTTTTAAAGCGGGCCACACAACTGTAGATGATAAACTAAAGAAGAAAACTAATAAGCAGACACTAGAAAACAGATCTTGGAATACACTCTATAATTTCTCACAGAATTTTTCTAGCCTAACAAAACAATTTGTGGGTTACCTTGATAAAGCTGTTATTCATGAAATGAGTGCCCAAACTGGAAAATTTGAAAGAATGTTTTCTGCAGGAAAACCAACGAGCATACCCACATCCAGTGCCTTACCTGTCAAATGTTACTCAAAGCCTTTTAAATATATATATGAACTAAATAATGTAACGCCACTGGATAATTTGTTAAACTTATCAAATGAAATTTTAAATGCCTCA\n",
            "ENSG00000203963\n",
            "ENSG00000203963\n",
            "ENSG00000203963\n",
            "19094\n",
            "18642\n",
            "20253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exons = pd.read_csv(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\Kelley_et_al_exon_junctions.txt', delimiter = \"\\t\")\n",
        "exons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "HrhPuo0w0di6",
        "outputId": "d106cd7f-6f7d-46f7-da8f-e03ee82be0a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                GeneID  UTR5_len  \\\n",
              "0      ENSG00000000003       112   \n",
              "1      ENSG00000000457       222   \n",
              "2      ENSG00000000460       700   \n",
              "3      ENSG00000000938       289   \n",
              "4      ENSG00000000971       240   \n",
              "...                ...       ...   \n",
              "13225  ENSG00000278615        48   \n",
              "13226  ENSG00000278619       239   \n",
              "13227  ENSG00000278845       161   \n",
              "13228  ENSG00000280789       574   \n",
              "13229  ENSG00000281991       330   \n",
              "\n",
              "                         Exon_Junctions_In_Full_Sequence  \n",
              "0                                199,388,463,562,697,781  \n",
              "1      387,573,687,744,847,959,1037,1177,1362,1534,16...  \n",
              "2      766,871,1012,1178,1263,1402,1483,1548,1697,182...  \n",
              "3           515,618,717,821,971,1127,1307,1384,1538,1670  \n",
              "4      298,484,590,667,859,1030,1204,1399,1576,1759,1...  \n",
              "...                                                  ...  \n",
              "13225                                         87,212,310  \n",
              "13226                                  781,875,1008,1128  \n",
              "13227                        227,405,523,622,671,821,995  \n",
              "13228                                          1056,1139  \n",
              "13229                                                495  \n",
              "\n",
              "[13230 rows x 3 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GeneID</th>\n",
              "      <th>UTR5_len</th>\n",
              "      <th>Exon_Junctions_In_Full_Sequence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ENSG00000000003</td>\n",
              "      <td>112</td>\n",
              "      <td>199,388,463,562,697,781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENSG00000000457</td>\n",
              "      <td>222</td>\n",
              "      <td>387,573,687,744,847,959,1037,1177,1362,1534,16...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENSG00000000460</td>\n",
              "      <td>700</td>\n",
              "      <td>766,871,1012,1178,1263,1402,1483,1548,1697,182...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ENSG00000000938</td>\n",
              "      <td>289</td>\n",
              "      <td>515,618,717,821,971,1127,1307,1384,1538,1670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENSG00000000971</td>\n",
              "      <td>240</td>\n",
              "      <td>298,484,590,667,859,1030,1204,1399,1576,1759,1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13225</th>\n",
              "      <td>ENSG00000278615</td>\n",
              "      <td>48</td>\n",
              "      <td>87,212,310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13226</th>\n",
              "      <td>ENSG00000278619</td>\n",
              "      <td>239</td>\n",
              "      <td>781,875,1008,1128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13227</th>\n",
              "      <td>ENSG00000278845</td>\n",
              "      <td>161</td>\n",
              "      <td>227,405,523,622,671,821,995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13228</th>\n",
              "      <td>ENSG00000280789</td>\n",
              "      <td>574</td>\n",
              "      <td>1056,1139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13229</th>\n",
              "      <td>ENSG00000281991</td>\n",
              "      <td>330</td>\n",
              "      <td>495</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13230 rows × 3 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chromosomes = pd.read_csv(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\Kelley_et_al_chromosomes.txt', delimiter = \"\\t\")\n",
        "chromosomes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "N_zoThwx0u0w",
        "outputId": "8928266c-4b4a-4746-d5c0-1f20202eb99c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                GeneID  Chromosome\n",
              "0      ENSG00000186092           1\n",
              "1      ENSG00000279928           1\n",
              "2      ENSG00000279457           1\n",
              "3      ENSG00000278566           1\n",
              "4      ENSG00000273547           1\n",
              "...                ...         ...\n",
              "20290  ENSG00000277856  KI270726.1\n",
              "20291  ENSG00000275063  KI270726.1\n",
              "20292  ENSG00000271254  KI270711.1\n",
              "20293  ENSG00000277475  KI270713.1\n",
              "20294  ENSG00000268674  KI270713.1\n",
              "\n",
              "[20295 rows x 2 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GeneID</th>\n",
              "      <th>Chromosome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ENSG00000186092</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENSG00000279928</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENSG00000279457</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ENSG00000278566</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENSG00000273547</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20290</th>\n",
              "      <td>ENSG00000277856</td>\n",
              "      <td>KI270726.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20291</th>\n",
              "      <td>ENSG00000275063</td>\n",
              "      <td>KI270726.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20292</th>\n",
              "      <td>ENSG00000271254</td>\n",
              "      <td>KI270711.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20293</th>\n",
              "      <td>ENSG00000277475</td>\n",
              "      <td>KI270713.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20294</th>\n",
              "      <td>ENSG00000268674</td>\n",
              "      <td>KI270713.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20295 rows × 2 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chromosomes['Chromosome'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7fzASja1JI6",
        "outputId": "b840dda9-2f62-4b0c-94fa-3d94f43699c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1             2053\n",
              "19            1458\n",
              "11            1316\n",
              "2             1298\n",
              "17            1185\n",
              "3             1070\n",
              "6             1045\n",
              "12            1033\n",
              "7              980\n",
              "5              868\n",
              "16             865\n",
              "X              824\n",
              "14             824\n",
              "9              772\n",
              "4              747\n",
              "10             730\n",
              "8              670\n",
              "15             609\n",
              "20             541\n",
              "22             489\n",
              "13             320\n",
              "18             269\n",
              "21             233\n",
              "Y               54\n",
              "MT              13\n",
              "KI270728.1       6\n",
              "KI270727.1       4\n",
              "KI270734.1       3\n",
              "GL000194.1       2\n",
              "GL000195.1       2\n",
              "KI270726.1       2\n",
              "KI270713.1       2\n",
              "GL000009.2       1\n",
              "GL000205.2       1\n",
              "GL000219.1       1\n",
              "GL000213.1       1\n",
              "GL000218.1       1\n",
              "KI270731.1       1\n",
              "KI270721.1       1\n",
              "KI270711.1       1\n",
              "Name: Chromosome, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XRaMXVFv1YvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rbp_k = np.load(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\RBP_k.npy')\n",
        "rbp_k.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNwe4p0T4_u7",
        "outputId": "fb486317-c548-4fa4-b649-5774308f8c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13230, 59)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So not only do we have much more chromosomes, we the data comes from Pedro (df), Saluki (the sequences), from Pauline (chromosomes, exon junctions, via Saluki-chosen transcript), and from Yasmine (RBPs using Deepripe).\n",
        "Thus we should take good care that we merge the tables correctly."
      ],
      "metadata": {
        "id": "ITVKHQAf5c5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d = {'geneID': UTR5_identifiers, 'UTR5_seqs': UTR5_seqs}\n",
        "UTR5 = pd.DataFrame(data=d)\n",
        "d = {'geneID': ORF_identifiers, 'ORF_seqs': ORF_seqs}\n",
        "ORF = pd.DataFrame(data=d)\n",
        "d = {'geneID': UTR3_identifiers, 'UTR3_seqs': UTR3_seqs}\n",
        "UTR3 = pd.DataFrame(data=d)\n",
        "\n",
        "#merge every data frame to sequences\n",
        "halflife = hl[[\"Ensembl Gene Id\", \"zscore\"]]\n",
        "seqs = pd.merge(pd.merge(UTR5, ORF, on ='geneID'), UTR3, on = 'geneID')\n",
        "sequences = pd.merge(halflife, seqs, right_on = 'geneID', left_on = 'Ensembl Gene Id')\n",
        "sequences = sequences.drop(columns=[\"geneID\"])\n",
        "sequences = sequences.rename(columns={\"Ensembl Gene Id\": \"geneID\"})\n",
        "sequences = pd.merge(sequences, chromosomes, left_on='geneID', right_on='GeneID')\n",
        "sequences = sequences.drop(columns=[\"GeneID\"])\n",
        "sequences = pd.merge(sequences, exons, left_on='geneID', right_on='GeneID')\n",
        "sequences = sequences.drop(columns=[\"GeneID\"])\n",
        "\n",
        "#transform seqs into strings:\n",
        "sequences[\"UTR5_seqs\"] = sequences[\"UTR5_seqs\"].apply(str)\n",
        "sequences[\"UTR3_seqs\"] = sequences[\"UTR3_seqs\"].apply(str)\n",
        "sequences[\"ORF_seqs\"] = sequences[\"ORF_seqs\"].apply(str)\n",
        "\n",
        "rubbish = [d, UTR3, ORF, UTR5, UTR5_seqs, UTR3_seqs, ORF_seqs, UTR3_identifiers, UTR5_identifiers, ORF_identifiers, hl, halflife]\n",
        "del rubbish\n",
        "\n",
        "sequences.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "M7rLipk85dwm",
        "outputId": "73afc953-19cb-47e7-a521-7b1972415719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            geneID    zscore  \\\n",
              "0  ENSG00000000003  1.807620   \n",
              "1  ENSG00000000457 -1.446182   \n",
              "2  ENSG00000000460  0.092022   \n",
              "3  ENSG00000000938 -0.196955   \n",
              "4  ENSG00000000971  1.611324   \n",
              "\n",
              "                                           UTR5_seqs  \\\n",
              "0  AGTTGTGGACGCTCGTAAGTTTTCGGCAGTTTCCGGGGAGACTCGG...   \n",
              "1  TGTCCCGTTTCCGGACCCGTCTCTATGGTGTAGGAGAAACCCGGCC...   \n",
              "2  GGCTTTGGCCCTGGAAAGCCTCGCGGACGTGTTCTGACCCAAGGTT...   \n",
              "3  GGCTTGGGGCTAGGGCGTGACTGTCTCCCTGCCACCATCACCGCCC...   \n",
              "4  ACAGCATTAACATTTAGTGGGAGTGCAGTGAGAATTGGGTTTAACT...   \n",
              "\n",
              "                                            ORF_seqs  \\\n",
              "0  ATGGCGTCCCCGTCTCGGAGACTGCAGACTAAACCAGTCATTACTT...   \n",
              "1  ATGGGATCAGAGAACAGTGCTTTAAAGAGCTATACACTGAGAGAAC...   \n",
              "2  ATGTTTTTACCTCATATGAACCACCTGACATTGGAACAGACTTTCT...   \n",
              "3  ATGGGCTGTGTGTTCTGCAAGAAATTGGAGCCGGTGGCCACGGCCA...   \n",
              "4  ATGAGACTTCTAGCAAAGATTATTTGCCTTATGTTATGGGCTATTT...   \n",
              "\n",
              "                                           UTR3_seqs Chromosome  UTR5_len  \\\n",
              "0  CCCAATGTATCTGTGGGCCTATTCCTCTCTACCTTTAAGGACATTT...          X       112   \n",
              "1  CAATAGATGTGAGTTAAACTTTAGGAAAAAGGATTCCCTTTTTTTA...          1       222   \n",
              "2  AACTTATCACTAGGCAGAACTGGGTTTGATGCTTTGTCAACTGAAA...          1       700   \n",
              "3  CCTGTCCGGGCATCAACCCTCTCTGGCGGTGGCCACCAGTCCTTGC...          1       289   \n",
              "4  AATCAATCATAAAGTGCACACCTTTATTCAGAACTTTAGTATTAAA...          1       240   \n",
              "\n",
              "                     Exon_Junctions_In_Full_Sequence  \n",
              "0                            199,388,463,562,697,781  \n",
              "1  387,573,687,744,847,959,1037,1177,1362,1534,16...  \n",
              "2  766,871,1012,1178,1263,1402,1483,1548,1697,182...  \n",
              "3       515,618,717,821,971,1127,1307,1384,1538,1670  \n",
              "4  298,484,590,667,859,1030,1204,1399,1576,1759,1...  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>geneID</th>\n",
              "      <th>zscore</th>\n",
              "      <th>UTR5_seqs</th>\n",
              "      <th>ORF_seqs</th>\n",
              "      <th>UTR3_seqs</th>\n",
              "      <th>Chromosome</th>\n",
              "      <th>UTR5_len</th>\n",
              "      <th>Exon_Junctions_In_Full_Sequence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ENSG00000000003</td>\n",
              "      <td>1.807620</td>\n",
              "      <td>AGTTGTGGACGCTCGTAAGTTTTCGGCAGTTTCCGGGGAGACTCGG...</td>\n",
              "      <td>ATGGCGTCCCCGTCTCGGAGACTGCAGACTAAACCAGTCATTACTT...</td>\n",
              "      <td>CCCAATGTATCTGTGGGCCTATTCCTCTCTACCTTTAAGGACATTT...</td>\n",
              "      <td>X</td>\n",
              "      <td>112</td>\n",
              "      <td>199,388,463,562,697,781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENSG00000000457</td>\n",
              "      <td>-1.446182</td>\n",
              "      <td>TGTCCCGTTTCCGGACCCGTCTCTATGGTGTAGGAGAAACCCGGCC...</td>\n",
              "      <td>ATGGGATCAGAGAACAGTGCTTTAAAGAGCTATACACTGAGAGAAC...</td>\n",
              "      <td>CAATAGATGTGAGTTAAACTTTAGGAAAAAGGATTCCCTTTTTTTA...</td>\n",
              "      <td>1</td>\n",
              "      <td>222</td>\n",
              "      <td>387,573,687,744,847,959,1037,1177,1362,1534,16...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENSG00000000460</td>\n",
              "      <td>0.092022</td>\n",
              "      <td>GGCTTTGGCCCTGGAAAGCCTCGCGGACGTGTTCTGACCCAAGGTT...</td>\n",
              "      <td>ATGTTTTTACCTCATATGAACCACCTGACATTGGAACAGACTTTCT...</td>\n",
              "      <td>AACTTATCACTAGGCAGAACTGGGTTTGATGCTTTGTCAACTGAAA...</td>\n",
              "      <td>1</td>\n",
              "      <td>700</td>\n",
              "      <td>766,871,1012,1178,1263,1402,1483,1548,1697,182...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ENSG00000000938</td>\n",
              "      <td>-0.196955</td>\n",
              "      <td>GGCTTGGGGCTAGGGCGTGACTGTCTCCCTGCCACCATCACCGCCC...</td>\n",
              "      <td>ATGGGCTGTGTGTTCTGCAAGAAATTGGAGCCGGTGGCCACGGCCA...</td>\n",
              "      <td>CCTGTCCGGGCATCAACCCTCTCTGGCGGTGGCCACCAGTCCTTGC...</td>\n",
              "      <td>1</td>\n",
              "      <td>289</td>\n",
              "      <td>515,618,717,821,971,1127,1307,1384,1538,1670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENSG00000000971</td>\n",
              "      <td>1.611324</td>\n",
              "      <td>ACAGCATTAACATTTAGTGGGAGTGCAGTGAGAATTGGGTTTAACT...</td>\n",
              "      <td>ATGAGACTTCTAGCAAAGATTATTTGCCTTATGTTATGGGCTATTT...</td>\n",
              "      <td>AATCAATCATAAAGTGCACACCTTTATTCAGAACTTTAGTATTAAA...</td>\n",
              "      <td>1</td>\n",
              "      <td>240</td>\n",
              "      <td>298,484,590,667,859,1030,1204,1399,1576,1759,1...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 10000 #this is slightly longer than the 95% quantile, but lower than Saluki's implementation\n",
        "\n",
        "seqs = sequences['UTR5_seqs'] + sequences['ORF_seqs'] + sequences['UTR3_seqs']\n",
        "def pad_sequence(seqs, max_len, anchor='start', value='N'):\n",
        "  padded_seqs = [fixed_len(seq, max_len, anchor=anchor) for seq in seqs.astype(\"string\")]\n",
        "  return padded_seqs\n",
        "fixed_len_seqs = np.array(pad_sequence(seqs, max_len))\n",
        "print(fixed_len_seqs[0:4])\n",
        "del seqs\n",
        "print(fixed_len_seqs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8oUTj9w-XQg",
        "outputId": "05b15a0a-6caa-4b79-8c24-5b4c6cb1499b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AGTTGTGGACGCTCGTAAGTTTTCGGCAGTTTCCGGGGAGACTCGGGGACTCCGCGTCTCGCTCTCTGTGTTCCAATCGCCCGGTGCGGTGGTGCAGGGTCTCGGGCTAGTCATGGCGTCCCCGTCTCGGAGACTGCAGACTAAACCAGTCATTACTTGTTTCAAGAGCGTTCTGCTAATCTACACTTTTATTTTCTGGATCACTGGCGTTATCCTTCTTGCAGTTGGCATTTGGGGCAAGGTGAGCCTGGAGAATTACTTTTCTCTTTTAAATGAGAAGGCCACCAATGTCCCCTTCGTGCTCATTGCTACTGGTACCGTCATTATTCTTTTGGGCACCTTTGGTTGTTTTGCTACCTGCCGAGCTTCTGCATGGATGCTAAAACTGTATGCAATGTTTCTGACTCTCGTTTTTTTGGTCGAACTGGTCGCTGCCATCGTAGGATTTGTTTTCAGACATGAGATTAAGAACAGCTTTAAGAATAATTATGAGAAGGCTTTGAAGCAGTATAACTCTACAGGAGATTATAGAAGCCATGCAGTAGACAAGATCCAAAATACGTTGCATTGTTGTGGTGTCACCGATTATAGAGATTGGACAGATACTAATTATTACTCAGAAAAAGGATTTCCTAAGAGTTGCTGTAAACTTGAAGATTGTACTCCACAGAGAGATGCAGACAAAGTAAACAATGAAGGTTGTTTTATAAAGGTGATGACCATTATAGAGTCAGAAATGGGAGTCGTTGCAGGAATTTCCTTTGGAGTTGCTTGCTTCCAACTGATTGGAATCTTTCTCGCCTACTGCCTCTCTCGTGCCATAACAAATAACCAGTATGAGATAGTGCCCAATGTATCTGTGGGCCTATTCCTCTCTACCTTTAAGGACATTTAGGGTCCCCCCTGTGAATTAGAAAGTTGCTTGGCTGGAGAACTGACAACACTACTTACTGATAGACCAAAAAACTACACCAGTAGGTTGATTCAATCAAGATGTATGTAGACCTAAAACTACACCAATAGGCTGATTCAATCAAGATCCGTGCTCGCAGTGGGCTGATTCAATCAAGATGTATGTTTGCTATGTTCTAAGTCCACCTTCTATCCCATTCATGTTAGATCGTTGAAACCCTGTATCCCTCTGAAACACTGGAAGAGCTAGTAAATTGTAAATGAAGTAATACTGTGTTCCTCTTGACTGTTATTTTTCTTAGTAGGGGGCCTTTGGAAGGCACTGTGAATTTGCTATTTTGATGTAGTGTTACAAGATGGAAAATTGATTCCTCTGACTTTGCTATTGATGTAGTGTGATAGAAAATTCACCCCTCTGAACTGGCTCCTTCCCAGTCAAGGTTATCTGGTTTGATTGTATAATTTGCACCAAGAAGTTAAAATGTTTTATGACTCTCTGTTCTGCTGACAGGCAGAGAGTCACATTGTGTAATTTAATTTCAGTCAGTCAATAGATGGCATCCCTCATCAGGGTTGCCAGATGGTGATAACAGTGTAAGGCCTTGGGTCTAAGGCATCCACGACTGGAAGGGACTACTGATGTTCTGTGATACATCAGGTTTCAGCACACAACTTACATTTCTTTGCCTCCAAATTGAGGCATTTATTATGATGTTCATACTTTCCCTCTTGTTTGAAAGTTTCTAATTATTAAATGGTGTCGGAATTGTTGTATTTTCCTTAGGAATTCAGTGGAACTTATCTTCATTAAATTTAGCTGGTACCAGGTTGATATGACTTGTCAATATTATGGTCAACTTTAAGTCTTAGTTTTCGTTTGTGCCTTTGATTAATAAGTATAACTCTTATACAATAAATACTGCTTTCCTCTAAAAAGATCGTGTTTAAATTAACTTGTAGAAAATCTGCTGGAATGGTTGTTGTTTTCCACTGAGAAAGCTAAGCCCTACATTTCTATTCAGAGTACTGTTTTTAGATGTGAAATATAAGCCTGCGGCCTTAACTCTGTATTAAAAAAAATGTTTTTGTTTAAAAAAAACTGTTCCCATAGGTGCAGCAAACCACCATGGCACATGTATACCTATGTAACAAACCTGCACATTCTGCACATGTATCCCAGAACTTAATGTAAACAAAAAAATCTTAAAGTGCAAATATTAAAAAAAACTGTTCTCTGTGAAAAAAATTATATTCCATGTTATAAAGTAGCATATGACTAGTGTTCTCCTAGNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN'\n",
            " 'TGTCCCGTTTCCGGACCCGTCTCTATGGTGTAGGAGAAACCCGGCCCCCAGAAGATTGTGGGTGTAGTGGCCACAGCCTTACAGGCAGGCAGGGGTGGTTGGTGTCAACAGGGGGGCCAACAGGGTACCAGAGCCAAGACCCTCGGCCTCCTCCCCCGCCGCCTTCCTGCAGATCTGCTTGGCTTTGAGGAAGAGTGGCAGTACTGCCTCACTGCATAAGGGATGGGATCAGAGAACAGTGCTTTAAAGAGCTATACACTGAGAGAACCACCATTTACCTTACCCTCTGGACTTGCTGTTTATCCCGCTGTACTGCAAGATGGCAAATTTGCTTCAGTTTTTGTGTATAAGAGAGAAAATGAAGACAAGGTTAATAAAGCTGCCAAGCATTTGAAGACACTTCGTCACCCTTGCTTGCTAAGATTTTTATCTTGTACTGTGGAAGCGGATGGCATTCATCTTGTCACTGAGCGAGTACAGCCCCTGGAAGTGGCTTTGGAAACATTGTCTTCTGCAGAGGTCTGTGCTGGGATCTATGACATATTGCTGGCTCTTATCTTCCTTCATGACAGAGGACACCTAACACACAATAATGTCTGTTTATCATCTGTGTTTGTGAGTGAAGATGGACACTGGAAGCTAGGAGGAATGGAAACTGTTTGTAAAGTTTCTCAGGCCACACCAGAGTTTCTGAGGAGTATTCAGTCAATAAGAGACCCAGCATCTATCCCTCCTGAAGAGATGTCTCCAGAATTCACAACTCTCCCAGAGTGTCATGGACATGCCCGGGATGCCTTTTCATTTGGAACATTGGTGGAAAGTTTGCTCACAATCTTAAATGAACAGGTTTCAGCGGATGTTCTCTCCAGCTTTCAACAGACCTTGCACTCAACTTTGCTGAATCCCATTCCAAAATGTCGGCCAGCGCTCTGCACCTTACTATCTCATGACTTCTTCAGAAATGATTTTCTGGAAGTTGTGAATTTCTTGAAAAGTTTAACATTGAAGAGTGAAGAGGAGAAAACGGAATTCTTTAAATTTCTGCTGGACAGAGTCAGCTGCTTGTCAGAGGAATTGATAGCTTCAAGGTTGGTGCCTCTTCTGCTTAATCAGTTGGTGTTTGCAGAGCCAGTGGCTGTTAAGAGTTTTCTTCCTTATCTGCTTGGCCCCAAAAAAGATCATGCGCAGGGAGAAACTCCTTGCTTGCTCTCACCAGCCCTGTTCCAGTCACGGGTGATCCCCGTGCTTCTCCAGTTGTTTGAAGTTCATGAAGAGCATGTGCGGATGGTGCTGCTGTCTCACATCGAGGCCTACGTGGAGCACTTCACTCAGGAGCAGCTGAAGAAAGTCATCTTGCCACAGGTTTTGCTGGGCCTGCGTGATACTAGCGATTCCATTGTGGCAATTACTCTGCATAGCCTAGCAGTGCTGGTCTCTCTGCTTGGACCAGAGGTGGTTGTGGGAGGAGAACGAACCAAGATCTTCAAACGCACTGCCCCAAGTTTTACTAAAAATACTGACCTTTCTCTAGAAGATTCTCCTATGTGTGTCGTCTGCAGCCATCACAGTCAGATCTCGCCAATCTTGGAGAACCCCTTCTCTAGCATATTCCCTAAATGTTTCTTTTCTGGCAGCACGCCCATCAACAGCAAGAAGCACATACAGCGAGATTACTACAATACTCTTTTACAGACAGGCGATCCATTTTCTCAGCCTATTAAATTTCCCATAAATGGACTCTCAGATGTAAAAAATACTTCGGAGGACAGTGAAAACTTCCCATCAAGTTCTAAAAAGTCTGAGGAGTGGCCTGACTGGAGTGAACCTGAGGAGCCTGAAAATCAAACTGTCAACATACAGATTTGGCCTAGAGAACCTTGTGATGATGTCAAGTCCCAGTGCACTACCTTGGATGTGGAAGAGTCATCTTGGGATGACTGCGAGCCCAGCAGCTTAGATACTAAAGTAAACCCAGGAGGTGGAATCACTGCTACAAAACCTGTTACCTCAGGGGAGCAGAAGCCTATTCCTGCTTTGCTTTCACTCACTGAAGAGTCTATGCCTTGGAAATCAAGCTTACCCCAAAAGATTAGCCTTGTACAAAGGGGGGATGACGCAGACCAAATCGAGCCGCCAAAAGTGTCATCACAAGAAAGGCCCCTTAAGGTTCCATCAGAACTTGGTTTAGGAGAGGAATTCACCATTCAAGTAAAAAAGAAGCCAGTAAAAGATCCTGAGATGGATTGGTTTGCTGATATGATCCCAGAAATTAAGCCTTCTGCTGCTTTTCTTATATTACCTGAACTGAGGACAGAAATGGTCCCAAAAAAGGATGATGTCTCCCCAGTGATGCAGTTTTCCTCAAAATTTGCTGCAGCAGAAATTACTGAGGGAGAGGCTGAAGGCTGGGAAGAAGAAGGGGAGCTGAACTGGGAAGATAATAACTGGCAATAGATGTGAGTTAAACTTTAGGAAAAAGGATTCCCTTTTTTTAAAAAAAATCAATACCTCAAAAGCAGGCTTTGGGACAAGAAAACCCCAAAGTGGCCTGCTTTTCCCATCCCAGGAGCTCATTATCCAGTCTGTGCCAACTGAAGTAGGAGACTGACTGTGAGTGCTGGCTAAAAGCCCTGGGTGGTGAGGCTCACAGTACTGGTTTCCAGGAGGAAGAGCCTTTGTGCATTTGACTGAGGCCAGTTTCTATGAAGAGCAAGTAGCTGAGGAGAGGTCGAATTTACTGCTTTTTCCAGGACAATTCTGGAAGTAAAGAAAATGTAATTCAAGCTGGTTAGCTTAATTTTGTGCCATTCTTTAACATAAGAGTAAGCTCTATTATGAAATACAACTTTAAAAAATTTTAGCTATAAATTATATAAATGATTTTAAATTGCTGAGGTTTCCTTAGGCAGCTTATTTATTTGTTTACAGTTAGACTATCTGAGTAAATGGTTCTTTGTGGACCTAGGCAGTTCCTGACTGTTCCACATGTAGTACATTGTACCAAAGTTCTTAATAAGAATATTCCCCACAATCCTGTTCTCTAAATGTCAAATAAAGATTATTTTCACTAGATTCAACTTTACAAAANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN'\n",
            " 'GGCTTTGGCCCTGGAAAGCCTCGCGGACGTGTTCTGACCCAAGGTTTTAGCAGTGGATGTGGCGTTTTCTTCCATTCCTTCTTTCAGTTTTTCTGTACTCGTTGCTTGCAATTAAGTGTAAATACTTTTGCTAGTGGATAATGGGGGAGGCAAGGACTGAGACCTGCGGTATGACGATAGCTCTGGCTCTTAATAGTTTGAGGTAAAGCGAGATACTCTGAGCTTTTGTCTCCCGTAAAAAGGGTGGTGAATATGAATAAGGGCTTTCTTAGCGTTATAAGAATTAAAGGGCATAGTTCTGTGGTGTGAAATCTTTAAAAGATGTTCAGTAAATAAAAATGATTTTCCTCCTTCCCCTCTCAGACCTCTTTTTCTTCTTTCTTTCTTTTTTTTTGACAAGTTCTCACTCCTCTCACCCAGGCTGGAGTCTTTCTGAAAGAGTTCTTCCGCTTGTTGTTGGCTTTCAACTGTTGGATTTGAGGCGCTTAGCGCCTTCTTCGTCCGGGTGCAGCACATTCTTGATTGGTCTCATGCCTTTGTGGTTGTAAATGTGCCTGGAATCCTAGCCTTTCATGGTTTGTTCTGAGTAATGAATACCCTATTACTATGATACTAGTATCTTCCTTAATTATCCTACTCATTGTCTCAACATTCTGACAGTTGGATTGAGCATATTCAAATTTTGAAAATTATTGTAGAAATGTTTTTACCTCATATGAACCACCTGACATTGGAACAGACTTTCTTTTCACAAGTGTTACCAAAGACTGTGAAATTATTCGATGACATGATGTATGAATTAACCAGTCAAGCCAGAGGACTGTCAAGCCAAAATTTGGAAATCCAGACCACTCTAAGGAATATTTTACAAACAATGGTGCAGCTCTTAGGAGCTCTCACAGGATGTGTTCAGCATATCTGTGCCACACAGGAATCCATCATTTTGGAAAATATTCAGAGTCTCCCCTCCTCAGTCCTTCATATAATTAAAAGCACATTTGTGCATTGTAAGAATAGTGAATCTGTGTATTCTGGGTGTTTACACCTAGTTTCAGACCTTCTCCAGGCTCTTTTCAAGGAGGCCTATTCTCTTCAAAAGCAGTTAATGGAACTGCTGGACATGGTTTGCATGGACCCTTTAGTAGATGACAATGATGATATTTTGAATATGGTAATAGTTATTCATTCTTTATTGGATATCTGCTCTGTTATTTCCAGTATGGACCATGCATTTCATGCCAATACTTGGAAGTTTATAATTAAGCAGAGCCTTAAGCACCAGTCCATAATAAAAAGCCAGTTGAAACACAAAGATATAATTACTAGCTTGTGTGAAGACATTCTTTTCTCCTTCCATTCTTGTTTACAGTTAGCTGAGCAGATGACACAGTCAGATGCACAGGATAATGCTGACTACAGATTATTTCAGAAAACACTCAAATTGTGTCGTTTTTTTGCCAACTCCCTTTTGCACTACGCTAAGGAATTTCTTCCTTTCCTCTCTGATTCTTGCTGTACTTTGCACCAACTGTATCTTCAGATACACAGCAAGTTTCCTCCAAGCCTTTATGCTACCAGGATTTCTAAAGCACACCAAGAGGAAATAGCAGGTGCTTTCCTAGTGACACTGGATCCACTTATCAGTCAGCTGCTCACATTTCAGCCTTTCATGCAGGTGGTTTTGGACAGTAAATTAGACCTGCCATGTGAACTGCAGTTTCCACAATGTCTTCTTCTGGTTGTTGTCATGGATAAGCTGCCATCTCAGCCTAAGGAAGTGCAAACCCTGTGGTGCACAGACAGCCAGGTCTCAGAAACGACAACCAGGATATCTCTACTCAAAGCCGTTTTCTACAGTTTTGAGCAGTGTTCTGGTGAACTCTCTCTACCTGTTCATTTACAGGGATTAAAGAGTAAGGGGAAAGCTGAGGTGGCTGTCACCTTGTATCAGCATGTTTGTGTTCATCTGTGTACATTTATTACTTCCTTTCATCCCTCACTGTTTGCTGAACTGGATGCTGCTCTGCTGAATGCTGTACTTAGTGCTAATATGATCACCTCTTTGTTAGCTATGGATGCATGGTGCTTCCTTGCTCGATATGGGACTGCTGAACTGTGTGCACACCATGTCACCATAGTGGCTCATCTGATAAAGTCATGCCCTGGAGAATGTTATCAACTCATCAACCTATCAATACTGTTGAAGCGTCTCTTTTTCTTCATGGCACCACCCCATCAGCTGGAGTTTATCCAGAAATTTTCCCCAAAAGAAGCAGAAAATCTGCCTCTGTGGCAACATATTTCCTTCCAGGCGTTACCTCCTGAGCTTAGGGAACAAACTGTCCATGAGGTCACCACAGTAGGCACTGCAGAATGCAGGAAATGGCTGAGCAGGAGTCGTACTTTGGGAGAACTAGAATCTCTGAACACAGTACTGTCTGCTTTGCTTGCAGTATGTAATTCTGCTGGTGAAGCTTTGGATACAGGAAAACAAACTGCAATTATCGAAGTTGTGAGTCAGCTTTGGGCTTTTTTAAACATTAAACAGGTAGCAGATCAACCTTATGTTCAACAGACATTCAGCCTTTTACTTCCACTGTTGGGATTTTTCATTCAAACTCTAGATCCTAAACTGATACTTCAGGCAGTAACTTTGCAGACCTCGCTACTTAAATTAGAGCTTCCTGACTATGTTCGTTTGGCAATGTTGGATTTTGTATCTTCTTTAGGAAAACTTTTTATACCTGAAGCTATCCAGGACAGAATTCTGCCCAACCTGTCCTGTATGTTTGCCTTACTGCTAGCTGACAGGAGTTGGCTGCTAGAACAACATACCTTGGAGGCGTTTACTCAGTTCGCTGAGGGAACAAATCATGAAGAGATAGTTCCACAGTGTCTCAGTTCTGAAGAAACTAAGAACAAAGTTGTATCCTTTCTGGAGAAGACTGGGTTTGTAGATGAAACTGAAGCTGCCAAAGTGGAACGTGTGAAACAGGAAAAAGGTATTTTCTGGGAACCCTTTGCTAATGTGACTGTAGAAGAAGCAAAGAGGTCATCTTTACAGCCTTATGCAAAAAGAGCTCGTCAGGAGTTCCCCTGGGAAGAAGAGTACAGGTCAGCGCTGCATACAATAGCAGGGGCTTTGGAAGCAACTGAGTCACTACTCCAAAAGGGTCCTGCTCCAGCCTGGCTTTCAATGGAAATGGAGGCGCTCCAAGAAAGGATGGATAAGCTAAAACGTTACATACATACTCTAGGGAACTTATCACTAGGCAGAACTGGGTTTGATGCTTTGTCAACTGAAAATACTTATGTCTGTACATTTTCTAACAGATATAAAACAAATTTTGTAAAGTTGAATCTAGTGAAAATAATCTTTATTTGACATTTAGAGAACAGGATTGTGGGGAATATTCTTATTAAGAACTTTGGTACAATGTACTACATGTGGAACAGTCAGGAACTGCCTAGGTCCACAAAGAACCATTTACTCAGATAGTCTAACTGTAAACAAATAAATAAGCTGCCTAAGGAAACCTCAGCAATTTAAAATCATTTATATAATTTATAGCTAAAATTTTTTAAAGTTGTATTTCATAATAGAGCTTACTCTTATGTTAAAGAATGGCACAAAATTAAGCTAACCAGCTTGAATTACATTTTCTTTACTTCCAGAATTGTCCTGGAAAAAGCAGTAAATTCGACCTCTCCTCAGCTACTTGCTCTTCATAGAAACTGGCCTCAGTCAAATGCACAAAGGCTCTTCCTCCTGGAAACCAGTACTGTGAGCCTCACCACCCAGGGCTTTTAGCCAGCACTCACAGTCAGTCTCCTACTTCAGTTGGCACAGACTGGATAATGAGCTCCTGGGATGGGAAAAGCAGGCCACTTTGGGGTTTTCTTGTCCCAAAGCCTGCTTTTGAGGTATTGATTTTTTTTAAAAAAAGGGAATCCTTTTTCCTAAAGTTTAACTCACATCTATTGTCACCAGTTATTATCTTCCCAGTTCAGCTCCCCTTCTTCTTCCCAGCCTTCAGCCTCTCCCTGCAACAAAATAAAGCACACCAAGAACCCACTGAAACAAATCATATGCAAAAATCATACGCAAATTTGAAAAAGCAGGAATTTAAAATTTATCTTTTGATGCCAGAAACACTACCTCGTACTAAGTAAAATAACTTAGAGCTCTAACAGAAAGTTGAAAAGTAGGATTACAAAACCATGGCACTGGAAAATAGCTTTTCAAAAACCATAAAATCCAGTAAAAATCAGTGTGGATGACACCAAATCCTTATTTTAATCCCTTTTTTTTCTTTTTCAAATAAAAAGGTTACAATAGCTCATTAAACAAANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN'\n",
            " 'GGCTTGGGGCTAGGGCGTGACTGTCTCCCTGCCACCATCACCGCCCGCCGGCCGTGACTGCAATAAGAGAAGTCCGAGGCGGCTTCCTCCTCCCTGCCCAGCAGGGGCGGCGGTCAGAGGCGGGCAGCACCCCAGTTCTCCCCGCACGCCGGCACTCGCGGCTGCTGGAGCCCCGGCTGGCTCACCCCGGGGCCGGGCAGAATTGGGCTCCAGGTCTCTGACCCCTCCCAAGGATCATGCCGCAGCCCCACTGACCCAGGAGTAGGGGCCTAAGGGCAGGGAACCTGGAATGGGCTGTGTGTTCTGCAAGAAATTGGAGCCGGTGGCCACGGCCAAGGAGGATGCTGGCCTGGAAGGGGACTTCAGAAGCTACGGGGCAGCAGACCACTATGGGCCTGACCCCACTAAGGCCCGGCCTGCATCCTCATTTGCCCACATCCCCAACTACAGCAACTTCTCCTCTCAGGCCATCAACCCTGGCTTCCTTGATAGTGGCACCATCAGGGGTGTGTCAGGGATTGGGGTGACCCTGTTCATTGCCCTGTATGACTATGAGGCTCGAACTGAGGATGACCTCACCTTCACCAAGGGCGAGAAGTTCCACATCCTGAACAATACTGAAGGTGACTGGTGGGAGGCTCGGTCTCTCAGCTCCGGAAAAACTGGCTGCATTCCCAGCAACTACGTGGCCCCTGTTGACTCAATCCAAGCTGAAGAGTGGTACTTTGGAAAGATTGGGAGAAAGGATGCAGAGAGGCAGCTGCTTTCACCAGGCAACCCCCAGGGGGCCTTTCTCATTCGGGAAAGCGAGACCACCAAAGGTGCCTACTCCCTGTCCATCCGGGACTGGGATCAGACCAGAGGCGATCATGTGAAGCATTACAAGATCCGCAAACTGGACATGGGCGGCTACTACATCACCACACGGGTTCAGTTCAACTCGGTGCAGGAGCTGGTGCAGCACTACATGGAGGTGAATGACGGGCTGTGCAACCTGCTCATCGCGCCCTGCACCATCATGAAGCCGCAGACGCTGGGCCTGGCCAAGGACGCCTGGGAGATCAGCCGCAGCTCCATCACGCTGGAGCGCCGGCTGGGCACCGGCTGCTTCGGGGATGTGTGGCTGGGCACGTGGAACGGCAGCACTAAGGTGGCGGTGAAGACGCTGAAGCCGGGCACCATGTCCCCGAAGGCCTTCCTGGAGGAGGCGCAGGTCATGAAGCTGCTGCGGCACGACAAGCTGGTGCAGCTGTACGCCGTGGTGTCGGAGGAGCCCATCTACATCGTGACCGAGTTCATGTGTCACGGCAGCTTGCTGGATTTTCTCAAGAACCCAGAGGGCCAGGATTTGAGGCTGCCCCAATTGGTGGACATGGCAGCCCAGGTAGCTGAGGGCATGGCCTACATGGAACGCATGAACTACATTCACCGCGACCTGAGGGCAGCCAACATCCTGGTTGGGGAGCGGCTGGCGTGCAAGATCGCAGACTTTGGCTTGGCGCGTCTCATCAAGGACGATGAGTACAACCCCTGCCAAGGTTCCAAGTTCCCCATCAAGTGGACAGCCCCAGAAGCTGCCCTCTTTGGCAGATTCACCATCAAGTCAGACGTGTGGTCCTTTGGGATCCTGCTCACTGAGCTCATCACCAAGGGCCGAATCCCCTACCCAGGCATGAATAAACGGGAAGTGTTGGAACAGGTGGAGCAGGGCTACCACATGCCGTGCCCTCCAGGCTGCCCAGCATCCCTGTACGAGGCCATGGAACAGACCTGGCGTCTGGACCCGGAGGAGAGGCCTACCTTCGAGTACCTGCAGTCCTTCCTGGAGGACTACTTCACCTCCGCTGAACCACAGTACCAGCCCGGGGATCAGACACCTGTCCGGGCATCAACCCTCTCTGGCGGTGGCCACCAGTCCTTGCCAATCCCCAGAGCTGTTCTTCCAAAGCCCCCAGGCTGGCTTAGAACCCCATAGAGTCCTAGCATCACCGAGGACGTGGCTGCTCTGACACCACCTAGGGCAACCTACTTGTTTTACAGATGGGGCAAAAGGAGGCCCAGAGCTGATCTCTCATCCGCTCTGGCCCCAAGCACTATTTCTTCCTTTTCCACTTAGGCCCCTACATGCCTGTAGCCTTTCTCACTCCATCCCCACCCAAAGTGCTCAGACCTTGTCTAGTTATTTATAAAACTGTATGTACCTCCCTCACTTCTCTCCTATCACTGCTTTCCTACTCTCCTTTTATCTCACTCTAGTCCAGGTGCCAAGAATTTCCCTTCTACCCTCTATTCTCTTGTGTCTGTAAGTTACAAAGTCAGGAAAAGTCTTGGCTGGACCCCTTTCCTGCTGGGTGGATGCAGTGGTCCAGGACTGGGGTCTGGGCCCAGGTTTGAGGGAGAAGGTTGCAGAGCACTTCCCACCTCTCTGAATAGTGTGTATGTGTTGGTTTATTGATTCTGTAAATAAGTAAAATGACAATATGAATCCTCAAACCATGAAATACCCTTGAACCTTCCTTTGGGAGCGGGGGTGGTCAATAGGGGGTGAACGGACAGATATGGCTACAGGCAGCAGCAGGGGAAGCTGGAGAGGGCCCTAATGCCTACCAAGCACGGGGCATCCAAGGTGTGGAGTTTTAGAACACCCAGAGTCCCACTGCTCATCTGCACGTGAGTTTAGAAGACAAGCAGCTGAAGATACATTAAAATGTCCCCTTCGTTGCTGANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN']\n",
            "(13230,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot for track 1-4: the nucleotides \n",
        "\n",
        "one_hot_seqs = np.array([one_hot(seq, neutral_value=0) for seq in fixed_len_seqs])\n",
        "print(one_hot_seqs[0:2])\n",
        "print(one_hot_seqs.shape)\n",
        "rubbish = [fixed_len_seqs]\n",
        "del rubbish"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdyuPKVL_0QT",
        "outputId": "42452b31-5198-47aa-deaf-c40b70deb424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[1. 0. 0. 0.]\n",
            "  [0. 0. 1. 0.]\n",
            "  [0. 0. 0. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 1.]\n",
            "  [0. 0. 1. 0.]\n",
            "  [0. 0. 0. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]]]\n",
            "(13230, 10000, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot for track 5: the exon binding sites\n",
        "# dunno why, I guess I still suck at python, but this took me over an hours to code and bugfix\n",
        "# lol this is future me from the next day, this was wrong and I had redo all the training\n",
        "\n",
        "exons = []\n",
        "\n",
        "for i in range(len(sequences)):\n",
        "  onehot = np.repeat(0, repeats = max_len)\n",
        "  if(isinstance(sequences[\"Exon_Junctions_In_Full_Sequence\"][i], str)):\n",
        "    current_exons = list(map(int, sequences[\"Exon_Junctions_In_Full_Sequence\"][i].split(\",\")))\n",
        "    assert len(current_exons) > 0\n",
        "    positions_capped = [x for x in current_exons if x <= 10000] # delete all exon junctions after 10000 since we're capping the sequence there\n",
        "    onehot[positions_capped] = 1\n",
        "    '''\n",
        "    for j in current_exons:\n",
        "      positions = [x+len(sequences['UTR5_seqs'][i]) for x in current_exons] # have to add UTR5 length to indices\n",
        "      positions_capped = [x for x in positions if x <= 10000] # delete all exon junctions after 10000 since we're capping the sequence there\n",
        "      onehot[positions_capped] = 1 #exon junctions are 1 now\n",
        "      \n",
        "  if(isinstance(sequences[\"Exon_Junctions_In_Full_Sequence\"][i], float)):\n",
        "    if(not(math.isnan(sequences[\"Exon_Junctions_In_Full_Sequence\"][i]))):\n",
        "      onehot[int(sequences[\"Exon_Junctions_In_Full_Sequence\"][i])+len(sequences['UTR5_seqs'][i])] = 1\n",
        "    '''\n",
        "  exons.append(onehot)"
      ],
      "metadata": {
        "id": "27vK8R2SAD9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(map(int, sequences[\"Exon_Junctions_In_Full_Sequence\"][1].split(\",\"))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDsHa06rCEQq",
        "outputId": "bfc9c3ab-5364-4305-c41c-a5e594556523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[387, 573, 687, 744, 847, 959, 1037, 1177, 1362, 1534, 1696, 2391]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#one hot for track 6: Marking the beginning of each codon with 1\n",
        "starts = []\n",
        "for i in range(len(sequences)):\n",
        "  #assert len(sequences['ORF_seqs'].astype(\"string\")[i]) % 3 == 0 \n",
        "  lst = list(range(len(sequences['ORF_seqs'].astype(\"string\")[i])))\n",
        "  onehot = np.repeat(0, repeats = len(sequences['ORF_seqs'].astype(\"string\")[i]))\n",
        "  onehot[lst[0::3]] = 1\n",
        "  full = np.concatenate((np.repeat([0], repeats = len(sequences['UTR5_seqs'].astype(\"string\")[i])),\n",
        "                         onehot,\n",
        "                         np.repeat([0], repeats = len(sequences['UTR3_seqs'].astype(\"string\")[i]))), axis=None)\n",
        "  if (len(full) > max_len):\n",
        "    full = full[:max_len]\n",
        "  elif (len(full) < max_len):\n",
        "    full = np.concatenate((full, np.repeat(0, repeats = max_len - len(full))),axis = None)\n",
        "  starts.append(full)"
      ],
      "metadata": {
        "id": "pRuNTsz4AHLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rubbish = [fixed_len_seqs]\n",
        "\n",
        "rubbish = [fixed_len_seqs, d, UTR3, ORF, UTR5, UTR5_seqs, UTR3_seqs, ORF_seqs, hl,\n",
        "           UTR3_identifiers, UTR5_identifiers, ORF_identifiers, halflife, max_len]\n",
        "del rubbish"
      ],
      "metadata": {
        "id": "lt-QVEBiP5Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This takes about 18 GB, so beware of that\n",
        "onehot = np.concatenate((one_hot_seqs,np.array(exons)[:, :, None], np.array(starts)[:, :, None]), axis = 2)\n",
        "print(onehot.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZpZoaGuPp3c",
        "outputId": "0459924b-8f9b-41d7-9913-3b55a6400cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13230, 10000, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del exons\n",
        "del starts"
      ],
      "metadata": {
        "id": "8qpC7Tr5RZae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for The Train-Val-Test split we split as recommended on Chromosomes:"
      ],
      "metadata": {
        "id": "oM3U9VTxSy0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chrom_val = ['2', '3', '4']\n",
        "chrom_test = ['1', '8', '9']"
      ],
      "metadata": {
        "id": "Ih4OOJxISyJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_test = np.where(sequences.Chromosome.isin(chrom_test))[0]\n",
        "idx_val = np.where(sequences.Chromosome.isin(chrom_val))[0]\n",
        "idx_train = np.where(~(sequences.Chromosome.isin(chrom_test)| sequences.Chromosome.isin(chrom_val)))[0]"
      ],
      "metadata": {
        "id": "FQcQCI4BTFSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(array, idx_train, idx_val, idx_test):\n",
        "  return array[idx_train], array[idx_val], array[idx_test]"
      ],
      "metadata": {
        "id": "k7GWuUtuTJxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(idx_test))\n",
        "print(len(idx_val))\n",
        "print(len(idx_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOrm43s0Z8M2",
        "outputId": "7b761d44-f30a-45d9-da16-e7b01bfac61b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2334\n",
            "2118\n",
            "8778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, X_test = train_test_split(onehot, idx_train, idx_val, idx_test)\n",
        "y_train, y_val, y_test = train_test_split(sequences['zscore'].values, idx_train, idx_val, idx_test)"
      ],
      "metadata": {
        "id": "mmpWExHRTfMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExplainedVariance(keras.callbacks.Callback):\n",
        "    def __init__(self, validation_data=(), interval=10):\n",
        "        super(keras.callbacks.Callback, self).__init__()\n",
        "\n",
        "        self.interval = interval\n",
        "        self.X_val, self.y_val = validation_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.interval == 0:\n",
        "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "            var_score = explained_variance_score(self.y_val, y_pred)\n",
        "            #r2 = r2_score(self.y_val, y_pred)\n",
        "            del y_pred\n",
        "            #print(\" - interval evaluation - epoch: {:d} - explained variance: {:.6f} - R2: {:.6f}\".format(epoch, var_score, r2))\n",
        "            print(\"interval evaluation - epoch: {:d} - explained variance: {:.6f}\".format(epoch, var_score))\n",
        "        gc.collect()"
      ],
      "metadata": {
        "id": "xf5ZZA63U-qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saluki-Type model\n",
        "input = kl.Input((X_train.shape[1:]))\n",
        "\n",
        "x = kl.Conv1D(96, kernel_size=5, activation=None, kernel_regularizer=tf.keras.regularizers.l2(l=0.001))(input)\n",
        "x = tfa.layers.GroupNormalization(groups = 1, axis = 2)(x)\n",
        "x = kl.Activation(\"relu\")(x)\n",
        "\n",
        "for i in range(6):\n",
        "\n",
        "  x0 = kl.Conv1D(16, kernel_size=7, padding=\"same\", activation=None, kernel_regularizer=tf.keras.regularizers.l2(l=0.001))(x)\n",
        "  x0 = kl.MaxPooling1D(pool_size=2)(x0)\n",
        "  x0 = tfa.layers.GroupNormalization(groups = 1, axis = 2)(x0)\n",
        "  x0 = kl.Activation(\"relu\")(x0)\n",
        "  x0 = kl.Dropout(0.25)(x0)\n",
        "\n",
        "  x1 = kl.Conv1D(32, kernel_size=5, padding=\"same\", activation=None, kernel_regularizer=tf.keras.regularizers.l2(l=0.001))(x)\n",
        "  x1 = kl.MaxPooling1D(pool_size=2)(x1)\n",
        "  x1 = tfa.layers.GroupNormalization(groups = 1, axis = 2)(x1)\n",
        "  x1 = kl.Activation(\"relu\")(x1)\n",
        "  x1 = kl.Dropout(0.25)(x1)\n",
        "\n",
        "  x2 = kl.Conv1D(32, kernel_size=3, padding = \"same\", activation=None, kernel_regularizer=tf.keras.regularizers.l2(l=0.001))(x)\n",
        "  x2 = kl.MaxPooling1D(pool_size=2)(x2)\n",
        "  x2 = tfa.layers.GroupNormalization(groups = 1, axis = 2)(x2)\n",
        "  x2 = kl.Activation(\"relu\")(x2)\n",
        "  x2 = kl.Dropout(0.25)(x2)\n",
        "\n",
        "  x3 = kl.MaxPooling1D(pool_size=2)(x)\n",
        "  x3 = kl.Conv1D(32, kernel_size=1, padding='same', activation=None, kernel_regularizer=tf.keras.regularizers.l2(l=0.001))(x3)\n",
        "  x3 = tfa.layers.GroupNormalization(groups = 1, axis = 2)(x3)\n",
        "  x3 = kl.Activation(\"relu\")(x3)\n",
        "  x3 = kl.Dropout(0.25)(x3)\n",
        "\n",
        "  x4 = kl.Conv1D(16, kernel_size=1, padding='same', activation=None, kernel_regularizer=tf.keras.regularizers.l2(l=0.001))(x)\n",
        "  x4 = kl.MaxPooling1D(pool_size=2)(x4)\n",
        "  x4 = tfa.layers.GroupNormalization(groups = 1, axis = 2)(x4)\n",
        "  x4 = kl.Activation(\"relu\")(x4)\n",
        "  x4 = kl.Dropout(0.25)(x4)\n",
        "\n",
        "  x = kl.concatenate([x0, x1, x2, x3, x4], axis = 2)\n",
        "\n",
        "x = kl.LSTM(128, activation='tanh', recurrent_activation='sigmoid', go_backwards=True, kernel_regularizer=tf.keras.regularizers.l2(l=0.001))(x)\n",
        "x = kl.LayerNormalization()(x)\n",
        "x = kl.Dropout(0.25)(x)\n",
        "\n",
        "x = kl.Dense(128, kernel_regularizer=tf.keras.regularizers.l2(l=0.001))(x) #backwards so it encounters padding first\n",
        "#x = kl.Dropout(0.3)(x)\n",
        "#x = kl.BatchNormalization()(x) #batch is fine here, no padding to consider anymore\n",
        "x = kl.Activation(\"relu\")(x)\n",
        "\n",
        "output = kl.Dense(units=1)(x)\n",
        "\n",
        "my_saluki = Model(inputs=input, outputs=output)\n",
        "my_saluki.summary()\n",
        "\n",
        "#Saluki: We chose layer normalization over batch normalization because most of the 3′ positions are zero padded and would confuse the batch statistics.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0vIgaHIVpyI",
        "outputId": "54eb6c5f-b206-4fad-b988-a4bfbe7500b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 10000, 6)]   0           []                               \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 9996, 96)     2976        ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " group_normalization (GroupNorm  (None, 9996, 96)    192         ['conv1d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 9996, 96)     0           ['group_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 9996, 16)     10768       ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 9996, 32)     15392       ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 9996, 32)     9248        ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling1d_3 (MaxPooling1D)  (None, 4998, 96)    0           ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 9996, 16)     1552        ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 4998, 16)     0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 4998, 32)    0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 4998, 32)    0           ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 4998, 32)     3104        ['max_pooling1d_3[0][0]']        \n",
            "                                                                                                  \n",
            " max_pooling1d_4 (MaxPooling1D)  (None, 4998, 16)    0           ['conv1d_5[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_1 (GroupNo  (None, 4998, 16)    32          ['max_pooling1d[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " group_normalization_2 (GroupNo  (None, 4998, 32)    64          ['max_pooling1d_1[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " group_normalization_3 (GroupNo  (None, 4998, 32)    64          ['max_pooling1d_2[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " group_normalization_4 (GroupNo  (None, 4998, 32)    64          ['conv1d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " group_normalization_5 (GroupNo  (None, 4998, 16)    32          ['max_pooling1d_4[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 4998, 16)     0           ['group_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 4998, 32)     0           ['group_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 4998, 32)     0           ['group_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 4998, 32)     0           ['group_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 4998, 16)     0           ['group_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 4998, 16)     0           ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 4998, 32)     0           ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 4998, 32)     0           ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 4998, 32)     0           ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 4998, 16)     0           ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 4998, 128)    0           ['dropout[0][0]',                \n",
            "                                                                  'dropout_1[0][0]',              \n",
            "                                                                  'dropout_2[0][0]',              \n",
            "                                                                  'dropout_3[0][0]',              \n",
            "                                                                  'dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 4998, 16)     14352       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 4998, 32)     20512       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 4998, 32)     12320       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling1d_8 (MaxPooling1D)  (None, 2499, 128)   0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 4998, 16)     2064        ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling1d_5 (MaxPooling1D)  (None, 2499, 16)    0           ['conv1d_6[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_6 (MaxPooling1D)  (None, 2499, 32)    0           ['conv1d_7[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_7 (MaxPooling1D)  (None, 2499, 32)    0           ['conv1d_8[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 2499, 32)     4128        ['max_pooling1d_8[0][0]']        \n",
            "                                                                                                  \n",
            " max_pooling1d_9 (MaxPooling1D)  (None, 2499, 16)    0           ['conv1d_10[0][0]']              \n",
            "                                                                                                  \n",
            " group_normalization_6 (GroupNo  (None, 2499, 16)    32          ['max_pooling1d_5[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " group_normalization_7 (GroupNo  (None, 2499, 32)    64          ['max_pooling1d_6[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " group_normalization_8 (GroupNo  (None, 2499, 32)    64          ['max_pooling1d_7[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " group_normalization_9 (GroupNo  (None, 2499, 32)    64          ['conv1d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " group_normalization_10 (GroupN  (None, 2499, 16)    32          ['max_pooling1d_9[0][0]']        \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 2499, 16)     0           ['group_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 2499, 32)     0           ['group_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 2499, 32)     0           ['group_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 2499, 32)     0           ['group_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 2499, 16)     0           ['group_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 2499, 16)     0           ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 2499, 32)     0           ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 2499, 32)     0           ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 2499, 32)     0           ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 2499, 16)     0           ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 2499, 128)    0           ['dropout_5[0][0]',              \n",
            "                                                                  'dropout_6[0][0]',              \n",
            "                                                                  'dropout_7[0][0]',              \n",
            "                                                                  'dropout_8[0][0]',              \n",
            "                                                                  'dropout_9[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 2499, 16)     14352       ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 2499, 32)     20512       ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 2499, 32)     12320       ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_13 (MaxPooling1D  (None, 1249, 128)   0           ['concatenate_1[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 2499, 16)     2064        ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_10 (MaxPooling1D  (None, 1249, 16)    0           ['conv1d_11[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling1d_11 (MaxPooling1D  (None, 1249, 32)    0           ['conv1d_12[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling1d_12 (MaxPooling1D  (None, 1249, 32)    0           ['conv1d_13[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 1249, 32)     4128        ['max_pooling1d_13[0][0]']       \n",
            "                                                                                                  \n",
            " max_pooling1d_14 (MaxPooling1D  (None, 1249, 16)    0           ['conv1d_15[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " group_normalization_11 (GroupN  (None, 1249, 16)    32          ['max_pooling1d_10[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_12 (GroupN  (None, 1249, 32)    64          ['max_pooling1d_11[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_13 (GroupN  (None, 1249, 32)    64          ['max_pooling1d_12[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_14 (GroupN  (None, 1249, 32)    64          ['conv1d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_15 (GroupN  (None, 1249, 16)    32          ['max_pooling1d_14[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1249, 16)     0           ['group_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1249, 32)     0           ['group_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 1249, 32)     0           ['group_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 1249, 32)     0           ['group_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 1249, 16)     0           ['group_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 1249, 16)     0           ['activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 1249, 32)     0           ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 1249, 32)     0           ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 1249, 32)     0           ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)           (None, 1249, 16)     0           ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 1249, 128)    0           ['dropout_10[0][0]',             \n",
            "                                                                  'dropout_11[0][0]',             \n",
            "                                                                  'dropout_12[0][0]',             \n",
            "                                                                  'dropout_13[0][0]',             \n",
            "                                                                  'dropout_14[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)             (None, 1249, 16)     14352       ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_17 (Conv1D)             (None, 1249, 32)     20512       ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_18 (Conv1D)             (None, 1249, 32)     12320       ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_18 (MaxPooling1D  (None, 624, 128)    0           ['concatenate_2[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_20 (Conv1D)             (None, 1249, 16)     2064        ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_15 (MaxPooling1D  (None, 624, 16)     0           ['conv1d_16[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling1d_16 (MaxPooling1D  (None, 624, 32)     0           ['conv1d_17[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling1d_17 (MaxPooling1D  (None, 624, 32)     0           ['conv1d_18[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_19 (Conv1D)             (None, 624, 32)      4128        ['max_pooling1d_18[0][0]']       \n",
            "                                                                                                  \n",
            " max_pooling1d_19 (MaxPooling1D  (None, 624, 16)     0           ['conv1d_20[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " group_normalization_16 (GroupN  (None, 624, 16)     32          ['max_pooling1d_15[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_17 (GroupN  (None, 624, 32)     64          ['max_pooling1d_16[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_18 (GroupN  (None, 624, 32)     64          ['max_pooling1d_17[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_19 (GroupN  (None, 624, 32)     64          ['conv1d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_20 (GroupN  (None, 624, 16)     32          ['max_pooling1d_19[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 624, 16)      0           ['group_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 624, 32)      0           ['group_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 624, 32)      0           ['group_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 624, 32)      0           ['group_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 624, 16)      0           ['group_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 624, 16)      0           ['activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 624, 32)      0           ['activation_17[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)           (None, 624, 32)      0           ['activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)           (None, 624, 32)      0           ['activation_19[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 624, 16)      0           ['activation_20[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 624, 128)     0           ['dropout_15[0][0]',             \n",
            "                                                                  'dropout_16[0][0]',             \n",
            "                                                                  'dropout_17[0][0]',             \n",
            "                                                                  'dropout_18[0][0]',             \n",
            "                                                                  'dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_21 (Conv1D)             (None, 624, 16)      14352       ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_22 (Conv1D)             (None, 624, 32)      20512       ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_23 (Conv1D)             (None, 624, 32)      12320       ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_23 (MaxPooling1D  (None, 312, 128)    0           ['concatenate_3[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_25 (Conv1D)             (None, 624, 16)      2064        ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_20 (MaxPooling1D  (None, 312, 16)     0           ['conv1d_21[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling1d_21 (MaxPooling1D  (None, 312, 32)     0           ['conv1d_22[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling1d_22 (MaxPooling1D  (None, 312, 32)     0           ['conv1d_23[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_24 (Conv1D)             (None, 312, 32)      4128        ['max_pooling1d_23[0][0]']       \n",
            "                                                                                                  \n",
            " max_pooling1d_24 (MaxPooling1D  (None, 312, 16)     0           ['conv1d_25[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " group_normalization_21 (GroupN  (None, 312, 16)     32          ['max_pooling1d_20[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_22 (GroupN  (None, 312, 32)     64          ['max_pooling1d_21[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_23 (GroupN  (None, 312, 32)     64          ['max_pooling1d_22[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_24 (GroupN  (None, 312, 32)     64          ['conv1d_24[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_25 (GroupN  (None, 312, 16)     32          ['max_pooling1d_24[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 312, 16)      0           ['group_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 312, 32)      0           ['group_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 312, 32)      0           ['group_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 312, 32)      0           ['group_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 312, 16)      0           ['group_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_20 (Dropout)           (None, 312, 16)      0           ['activation_21[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_21 (Dropout)           (None, 312, 32)      0           ['activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_22 (Dropout)           (None, 312, 32)      0           ['activation_23[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_23 (Dropout)           (None, 312, 32)      0           ['activation_24[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_24 (Dropout)           (None, 312, 16)      0           ['activation_25[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 312, 128)     0           ['dropout_20[0][0]',             \n",
            "                                                                  'dropout_21[0][0]',             \n",
            "                                                                  'dropout_22[0][0]',             \n",
            "                                                                  'dropout_23[0][0]',             \n",
            "                                                                  'dropout_24[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_26 (Conv1D)             (None, 312, 16)      14352       ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_27 (Conv1D)             (None, 312, 32)      20512       ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_28 (Conv1D)             (None, 312, 32)      12320       ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_28 (MaxPooling1D  (None, 156, 128)    0           ['concatenate_4[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_30 (Conv1D)             (None, 312, 16)      2064        ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_25 (MaxPooling1D  (None, 156, 16)     0           ['conv1d_26[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling1d_26 (MaxPooling1D  (None, 156, 32)     0           ['conv1d_27[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling1d_27 (MaxPooling1D  (None, 156, 32)     0           ['conv1d_28[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_29 (Conv1D)             (None, 156, 32)      4128        ['max_pooling1d_28[0][0]']       \n",
            "                                                                                                  \n",
            " max_pooling1d_29 (MaxPooling1D  (None, 156, 16)     0           ['conv1d_30[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " group_normalization_26 (GroupN  (None, 156, 16)     32          ['max_pooling1d_25[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_27 (GroupN  (None, 156, 32)     64          ['max_pooling1d_26[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_28 (GroupN  (None, 156, 32)     64          ['max_pooling1d_27[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_29 (GroupN  (None, 156, 32)     64          ['conv1d_29[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_30 (GroupN  (None, 156, 16)     32          ['max_pooling1d_29[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 156, 16)      0           ['group_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " activation_27 (Activation)     (None, 156, 32)      0           ['group_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " activation_28 (Activation)     (None, 156, 32)      0           ['group_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " activation_29 (Activation)     (None, 156, 32)      0           ['group_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " activation_30 (Activation)     (None, 156, 16)      0           ['group_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_25 (Dropout)           (None, 156, 16)      0           ['activation_26[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_26 (Dropout)           (None, 156, 32)      0           ['activation_27[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_27 (Dropout)           (None, 156, 32)      0           ['activation_28[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_28 (Dropout)           (None, 156, 32)      0           ['activation_29[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_29 (Dropout)           (None, 156, 16)      0           ['activation_30[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 156, 128)     0           ['dropout_25[0][0]',             \n",
            "                                                                  'dropout_26[0][0]',             \n",
            "                                                                  'dropout_27[0][0]',             \n",
            "                                                                  'dropout_28[0][0]',             \n",
            "                                                                  'dropout_29[0][0]']             \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 128)          131584      ['concatenate_5[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 128)         256         ['lstm[0][0]']                   \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " dropout_30 (Dropout)           (None, 128)          0           ['layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          16512       ['dropout_30[0][0]']             \n",
            "                                                                                                  \n",
            " activation_31 (Activation)     (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            129         ['activation_31[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 460,129\n",
            "Trainable params: 460,129\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUAbPQFOV3mD",
        "outputId": "87424802-5324-47c9-8e52-d8c8ee6f4376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "import datetime"
      ],
      "metadata": {
        "id": "aIP6jd9lV5o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# So my GPU-memory is somewhat unstable and crashes for batchsize 16 or bigger"
      ],
      "metadata": {
        "id": "mo9tczmEapVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "my_saluki.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.99, clipnorm=0.5),\n",
        "              loss=\"mse\")\n",
        "\n",
        "logdir = os.path.join(os.path.join(os.getcwd(), \"logs\"), datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1, write_graph=True)\n",
        "\n",
        "# Train the model\n",
        "history = my_saluki.fit(X_train, y_train,\n",
        "                        #X_train[0:200,:], y_train[0:200], \n",
        "                        validation_data=(X_val, y_val),\n",
        "                        #validation_data=(X_val[0:100,:], y_val[0:100]),\n",
        "                        callbacks=[EarlyStopping(patience=25, restore_best_weights=True),   \n",
        "                                   History(),\n",
        "                                   ExplainedVariance(validation_data=(X_val, y_val), interval=4),\n",
        "                                   tensorboard_callback],\n",
        "                        batch_size=32,  #they used 64\n",
        "                        epochs=1000)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t13N24J-V8eq",
        "outputId": "08f57d47-c346-4e97-9def-95e2370cb684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "  6/275 [..............................] - ETA: 40s - loss: 3.6049WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0929s vs `on_train_batch_end` time: 0.1150s). Check your callbacks.\n",
            "275/275 [==============================] - ETA: 0s - loss: 2.0292interval evaluation - epoch: 0 - explained variance: 0.070838\n",
            "275/275 [==============================] - 82s 225ms/step - loss: 2.0292 - val_loss: 1.3243\n",
            "Epoch 2/1000\n",
            "275/275 [==============================] - 99s 362ms/step - loss: 1.3053 - val_loss: 1.0705\n",
            "Epoch 3/1000\n",
            "275/275 [==============================] - 154s 560ms/step - loss: 1.1105 - val_loss: 0.9386\n",
            "Epoch 4/1000\n",
            "275/275 [==============================] - 161s 586ms/step - loss: 1.0338 - val_loss: 0.8982\n",
            "Epoch 5/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 1.0080interval evaluation - epoch: 4 - explained variance: 0.124345\n",
            "275/275 [==============================] - 174s 633ms/step - loss: 1.0080 - val_loss: 0.8702\n",
            "Epoch 6/1000\n",
            "275/275 [==============================] - 162s 589ms/step - loss: 0.8894 - val_loss: 0.7161\n",
            "Epoch 7/1000\n",
            "275/275 [==============================] - 161s 587ms/step - loss: 0.7910 - val_loss: 0.7217\n",
            "Epoch 8/1000\n",
            "275/275 [==============================] - 161s 587ms/step - loss: 0.7796 - val_loss: 0.8982\n",
            "Epoch 9/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.7590interval evaluation - epoch: 8 - explained variance: 0.338671\n",
            "275/275 [==============================] - 174s 633ms/step - loss: 0.7590 - val_loss: 0.6578\n",
            "Epoch 10/1000\n",
            "275/275 [==============================] - 162s 589ms/step - loss: 0.7473 - val_loss: 0.7914\n",
            "Epoch 11/1000\n",
            "275/275 [==============================] - 162s 588ms/step - loss: 0.7328 - val_loss: 0.8053\n",
            "Epoch 12/1000\n",
            "275/275 [==============================] - 161s 586ms/step - loss: 0.7290 - val_loss: 0.6643\n",
            "Epoch 13/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.7194interval evaluation - epoch: 12 - explained variance: 0.324420\n",
            "275/275 [==============================] - 156s 567ms/step - loss: 0.7194 - val_loss: 0.7192\n",
            "Epoch 14/1000\n",
            "275/275 [==============================] - 153s 557ms/step - loss: 0.7081 - val_loss: 0.6594\n",
            "Epoch 15/1000\n",
            "275/275 [==============================] - 159s 579ms/step - loss: 0.7125 - val_loss: 0.9804\n",
            "Epoch 16/1000\n",
            "275/275 [==============================] - 162s 588ms/step - loss: 0.7080 - val_loss: 0.6629\n",
            "Epoch 17/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6968interval evaluation - epoch: 16 - explained variance: 0.355161\n",
            "275/275 [==============================] - 174s 633ms/step - loss: 0.6968 - val_loss: 0.6629\n",
            "Epoch 18/1000\n",
            "275/275 [==============================] - 161s 587ms/step - loss: 0.6998 - val_loss: 0.6305\n",
            "Epoch 19/1000\n",
            "275/275 [==============================] - 162s 588ms/step - loss: 0.6879 - val_loss: 0.7045\n",
            "Epoch 20/1000\n",
            "275/275 [==============================] - 162s 588ms/step - loss: 0.6764 - val_loss: 0.8353\n",
            "Epoch 21/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6745interval evaluation - epoch: 20 - explained variance: 0.346990\n",
            "275/275 [==============================] - 169s 617ms/step - loss: 0.6745 - val_loss: 0.6497\n",
            "Epoch 22/1000\n",
            "275/275 [==============================] - 161s 585ms/step - loss: 0.6993 - val_loss: 0.9084\n",
            "Epoch 23/1000\n",
            "275/275 [==============================] - 161s 587ms/step - loss: 0.6922 - val_loss: 0.6435\n",
            "Epoch 24/1000\n",
            "275/275 [==============================] - 162s 587ms/step - loss: 0.6724 - val_loss: 0.6923\n",
            "Epoch 25/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6865interval evaluation - epoch: 24 - explained variance: 0.357704\n",
            "275/275 [==============================] - 174s 633ms/step - loss: 0.6865 - val_loss: 0.7433\n",
            "Epoch 26/1000\n",
            "275/275 [==============================] - 151s 551ms/step - loss: 0.6594 - val_loss: 0.6745\n",
            "Epoch 27/1000\n",
            "275/275 [==============================] - 155s 566ms/step - loss: 0.6709 - val_loss: 0.7283\n",
            "Epoch 28/1000\n",
            "275/275 [==============================] - 146s 531ms/step - loss: 0.6727 - val_loss: 0.9271\n",
            "Epoch 29/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6599interval evaluation - epoch: 28 - explained variance: 0.365943\n",
            "275/275 [==============================] - 155s 566ms/step - loss: 0.6599 - val_loss: 0.6434\n",
            "Epoch 30/1000\n",
            "275/275 [==============================] - 141s 515ms/step - loss: 0.6697 - val_loss: 0.7679\n",
            "Epoch 31/1000\n",
            "275/275 [==============================] - 149s 544ms/step - loss: 0.6573 - val_loss: 0.6660\n",
            "Epoch 32/1000\n",
            "275/275 [==============================] - 142s 518ms/step - loss: 0.6535 - val_loss: 0.7279\n",
            "Epoch 33/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6541interval evaluation - epoch: 32 - explained variance: 0.364153\n",
            "275/275 [==============================] - 155s 565ms/step - loss: 0.6541 - val_loss: 0.6302\n",
            "Epoch 34/1000\n",
            "275/275 [==============================] - 148s 541ms/step - loss: 0.6528 - val_loss: 0.8841\n",
            "Epoch 35/1000\n",
            "275/275 [==============================] - 141s 513ms/step - loss: 0.6496 - val_loss: 0.9294\n",
            "Epoch 36/1000\n",
            "275/275 [==============================] - 143s 523ms/step - loss: 0.6456 - val_loss: 0.7199\n",
            "Epoch 37/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6427interval evaluation - epoch: 36 - explained variance: 0.320889\n",
            "275/275 [==============================] - 149s 542ms/step - loss: 0.6427 - val_loss: 0.7620\n",
            "Epoch 38/1000\n",
            "275/275 [==============================] - 148s 540ms/step - loss: 0.6437 - val_loss: 0.6665\n",
            "Epoch 39/1000\n",
            "275/275 [==============================] - 141s 514ms/step - loss: 0.6361 - val_loss: 0.9692\n",
            "Epoch 40/1000\n",
            "275/275 [==============================] - 149s 542ms/step - loss: 0.6415 - val_loss: 0.7935\n",
            "Epoch 41/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6473interval evaluation - epoch: 40 - explained variance: 0.378983\n",
            "275/275 [==============================] - 152s 555ms/step - loss: 0.6473 - val_loss: 0.6454\n",
            "Epoch 42/1000\n",
            "275/275 [==============================] - 139s 506ms/step - loss: 0.6337 - val_loss: 0.6842\n",
            "Epoch 43/1000\n",
            "275/275 [==============================] - 134s 488ms/step - loss: 0.6316 - val_loss: 0.8105\n",
            "Epoch 44/1000\n",
            "275/275 [==============================] - 150s 547ms/step - loss: 0.6243 - val_loss: 0.6453\n",
            "Epoch 45/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6388interval evaluation - epoch: 44 - explained variance: 0.360465\n",
            "275/275 [==============================] - 140s 510ms/step - loss: 0.6388 - val_loss: 0.6316\n",
            "Epoch 46/1000\n",
            "275/275 [==============================] - 139s 505ms/step - loss: 0.6449 - val_loss: 0.7299\n",
            "Epoch 47/1000\n",
            "275/275 [==============================] - 148s 538ms/step - loss: 0.6276 - val_loss: 0.7479\n",
            "Epoch 48/1000\n",
            "275/275 [==============================] - 138s 504ms/step - loss: 0.6295 - val_loss: 0.7974\n",
            "Epoch 49/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6145interval evaluation - epoch: 48 - explained variance: 0.342651\n",
            "275/275 [==============================] - 149s 542ms/step - loss: 0.6145 - val_loss: 0.8672\n",
            "Epoch 50/1000\n",
            "275/275 [==============================] - 141s 514ms/step - loss: 0.6351 - val_loss: 0.7411\n",
            "Epoch 51/1000\n",
            "275/275 [==============================] - 133s 485ms/step - loss: 0.6291 - val_loss: 0.7934\n",
            "Epoch 52/1000\n",
            "275/275 [==============================] - 144s 526ms/step - loss: 0.6306 - val_loss: 0.7486\n",
            "Epoch 53/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6247interval evaluation - epoch: 52 - explained variance: 0.354355\n",
            "275/275 [==============================] - 148s 539ms/step - loss: 0.6247 - val_loss: 0.6495\n",
            "Epoch 54/1000\n",
            "275/275 [==============================] - 144s 524ms/step - loss: 0.6171 - val_loss: 0.8717\n",
            "Epoch 55/1000\n",
            "275/275 [==============================] - 135s 491ms/step - loss: 0.6221 - val_loss: 0.7926\n",
            "Epoch 56/1000\n",
            "275/275 [==============================] - 141s 513ms/step - loss: 0.6230 - val_loss: 0.7299\n",
            "Epoch 57/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6064interval evaluation - epoch: 56 - explained variance: 0.350637\n",
            "275/275 [==============================] - 143s 521ms/step - loss: 0.6064 - val_loss: 0.7673\n",
            "Epoch 58/1000\n",
            "275/275 [==============================] - 141s 512ms/step - loss: 0.6108 - val_loss: 0.8277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = my_saluki.predict(X_val)\n",
        "print(\"Explained Var Score: %.2f\" % explained_variance_score(y_val, y_pred))\n",
        "print(\"R2 Score: %.2f\" % r2_score(y_val, y_pred))"
      ],
      "metadata": {
        "id": "7hCHlQnlWPVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bda3ada6-c35f-4b16-f028-eeb80caa3d08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "67/67 [==============================] - 6s 93ms/step\n",
            "Explained Var Score: 0.36\n",
            "R2 Score: 0.36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_saluki.save('my_saluki5_28.h5')"
      ],
      "metadata": {
        "id": "e-SqW3qpct8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_loss(history):\n",
        "    fig, ax = plt.subplots(figsize = (5,5))\n",
        "    ax.plot(history['loss'][1:])\n",
        "    ax.plot(history['val_loss'][1:])\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('mean squared error')"
      ],
      "metadata": {
        "id": "76EPwtmyWN_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " plot_loss(history.history)"
      ],
      "metadata": {
        "id": "ser84xLLWSPq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "8eb835f2-4334-4927-be02-e06960b6fc95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}