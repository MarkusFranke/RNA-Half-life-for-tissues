{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inception v27 - LayerNorm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarkusFranke/RNA-Half-life-for-tissues/blob/main/models-for-general-half-life/Inception_v27_LayerNorm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKkQxNHexGtb"
      },
      "outputs": [],
      "source": [
        "from Bio import SeqIO #for parsing Fasta Files\n",
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "from kipoiseq.transforms.functional import one_hot, fixed_len\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from sklearn.metrics import explained_variance_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import backend as k\n",
        "from keras.callbacks import EarlyStopping, History\n",
        "from keras.models import Model\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow as tf\n",
        "import keras.layers as kl\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import subprocess\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we can't run this in google colab due to RAM limitations, I'm importing the data from disk. All the data should be available on google drive though, under the same filenames, either to be downloaded and run with a path to where the user saved them, or to be directly accessible by mounting the google drive (and setting a shortcut to our google drive data path in google drive)\n"
      ],
      "metadata": {
        "id": "ku26rqE7x06J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hl = pd.read_excel(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\kelley_et_al_corrected_hl.xlsx', skiprows=[0, 1])\n",
        "hl['zscore'] = zscore(hl['half-life (PC1)'])\n",
        "halflife = hl[[\"Ensembl Gene Id\", \"zscore\"]]\n",
        "hl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "7vRfq7bIxy4J",
        "outputId": "7a5f1f4c-95ca-4489-f612-6412ac34fdf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Ensembl Gene Id  Gene name  half-life (PC1)  Bazzini_ActD_HEK293_1  \\\n",
              "0      ENSG00000000003     TSPAN6         8.660955               0.763166   \n",
              "1      ENSG00000000419       DPM1         2.241221               0.529938   \n",
              "2      ENSG00000000457      SCYL3        -6.929173              -0.798471   \n",
              "3      ENSG00000000460   C1orf112         0.440909               0.461228   \n",
              "4      ENSG00000000938        FGR        -0.943680               0.164310   \n",
              "...                ...        ...              ...                    ...   \n",
              "13916  ENSG00000284770       TBCE         2.218664               0.100281   \n",
              "13917  ENSG00000285077  ARHGAP11B        -3.262964              -0.733980   \n",
              "13918  ENSG00000288596    C8orf44         2.118850              -0.485957   \n",
              "13919  ENSG00000288701     PRRC2B         0.133147               0.133770   \n",
              "13920  ENSG00000288722       F8A1        -3.485607              -0.607463   \n",
              "\n",
              "       Bazzini_ActD_HeLa_1  Bazzini_ActD_RPE_1  Bazzini_4sU_K562_1  \\\n",
              "0                 0.258448            0.106486            1.019072   \n",
              "1                 0.222678           -0.040666           -0.284952   \n",
              "2                -0.894854           -1.039150           -1.444532   \n",
              "3                 0.195794           -0.739672           -0.123925   \n",
              "4                 0.112064            0.095773            0.045345   \n",
              "...                    ...                 ...                 ...   \n",
              "13916            -0.187624           -0.143422            0.201024   \n",
              "13917            -0.934478           -0.952570           -0.461339   \n",
              "13918            -0.320560           -0.332189            0.582473   \n",
              "13919             0.214899            0.144491            0.090991   \n",
              "13920            -0.419258           -0.378650           -0.775774   \n",
              "\n",
              "       Akimitsu_BrU_HeLa_1  Rinn_ActD_K562_1  Rinn_ActD_K562_2  ...  \\\n",
              "0                 2.022504          1.744745          1.783356  ...   \n",
              "1                 0.145097          0.866919          0.832768  ...   \n",
              "2                -1.287191         -1.006317         -1.062201  ...   \n",
              "3                 0.162538         -0.023056         -0.008479  ...   \n",
              "4                 0.024136         -0.209157         -0.223700  ...   \n",
              "...                    ...               ...               ...  ...   \n",
              "13916             0.933352          0.465403          0.451758  ...   \n",
              "13917            -0.940048          0.225157          0.161858  ...   \n",
              "13918            -0.614775          0.305297          0.336730  ...   \n",
              "13919             0.131956         -0.040831         -0.035576  ...   \n",
              "13920             0.400149         -1.060379         -1.033201  ...   \n",
              "\n",
              "       Gejman_4sU_GM12812_1  Gejman_4sU_GM12814_1  Gejman_4sU_GM12815_1  \\\n",
              "0                  1.037361              0.969166              1.209562   \n",
              "1                 -0.212424             -0.747989              0.371214   \n",
              "2                 -1.155096             -1.421651             -1.568912   \n",
              "3                  1.365208              1.017193             -0.239569   \n",
              "4                  0.722198              1.024313              1.484018   \n",
              "...                     ...                   ...                   ...   \n",
              "13916              1.082013              0.917651              0.897480   \n",
              "13917              0.256969              0.033071             -0.249782   \n",
              "13918              1.023526             -0.155662             -0.950892   \n",
              "13919             -0.091563             -0.021327             -0.017414   \n",
              "13920              0.433892              0.446224              1.344124   \n",
              "\n",
              "       Simon_4sU_K562_1  Simon_4sU_K562_2  Rissland_4sU_HEK293_1  \\\n",
              "0              2.080643          2.095154               0.674058   \n",
              "1              0.772595          0.712843              -0.350914   \n",
              "2             -1.308978         -1.311572              -0.387172   \n",
              "3              0.373929          0.380154              -0.063720   \n",
              "4             -0.375671         -0.384504               0.232924   \n",
              "...                 ...               ...                    ...   \n",
              "13916          0.866792          0.868080              -0.104068   \n",
              "13917         -1.116405         -1.078302              -1.495412   \n",
              "13918         -0.175193         -0.166760               2.505294   \n",
              "13919          0.088581          0.107441               0.345123   \n",
              "13920         -0.631782         -0.629741              -0.413725   \n",
              "\n",
              "       Rissland_4sU_HEK293_2  Rissland_4sU_HEK293_3  Rissland_4sU_HEK293_4  \\\n",
              "0                   1.120375               1.456258               1.791769   \n",
              "1                  -0.879247              -0.825603               0.109861   \n",
              "2                  -1.229226              -1.122749              -0.570002   \n",
              "3                  -0.450610              -0.805719               0.453957   \n",
              "4                   0.347933               0.266619               0.063565   \n",
              "...                      ...                    ...                    ...   \n",
              "13916              -0.169414              -0.404886              -0.165188   \n",
              "13917              -1.006317              -1.240272              -1.440629   \n",
              "13918               1.068162               1.307039               1.563308   \n",
              "13919              -0.188176              -0.119128               0.001050   \n",
              "13920              -0.071227              -0.105036              -0.197549   \n",
              "\n",
              "         zscore  \n",
              "0      1.807620  \n",
              "1      0.467763  \n",
              "2     -1.446182  \n",
              "3      0.092022  \n",
              "4     -0.196955  \n",
              "...         ...  \n",
              "13916  0.463055  \n",
              "13917 -0.681011  \n",
              "13918  0.442223  \n",
              "13919  0.027789  \n",
              "13920 -0.727478  \n",
              "\n",
              "[13921 rows x 58 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ensembl Gene Id</th>\n",
              "      <th>Gene name</th>\n",
              "      <th>half-life (PC1)</th>\n",
              "      <th>Bazzini_ActD_HEK293_1</th>\n",
              "      <th>Bazzini_ActD_HeLa_1</th>\n",
              "      <th>Bazzini_ActD_RPE_1</th>\n",
              "      <th>Bazzini_4sU_K562_1</th>\n",
              "      <th>Akimitsu_BrU_HeLa_1</th>\n",
              "      <th>Rinn_ActD_K562_1</th>\n",
              "      <th>Rinn_ActD_K562_2</th>\n",
              "      <th>...</th>\n",
              "      <th>Gejman_4sU_GM12812_1</th>\n",
              "      <th>Gejman_4sU_GM12814_1</th>\n",
              "      <th>Gejman_4sU_GM12815_1</th>\n",
              "      <th>Simon_4sU_K562_1</th>\n",
              "      <th>Simon_4sU_K562_2</th>\n",
              "      <th>Rissland_4sU_HEK293_1</th>\n",
              "      <th>Rissland_4sU_HEK293_2</th>\n",
              "      <th>Rissland_4sU_HEK293_3</th>\n",
              "      <th>Rissland_4sU_HEK293_4</th>\n",
              "      <th>zscore</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ENSG00000000003</td>\n",
              "      <td>TSPAN6</td>\n",
              "      <td>8.660955</td>\n",
              "      <td>0.763166</td>\n",
              "      <td>0.258448</td>\n",
              "      <td>0.106486</td>\n",
              "      <td>1.019072</td>\n",
              "      <td>2.022504</td>\n",
              "      <td>1.744745</td>\n",
              "      <td>1.783356</td>\n",
              "      <td>...</td>\n",
              "      <td>1.037361</td>\n",
              "      <td>0.969166</td>\n",
              "      <td>1.209562</td>\n",
              "      <td>2.080643</td>\n",
              "      <td>2.095154</td>\n",
              "      <td>0.674058</td>\n",
              "      <td>1.120375</td>\n",
              "      <td>1.456258</td>\n",
              "      <td>1.791769</td>\n",
              "      <td>1.807620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENSG00000000419</td>\n",
              "      <td>DPM1</td>\n",
              "      <td>2.241221</td>\n",
              "      <td>0.529938</td>\n",
              "      <td>0.222678</td>\n",
              "      <td>-0.040666</td>\n",
              "      <td>-0.284952</td>\n",
              "      <td>0.145097</td>\n",
              "      <td>0.866919</td>\n",
              "      <td>0.832768</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.212424</td>\n",
              "      <td>-0.747989</td>\n",
              "      <td>0.371214</td>\n",
              "      <td>0.772595</td>\n",
              "      <td>0.712843</td>\n",
              "      <td>-0.350914</td>\n",
              "      <td>-0.879247</td>\n",
              "      <td>-0.825603</td>\n",
              "      <td>0.109861</td>\n",
              "      <td>0.467763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENSG00000000457</td>\n",
              "      <td>SCYL3</td>\n",
              "      <td>-6.929173</td>\n",
              "      <td>-0.798471</td>\n",
              "      <td>-0.894854</td>\n",
              "      <td>-1.039150</td>\n",
              "      <td>-1.444532</td>\n",
              "      <td>-1.287191</td>\n",
              "      <td>-1.006317</td>\n",
              "      <td>-1.062201</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.155096</td>\n",
              "      <td>-1.421651</td>\n",
              "      <td>-1.568912</td>\n",
              "      <td>-1.308978</td>\n",
              "      <td>-1.311572</td>\n",
              "      <td>-0.387172</td>\n",
              "      <td>-1.229226</td>\n",
              "      <td>-1.122749</td>\n",
              "      <td>-0.570002</td>\n",
              "      <td>-1.446182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ENSG00000000460</td>\n",
              "      <td>C1orf112</td>\n",
              "      <td>0.440909</td>\n",
              "      <td>0.461228</td>\n",
              "      <td>0.195794</td>\n",
              "      <td>-0.739672</td>\n",
              "      <td>-0.123925</td>\n",
              "      <td>0.162538</td>\n",
              "      <td>-0.023056</td>\n",
              "      <td>-0.008479</td>\n",
              "      <td>...</td>\n",
              "      <td>1.365208</td>\n",
              "      <td>1.017193</td>\n",
              "      <td>-0.239569</td>\n",
              "      <td>0.373929</td>\n",
              "      <td>0.380154</td>\n",
              "      <td>-0.063720</td>\n",
              "      <td>-0.450610</td>\n",
              "      <td>-0.805719</td>\n",
              "      <td>0.453957</td>\n",
              "      <td>0.092022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENSG00000000938</td>\n",
              "      <td>FGR</td>\n",
              "      <td>-0.943680</td>\n",
              "      <td>0.164310</td>\n",
              "      <td>0.112064</td>\n",
              "      <td>0.095773</td>\n",
              "      <td>0.045345</td>\n",
              "      <td>0.024136</td>\n",
              "      <td>-0.209157</td>\n",
              "      <td>-0.223700</td>\n",
              "      <td>...</td>\n",
              "      <td>0.722198</td>\n",
              "      <td>1.024313</td>\n",
              "      <td>1.484018</td>\n",
              "      <td>-0.375671</td>\n",
              "      <td>-0.384504</td>\n",
              "      <td>0.232924</td>\n",
              "      <td>0.347933</td>\n",
              "      <td>0.266619</td>\n",
              "      <td>0.063565</td>\n",
              "      <td>-0.196955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13916</th>\n",
              "      <td>ENSG00000284770</td>\n",
              "      <td>TBCE</td>\n",
              "      <td>2.218664</td>\n",
              "      <td>0.100281</td>\n",
              "      <td>-0.187624</td>\n",
              "      <td>-0.143422</td>\n",
              "      <td>0.201024</td>\n",
              "      <td>0.933352</td>\n",
              "      <td>0.465403</td>\n",
              "      <td>0.451758</td>\n",
              "      <td>...</td>\n",
              "      <td>1.082013</td>\n",
              "      <td>0.917651</td>\n",
              "      <td>0.897480</td>\n",
              "      <td>0.866792</td>\n",
              "      <td>0.868080</td>\n",
              "      <td>-0.104068</td>\n",
              "      <td>-0.169414</td>\n",
              "      <td>-0.404886</td>\n",
              "      <td>-0.165188</td>\n",
              "      <td>0.463055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13917</th>\n",
              "      <td>ENSG00000285077</td>\n",
              "      <td>ARHGAP11B</td>\n",
              "      <td>-3.262964</td>\n",
              "      <td>-0.733980</td>\n",
              "      <td>-0.934478</td>\n",
              "      <td>-0.952570</td>\n",
              "      <td>-0.461339</td>\n",
              "      <td>-0.940048</td>\n",
              "      <td>0.225157</td>\n",
              "      <td>0.161858</td>\n",
              "      <td>...</td>\n",
              "      <td>0.256969</td>\n",
              "      <td>0.033071</td>\n",
              "      <td>-0.249782</td>\n",
              "      <td>-1.116405</td>\n",
              "      <td>-1.078302</td>\n",
              "      <td>-1.495412</td>\n",
              "      <td>-1.006317</td>\n",
              "      <td>-1.240272</td>\n",
              "      <td>-1.440629</td>\n",
              "      <td>-0.681011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13918</th>\n",
              "      <td>ENSG00000288596</td>\n",
              "      <td>C8orf44</td>\n",
              "      <td>2.118850</td>\n",
              "      <td>-0.485957</td>\n",
              "      <td>-0.320560</td>\n",
              "      <td>-0.332189</td>\n",
              "      <td>0.582473</td>\n",
              "      <td>-0.614775</td>\n",
              "      <td>0.305297</td>\n",
              "      <td>0.336730</td>\n",
              "      <td>...</td>\n",
              "      <td>1.023526</td>\n",
              "      <td>-0.155662</td>\n",
              "      <td>-0.950892</td>\n",
              "      <td>-0.175193</td>\n",
              "      <td>-0.166760</td>\n",
              "      <td>2.505294</td>\n",
              "      <td>1.068162</td>\n",
              "      <td>1.307039</td>\n",
              "      <td>1.563308</td>\n",
              "      <td>0.442223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13919</th>\n",
              "      <td>ENSG00000288701</td>\n",
              "      <td>PRRC2B</td>\n",
              "      <td>0.133147</td>\n",
              "      <td>0.133770</td>\n",
              "      <td>0.214899</td>\n",
              "      <td>0.144491</td>\n",
              "      <td>0.090991</td>\n",
              "      <td>0.131956</td>\n",
              "      <td>-0.040831</td>\n",
              "      <td>-0.035576</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.091563</td>\n",
              "      <td>-0.021327</td>\n",
              "      <td>-0.017414</td>\n",
              "      <td>0.088581</td>\n",
              "      <td>0.107441</td>\n",
              "      <td>0.345123</td>\n",
              "      <td>-0.188176</td>\n",
              "      <td>-0.119128</td>\n",
              "      <td>0.001050</td>\n",
              "      <td>0.027789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13920</th>\n",
              "      <td>ENSG00000288722</td>\n",
              "      <td>F8A1</td>\n",
              "      <td>-3.485607</td>\n",
              "      <td>-0.607463</td>\n",
              "      <td>-0.419258</td>\n",
              "      <td>-0.378650</td>\n",
              "      <td>-0.775774</td>\n",
              "      <td>0.400149</td>\n",
              "      <td>-1.060379</td>\n",
              "      <td>-1.033201</td>\n",
              "      <td>...</td>\n",
              "      <td>0.433892</td>\n",
              "      <td>0.446224</td>\n",
              "      <td>1.344124</td>\n",
              "      <td>-0.631782</td>\n",
              "      <td>-0.629741</td>\n",
              "      <td>-0.413725</td>\n",
              "      <td>-0.071227</td>\n",
              "      <td>-0.105036</td>\n",
              "      <td>-0.197549</td>\n",
              "      <td>-0.727478</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13921 rows × 58 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\Homo_sapiens.GRCh38.83.chosenTranscript.3pUTRs.fa') as fasta_file:  # Will close handle cleanly\n",
        "    UTR3_identifiers = []\n",
        "    UTR3_seqs = []\n",
        "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
        "        UTR3_identifiers.append(seq_record.id)\n",
        "        UTR3_seqs.append(seq_record.seq)\n",
        "\n",
        "with open(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\Homo_sapiens.GRCh38.83.chosenTranscript.5pUTRs.fa') as fasta_file:  # Will close handle cleanly\n",
        "    UTR5_identifiers = []\n",
        "    UTR5_seqs = []\n",
        "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
        "        UTR5_identifiers.append(seq_record.id)\n",
        "        UTR5_seqs.append(seq_record.seq)\n",
        "\n",
        "with open(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\Homo_sapiens.GRCh38.83.chosenTranscript.ORFs.fa') as fasta_file:  # Will close handle cleanly\n",
        "    ORF_identifiers = []\n",
        "    ORF_seqs = []\n",
        "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
        "        ORF_identifiers.append(seq_record.id)\n",
        "        ORF_seqs.append(seq_record.seq)\n",
        "\n",
        "print(UTR3_seqs[0])\n",
        "print(UTR5_seqs[0])\n",
        "print(ORF_seqs[0])\n",
        "print(UTR3_identifiers[0])\n",
        "print(UTR5_identifiers[0])\n",
        "print(ORF_identifiers[0])\n",
        "print(len(UTR3_seqs))\n",
        "print(len(UTR5_seqs))\n",
        "print(len(ORF_seqs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFU1gCykz-M7",
        "outputId": "73454149-d504-421b-f7aa-b25bb3b32ff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AATATTATGTATGCAGCAATATTTGAGTAACAAGAAGCAAATATCCAAGTTCCAAAATTATAAAAGAAATTCTTATCCAAATAGTAATGTTCTAATTGATCATATAAGAAAGCAAAGCATAGACATTAGAATTATAAGTCAGCAGTGGTCTGTTCAAGAACAATCAACATTTTTAGAAAATAGTAGGACAAAATTAGGAAATAATTATCACCAAGAGGATCTAGTTCATGACTTTCTATTATCTCAATTAGATTGCTCAATCATCAGCCTTCCTATACTAAACTCTGATTCAGGACCAAGAAAGGCATAGTCTGACTCTGGAAATGCGCTGTTGGAAGCCAAATAACATCAATACTCTTGTTCTATAATTGAATATCAAATAAGACAAATTACCATTAATTTAATGACTGTGGAGTTAATTGTATACCAGCATTTCAGCAAATCATCATCAATAGTATTACATTAGCAATTTATGCAATTAAAAGGGCTTTGTAAAACTTTGAATAGATTTTATTGTCATTAGTAGCTGTTGGAACTTCATTATTATATAATGTTTTTGCAAACTTTAACTTTTTTCTAAATTGTTAAATAAAAGAATAACTATCCTTAATCTAAATAATTTTGGTAGCAAATCCTATAAGGTATTAAACATTTTAAGGTATATTATTACATTGCTATTTTACTGTTTCTCATTAACCCAAACAGTTTAAAGGCAGAATTCCACTTAGAAACAAGTTGCATTTTGAAAGTTTATTTGTAATCCATTTGTTTGGAATTCAGAAATGTATTTCACATAAAAATAATCTTGGAAGTAATAAATTCCAAAATTAACTAACAAAA\n",
            "AGATGAGATTTCATCATGTTGGCCAGCCTGGTCTCAAACTCCTGACCTCAAGTGACCCGCCTGCCTCAGCCTCCCAAAGTGCTGGGATTACAGGAATTTAGTGATTGACA\n",
            "ATGGCAGAAAAAATCCTAGAGAAGTTGGATGTCCTTGATAAGCAAGCAGAGATAATCTTGGCCAGAAGAACAAAGATAAACAGGCTTCAGAGTGAAGGAAGAAAAACAACTATGGCTATACCCCTGACATTTGATTTTCAGTTGGAATTTGAAGAAGCTCTTGCTACATCCGCGTCTAAGGCAATATCAAAGATCAAAGAAGACAAGTCATGCAGCATTACAAAATCAAAAATGCATGTCTCTTTCAAATGTGAGCCTGAACCTAGAAAGAGTAATTTTGAAAAGTCAAATTTAAGACCATTCTTTATTCAAACAAATGTAAAAAATAAAGAAAGTGAGTCAACAGCTCAAATTGAAAAAAAACCTAGGAAACCATTGGATTCTGTTGGTCTCTTAGAAGGTGATAGAAATAAAAGAAAAAAATCTCCACAGATGAACGATTTTAATATAAAAGAAAACAAATCGGTCAGAAATTATCAATTAAGTAAGTATAGGTCAGTAAGAAAGAAAAGCTTGCTCCCGTTGTGCTTTGAGGATGAATTGAAAAATCCACATGCCAAGATAGTCAACGTTAGTCCAACAAAGACAGTAACTTCTCACATGGAACAAAAGGACACAAATCCCATAATTTTCCATGACACAGAATATGTACGAATGTTACTTTTGACAAAAAATAGATTTTCTTCTCATCCTTTGGAAAATGAAAACATTTACCCACATAAAAGAACAAATTTCATTTTAGAAAGAAATTGTGAAATCCTCAAATCTATAATTGGCAATCAATCTATTTCTCTTTTCAAACCCCAAAAAACTATGCCTACAGTACAGAGAAAAGATATACAGATCCCTATGTCTTTTAAAGCGGGCCACACAACTGTAGATGATAAACTAAAGAAGAAAACTAATAAGCAGACACTAGAAAACAGATCTTGGAATACACTCTATAATTTCTCACAGAATTTTTCTAGCCTAACAAAACAATTTGTGGGTTACCTTGATAAAGCTGTTATTCATGAAATGAGTGCCCAAACTGGAAAATTTGAAAGAATGTTTTCTGCAGGAAAACCAACGAGCATACCCACATCCAGTGCCTTACCTGTCAAATGTTACTCAAAGCCTTTTAAATATATATATGAACTAAATAATGTAACGCCACTGGATAATTTGTTAAACTTATCAAATGAAATTTTAAATGCCTCA\n",
            "ENSG00000203963\n",
            "ENSG00000203963\n",
            "ENSG00000203963\n",
            "19094\n",
            "18642\n",
            "20253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exons = pd.read_csv(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\Kelley_et_al_exon_junctions.txt', delimiter = \"\\t\")\n",
        "exons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "HrhPuo0w0di6",
        "outputId": "96baa15c-f33d-46cb-aa2a-45b82832ad5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                GeneID  UTR5_len  \\\n",
              "0      ENSG00000000003       112   \n",
              "1      ENSG00000000457       222   \n",
              "2      ENSG00000000460       700   \n",
              "3      ENSG00000000938       289   \n",
              "4      ENSG00000000971       240   \n",
              "...                ...       ...   \n",
              "13225  ENSG00000278615        48   \n",
              "13226  ENSG00000278619       239   \n",
              "13227  ENSG00000278845       161   \n",
              "13228  ENSG00000280789       574   \n",
              "13229  ENSG00000281991       330   \n",
              "\n",
              "                         Exon_Junctions_In_Full_Sequence  \n",
              "0                                199,388,463,562,697,781  \n",
              "1      387,573,687,744,847,959,1037,1177,1362,1534,16...  \n",
              "2      766,871,1012,1178,1263,1402,1483,1548,1697,182...  \n",
              "3           515,618,717,821,971,1127,1307,1384,1538,1670  \n",
              "4      298,484,590,667,859,1030,1204,1399,1576,1759,1...  \n",
              "...                                                  ...  \n",
              "13225                                         87,212,310  \n",
              "13226                                  781,875,1008,1128  \n",
              "13227                        227,405,523,622,671,821,995  \n",
              "13228                                          1056,1139  \n",
              "13229                                                495  \n",
              "\n",
              "[13230 rows x 3 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GeneID</th>\n",
              "      <th>UTR5_len</th>\n",
              "      <th>Exon_Junctions_In_Full_Sequence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ENSG00000000003</td>\n",
              "      <td>112</td>\n",
              "      <td>199,388,463,562,697,781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENSG00000000457</td>\n",
              "      <td>222</td>\n",
              "      <td>387,573,687,744,847,959,1037,1177,1362,1534,16...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENSG00000000460</td>\n",
              "      <td>700</td>\n",
              "      <td>766,871,1012,1178,1263,1402,1483,1548,1697,182...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ENSG00000000938</td>\n",
              "      <td>289</td>\n",
              "      <td>515,618,717,821,971,1127,1307,1384,1538,1670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENSG00000000971</td>\n",
              "      <td>240</td>\n",
              "      <td>298,484,590,667,859,1030,1204,1399,1576,1759,1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13225</th>\n",
              "      <td>ENSG00000278615</td>\n",
              "      <td>48</td>\n",
              "      <td>87,212,310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13226</th>\n",
              "      <td>ENSG00000278619</td>\n",
              "      <td>239</td>\n",
              "      <td>781,875,1008,1128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13227</th>\n",
              "      <td>ENSG00000278845</td>\n",
              "      <td>161</td>\n",
              "      <td>227,405,523,622,671,821,995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13228</th>\n",
              "      <td>ENSG00000280789</td>\n",
              "      <td>574</td>\n",
              "      <td>1056,1139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13229</th>\n",
              "      <td>ENSG00000281991</td>\n",
              "      <td>330</td>\n",
              "      <td>495</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13230 rows × 3 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chromosomes = pd.read_csv(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\Kelley_et_al_chromosomes.txt', delimiter = \"\\t\")\n",
        "chromosomes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "N_zoThwx0u0w",
        "outputId": "4a9d313f-0bc7-46a9-aecc-505ddcc9dddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                GeneID  Chromosome\n",
              "0      ENSG00000186092           1\n",
              "1      ENSG00000279928           1\n",
              "2      ENSG00000279457           1\n",
              "3      ENSG00000278566           1\n",
              "4      ENSG00000273547           1\n",
              "...                ...         ...\n",
              "20290  ENSG00000277856  KI270726.1\n",
              "20291  ENSG00000275063  KI270726.1\n",
              "20292  ENSG00000271254  KI270711.1\n",
              "20293  ENSG00000277475  KI270713.1\n",
              "20294  ENSG00000268674  KI270713.1\n",
              "\n",
              "[20295 rows x 2 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GeneID</th>\n",
              "      <th>Chromosome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ENSG00000186092</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENSG00000279928</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENSG00000279457</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ENSG00000278566</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENSG00000273547</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20290</th>\n",
              "      <td>ENSG00000277856</td>\n",
              "      <td>KI270726.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20291</th>\n",
              "      <td>ENSG00000275063</td>\n",
              "      <td>KI270726.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20292</th>\n",
              "      <td>ENSG00000271254</td>\n",
              "      <td>KI270711.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20293</th>\n",
              "      <td>ENSG00000277475</td>\n",
              "      <td>KI270713.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20294</th>\n",
              "      <td>ENSG00000268674</td>\n",
              "      <td>KI270713.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20295 rows × 2 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chromosomes['Chromosome'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7fzASja1JI6",
        "outputId": "a6da6dfa-b222-4aca-f281-3aa8cc7011ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1             2053\n",
              "19            1458\n",
              "11            1316\n",
              "2             1298\n",
              "17            1185\n",
              "3             1070\n",
              "6             1045\n",
              "12            1033\n",
              "7              980\n",
              "5              868\n",
              "16             865\n",
              "X              824\n",
              "14             824\n",
              "9              772\n",
              "4              747\n",
              "10             730\n",
              "8              670\n",
              "15             609\n",
              "20             541\n",
              "22             489\n",
              "13             320\n",
              "18             269\n",
              "21             233\n",
              "Y               54\n",
              "MT              13\n",
              "KI270728.1       6\n",
              "KI270727.1       4\n",
              "KI270734.1       3\n",
              "GL000194.1       2\n",
              "GL000195.1       2\n",
              "KI270726.1       2\n",
              "KI270713.1       2\n",
              "GL000009.2       1\n",
              "GL000205.2       1\n",
              "GL000219.1       1\n",
              "GL000213.1       1\n",
              "GL000218.1       1\n",
              "KI270731.1       1\n",
              "KI270721.1       1\n",
              "KI270711.1       1\n",
              "Name: Chromosome, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XRaMXVFv1YvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rbp_k = np.load(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\RBP_k.npy')\n",
        "rbp_k.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNwe4p0T4_u7",
        "outputId": "74853525-50a3-44d6-e841-924639195a13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13230, 59)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So not only do we have much more chromosomes, we the data comes from Pedro (df), Saluki (the sequences), from Pauline (chromosomes, exon junctions, via Saluki-chosen transcript), and from Yasmine (RBPs using Deepripe).\n",
        "Thus we should take good care that we merge the tables correctly."
      ],
      "metadata": {
        "id": "ITVKHQAf5c5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d = {'geneID': UTR5_identifiers, 'UTR5_seqs': UTR5_seqs}\n",
        "UTR5 = pd.DataFrame(data=d)\n",
        "d = {'geneID': ORF_identifiers, 'ORF_seqs': ORF_seqs}\n",
        "ORF = pd.DataFrame(data=d)\n",
        "d = {'geneID': UTR3_identifiers, 'UTR3_seqs': UTR3_seqs}\n",
        "UTR3 = pd.DataFrame(data=d)\n",
        "\n",
        "#merge every data frame to sequences\n",
        "halflife = hl[[\"Ensembl Gene Id\", \"zscore\"]]\n",
        "seqs = pd.merge(pd.merge(UTR5, ORF, on ='geneID'), UTR3, on = 'geneID')\n",
        "sequences = pd.merge(halflife, seqs, right_on = 'geneID', left_on = 'Ensembl Gene Id')\n",
        "sequences = sequences.drop(columns=[\"geneID\"])\n",
        "sequences = sequences.rename(columns={\"Ensembl Gene Id\": \"geneID\"})\n",
        "sequences = pd.merge(sequences, chromosomes, left_on='geneID', right_on='GeneID')\n",
        "sequences = sequences.drop(columns=[\"GeneID\"])\n",
        "sequences = pd.merge(sequences, exons, left_on='geneID', right_on='GeneID')\n",
        "sequences = sequences.drop(columns=[\"GeneID\"])\n",
        "\n",
        "#transform seqs into strings:\n",
        "sequences[\"UTR5_seqs\"] = sequences[\"UTR5_seqs\"].apply(str)\n",
        "sequences[\"UTR3_seqs\"] = sequences[\"UTR3_seqs\"].apply(str)\n",
        "sequences[\"ORF_seqs\"] = sequences[\"ORF_seqs\"].apply(str)\n",
        "\n",
        "rubbish = [d, UTR3, ORF, UTR5, UTR5_seqs, UTR3_seqs, ORF_seqs, UTR3_identifiers, UTR5_identifiers, ORF_identifiers, hl, halflife]\n",
        "del rubbish\n",
        "\n",
        "sequences.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "M7rLipk85dwm",
        "outputId": "0cdd0863-9f24-4fba-e12f-71c5e4216c4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            geneID    zscore  \\\n",
              "0  ENSG00000000003  1.807620   \n",
              "1  ENSG00000000457 -1.446182   \n",
              "2  ENSG00000000460  0.092022   \n",
              "3  ENSG00000000938 -0.196955   \n",
              "4  ENSG00000000971  1.611324   \n",
              "\n",
              "                                           UTR5_seqs  \\\n",
              "0  AGTTGTGGACGCTCGTAAGTTTTCGGCAGTTTCCGGGGAGACTCGG...   \n",
              "1  TGTCCCGTTTCCGGACCCGTCTCTATGGTGTAGGAGAAACCCGGCC...   \n",
              "2  GGCTTTGGCCCTGGAAAGCCTCGCGGACGTGTTCTGACCCAAGGTT...   \n",
              "3  GGCTTGGGGCTAGGGCGTGACTGTCTCCCTGCCACCATCACCGCCC...   \n",
              "4  ACAGCATTAACATTTAGTGGGAGTGCAGTGAGAATTGGGTTTAACT...   \n",
              "\n",
              "                                            ORF_seqs  \\\n",
              "0  ATGGCGTCCCCGTCTCGGAGACTGCAGACTAAACCAGTCATTACTT...   \n",
              "1  ATGGGATCAGAGAACAGTGCTTTAAAGAGCTATACACTGAGAGAAC...   \n",
              "2  ATGTTTTTACCTCATATGAACCACCTGACATTGGAACAGACTTTCT...   \n",
              "3  ATGGGCTGTGTGTTCTGCAAGAAATTGGAGCCGGTGGCCACGGCCA...   \n",
              "4  ATGAGACTTCTAGCAAAGATTATTTGCCTTATGTTATGGGCTATTT...   \n",
              "\n",
              "                                           UTR3_seqs Chromosome  UTR5_len  \\\n",
              "0  CCCAATGTATCTGTGGGCCTATTCCTCTCTACCTTTAAGGACATTT...          X       112   \n",
              "1  CAATAGATGTGAGTTAAACTTTAGGAAAAAGGATTCCCTTTTTTTA...          1       222   \n",
              "2  AACTTATCACTAGGCAGAACTGGGTTTGATGCTTTGTCAACTGAAA...          1       700   \n",
              "3  CCTGTCCGGGCATCAACCCTCTCTGGCGGTGGCCACCAGTCCTTGC...          1       289   \n",
              "4  AATCAATCATAAAGTGCACACCTTTATTCAGAACTTTAGTATTAAA...          1       240   \n",
              "\n",
              "                     Exon_Junctions_In_Full_Sequence  \n",
              "0                            199,388,463,562,697,781  \n",
              "1  387,573,687,744,847,959,1037,1177,1362,1534,16...  \n",
              "2  766,871,1012,1178,1263,1402,1483,1548,1697,182...  \n",
              "3       515,618,717,821,971,1127,1307,1384,1538,1670  \n",
              "4  298,484,590,667,859,1030,1204,1399,1576,1759,1...  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>geneID</th>\n",
              "      <th>zscore</th>\n",
              "      <th>UTR5_seqs</th>\n",
              "      <th>ORF_seqs</th>\n",
              "      <th>UTR3_seqs</th>\n",
              "      <th>Chromosome</th>\n",
              "      <th>UTR5_len</th>\n",
              "      <th>Exon_Junctions_In_Full_Sequence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ENSG00000000003</td>\n",
              "      <td>1.807620</td>\n",
              "      <td>AGTTGTGGACGCTCGTAAGTTTTCGGCAGTTTCCGGGGAGACTCGG...</td>\n",
              "      <td>ATGGCGTCCCCGTCTCGGAGACTGCAGACTAAACCAGTCATTACTT...</td>\n",
              "      <td>CCCAATGTATCTGTGGGCCTATTCCTCTCTACCTTTAAGGACATTT...</td>\n",
              "      <td>X</td>\n",
              "      <td>112</td>\n",
              "      <td>199,388,463,562,697,781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENSG00000000457</td>\n",
              "      <td>-1.446182</td>\n",
              "      <td>TGTCCCGTTTCCGGACCCGTCTCTATGGTGTAGGAGAAACCCGGCC...</td>\n",
              "      <td>ATGGGATCAGAGAACAGTGCTTTAAAGAGCTATACACTGAGAGAAC...</td>\n",
              "      <td>CAATAGATGTGAGTTAAACTTTAGGAAAAAGGATTCCCTTTTTTTA...</td>\n",
              "      <td>1</td>\n",
              "      <td>222</td>\n",
              "      <td>387,573,687,744,847,959,1037,1177,1362,1534,16...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENSG00000000460</td>\n",
              "      <td>0.092022</td>\n",
              "      <td>GGCTTTGGCCCTGGAAAGCCTCGCGGACGTGTTCTGACCCAAGGTT...</td>\n",
              "      <td>ATGTTTTTACCTCATATGAACCACCTGACATTGGAACAGACTTTCT...</td>\n",
              "      <td>AACTTATCACTAGGCAGAACTGGGTTTGATGCTTTGTCAACTGAAA...</td>\n",
              "      <td>1</td>\n",
              "      <td>700</td>\n",
              "      <td>766,871,1012,1178,1263,1402,1483,1548,1697,182...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ENSG00000000938</td>\n",
              "      <td>-0.196955</td>\n",
              "      <td>GGCTTGGGGCTAGGGCGTGACTGTCTCCCTGCCACCATCACCGCCC...</td>\n",
              "      <td>ATGGGCTGTGTGTTCTGCAAGAAATTGGAGCCGGTGGCCACGGCCA...</td>\n",
              "      <td>CCTGTCCGGGCATCAACCCTCTCTGGCGGTGGCCACCAGTCCTTGC...</td>\n",
              "      <td>1</td>\n",
              "      <td>289</td>\n",
              "      <td>515,618,717,821,971,1127,1307,1384,1538,1670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENSG00000000971</td>\n",
              "      <td>1.611324</td>\n",
              "      <td>ACAGCATTAACATTTAGTGGGAGTGCAGTGAGAATTGGGTTTAACT...</td>\n",
              "      <td>ATGAGACTTCTAGCAAAGATTATTTGCCTTATGTTATGGGCTATTT...</td>\n",
              "      <td>AATCAATCATAAAGTGCACACCTTTATTCAGAACTTTAGTATTAAA...</td>\n",
              "      <td>1</td>\n",
              "      <td>240</td>\n",
              "      <td>298,484,590,667,859,1030,1204,1399,1576,1759,1...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 10000 #this is slightly longer than the 95% quantile, but lower than Saluki's implementation\n",
        "\n",
        "seqs = sequences['UTR5_seqs'] + sequences['ORF_seqs'] + sequences['UTR3_seqs']\n",
        "def pad_sequence(seqs, max_len, anchor='start', value='N'):\n",
        "  padded_seqs = [fixed_len(seq, max_len, anchor=anchor) for seq in seqs.astype(\"string\")]\n",
        "  return padded_seqs\n",
        "fixed_len_seqs = np.array(pad_sequence(seqs, max_len))\n",
        "print(fixed_len_seqs[0:4])\n",
        "del seqs\n",
        "print(fixed_len_seqs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8oUTj9w-XQg",
        "outputId": "0710c428-c515-4603-80f4-38a3a074747a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AGTTGTGGACGCTCGTAAGTTTTCGGCAGTTTCCGGGGAGACTCGGGGACTCCGCGTCTCGCTCTCTGTGTTCCAATCGCCCGGTGCGGTGGTGCAGGGTCTCGGGCTAGTCATGGCGTCCCCGTCTCGGAGACTGCAGACTAAACCAGTCATTACTTGTTTCAAGAGCGTTCTGCTAATCTACACTTTTATTTTCTGGATCACTGGCGTTATCCTTCTTGCAGTTGGCATTTGGGGCAAGGTGAGCCTGGAGAATTACTTTTCTCTTTTAAATGAGAAGGCCACCAATGTCCCCTTCGTGCTCATTGCTACTGGTACCGTCATTATTCTTTTGGGCACCTTTGGTTGTTTTGCTACCTGCCGAGCTTCTGCATGGATGCTAAAACTGTATGCAATGTTTCTGACTCTCGTTTTTTTGGTCGAACTGGTCGCTGCCATCGTAGGATTTGTTTTCAGACATGAGATTAAGAACAGCTTTAAGAATAATTATGAGAAGGCTTTGAAGCAGTATAACTCTACAGGAGATTATAGAAGCCATGCAGTAGACAAGATCCAAAATACGTTGCATTGTTGTGGTGTCACCGATTATAGAGATTGGACAGATACTAATTATTACTCAGAAAAAGGATTTCCTAAGAGTTGCTGTAAACTTGAAGATTGTACTCCACAGAGAGATGCAGACAAAGTAAACAATGAAGGTTGTTTTATAAAGGTGATGACCATTATAGAGTCAGAAATGGGAGTCGTTGCAGGAATTTCCTTTGGAGTTGCTTGCTTCCAACTGATTGGAATCTTTCTCGCCTACTGCCTCTCTCGTGCCATAACAAATAACCAGTATGAGATAGTGCCCAATGTATCTGTGGGCCTATTCCTCTCTACCTTTAAGGACATTTAGGGTCCCCCCTGTGAATTAGAAAGTTGCTTGGCTGGAGAACTGACAACACTACTTACTGATAGACCAAAAAACTACACCAGTAGGTTGATTCAATCAAGATGTATGTAGACCTAAAACTACACCAATAGGCTGATTCAATCAAGATCCGTGCTCGCAGTGGGCTGATTCAATCAAGATGTATGTTTGCTATGTTCTAAGTCCACCTTCTATCCCATTCATGTTAGATCGTTGAAACCCTGTATCCCTCTGAAACACTGGAAGAGCTAGTAAATTGTAAATGAAGTAATACTGTGTTCCTCTTGACTGTTATTTTTCTTAGTAGGGGGCCTTTGGAAGGCACTGTGAATTTGCTATTTTGATGTAGTGTTACAAGATGGAAAATTGATTCCTCTGACTTTGCTATTGATGTAGTGTGATAGAAAATTCACCCCTCTGAACTGGCTCCTTCCCAGTCAAGGTTATCTGGTTTGATTGTATAATTTGCACCAAGAAGTTAAAATGTTTTATGACTCTCTGTTCTGCTGACAGGCAGAGAGTCACATTGTGTAATTTAATTTCAGTCAGTCAATAGATGGCATCCCTCATCAGGGTTGCCAGATGGTGATAACAGTGTAAGGCCTTGGGTCTAAGGCATCCACGACTGGAAGGGACTACTGATGTTCTGTGATACATCAGGTTTCAGCACACAACTTACATTTCTTTGCCTCCAAATTGAGGCATTTATTATGATGTTCATACTTTCCCTCTTGTTTGAAAGTTTCTAATTATTAAATGGTGTCGGAATTGTTGTATTTTCCTTAGGAATTCAGTGGAACTTATCTTCATTAAATTTAGCTGGTACCAGGTTGATATGACTTGTCAATATTATGGTCAACTTTAAGTCTTAGTTTTCGTTTGTGCCTTTGATTAATAAGTATAACTCTTATACAATAAATACTGCTTTCCTCTAAAAAGATCGTGTTTAAATTAACTTGTAGAAAATCTGCTGGAATGGTTGTTGTTTTCCACTGAGAAAGCTAAGCCCTACATTTCTATTCAGAGTACTGTTTTTAGATGTGAAATATAAGCCTGCGGCCTTAACTCTGTATTAAAAAAAATGTTTTTGTTTAAAAAAAACTGTTCCCATAGGTGCAGCAAACCACCATGGCACATGTATACCTATGTAACAAACCTGCACATTCTGCACATGTATCCCAGAACTTAATGTAAACAAAAAAATCTTAAAGTGCAAATATTAAAAAAAACTGTTCTCTGTGAAAAAAATTATATTCCATGTTATAAAGTAGCATATGACTAGTGTTCTCCTAGNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN'\n",
            " 'TGTCCCGTTTCCGGACCCGTCTCTATGGTGTAGGAGAAACCCGGCCCCCAGAAGATTGTGGGTGTAGTGGCCACAGCCTTACAGGCAGGCAGGGGTGGTTGGTGTCAACAGGGGGGCCAACAGGGTACCAGAGCCAAGACCCTCGGCCTCCTCCCCCGCCGCCTTCCTGCAGATCTGCTTGGCTTTGAGGAAGAGTGGCAGTACTGCCTCACTGCATAAGGGATGGGATCAGAGAACAGTGCTTTAAAGAGCTATACACTGAGAGAACCACCATTTACCTTACCCTCTGGACTTGCTGTTTATCCCGCTGTACTGCAAGATGGCAAATTTGCTTCAGTTTTTGTGTATAAGAGAGAAAATGAAGACAAGGTTAATAAAGCTGCCAAGCATTTGAAGACACTTCGTCACCCTTGCTTGCTAAGATTTTTATCTTGTACTGTGGAAGCGGATGGCATTCATCTTGTCACTGAGCGAGTACAGCCCCTGGAAGTGGCTTTGGAAACATTGTCTTCTGCAGAGGTCTGTGCTGGGATCTATGACATATTGCTGGCTCTTATCTTCCTTCATGACAGAGGACACCTAACACACAATAATGTCTGTTTATCATCTGTGTTTGTGAGTGAAGATGGACACTGGAAGCTAGGAGGAATGGAAACTGTTTGTAAAGTTTCTCAGGCCACACCAGAGTTTCTGAGGAGTATTCAGTCAATAAGAGACCCAGCATCTATCCCTCCTGAAGAGATGTCTCCAGAATTCACAACTCTCCCAGAGTGTCATGGACATGCCCGGGATGCCTTTTCATTTGGAACATTGGTGGAAAGTTTGCTCACAATCTTAAATGAACAGGTTTCAGCGGATGTTCTCTCCAGCTTTCAACAGACCTTGCACTCAACTTTGCTGAATCCCATTCCAAAATGTCGGCCAGCGCTCTGCACCTTACTATCTCATGACTTCTTCAGAAATGATTTTCTGGAAGTTGTGAATTTCTTGAAAAGTTTAACATTGAAGAGTGAAGAGGAGAAAACGGAATTCTTTAAATTTCTGCTGGACAGAGTCAGCTGCTTGTCAGAGGAATTGATAGCTTCAAGGTTGGTGCCTCTTCTGCTTAATCAGTTGGTGTTTGCAGAGCCAGTGGCTGTTAAGAGTTTTCTTCCTTATCTGCTTGGCCCCAAAAAAGATCATGCGCAGGGAGAAACTCCTTGCTTGCTCTCACCAGCCCTGTTCCAGTCACGGGTGATCCCCGTGCTTCTCCAGTTGTTTGAAGTTCATGAAGAGCATGTGCGGATGGTGCTGCTGTCTCACATCGAGGCCTACGTGGAGCACTTCACTCAGGAGCAGCTGAAGAAAGTCATCTTGCCACAGGTTTTGCTGGGCCTGCGTGATACTAGCGATTCCATTGTGGCAATTACTCTGCATAGCCTAGCAGTGCTGGTCTCTCTGCTTGGACCAGAGGTGGTTGTGGGAGGAGAACGAACCAAGATCTTCAAACGCACTGCCCCAAGTTTTACTAAAAATACTGACCTTTCTCTAGAAGATTCTCCTATGTGTGTCGTCTGCAGCCATCACAGTCAGATCTCGCCAATCTTGGAGAACCCCTTCTCTAGCATATTCCCTAAATGTTTCTTTTCTGGCAGCACGCCCATCAACAGCAAGAAGCACATACAGCGAGATTACTACAATACTCTTTTACAGACAGGCGATCCATTTTCTCAGCCTATTAAATTTCCCATAAATGGACTCTCAGATGTAAAAAATACTTCGGAGGACAGTGAAAACTTCCCATCAAGTTCTAAAAAGTCTGAGGAGTGGCCTGACTGGAGTGAACCTGAGGAGCCTGAAAATCAAACTGTCAACATACAGATTTGGCCTAGAGAACCTTGTGATGATGTCAAGTCCCAGTGCACTACCTTGGATGTGGAAGAGTCATCTTGGGATGACTGCGAGCCCAGCAGCTTAGATACTAAAGTAAACCCAGGAGGTGGAATCACTGCTACAAAACCTGTTACCTCAGGGGAGCAGAAGCCTATTCCTGCTTTGCTTTCACTCACTGAAGAGTCTATGCCTTGGAAATCAAGCTTACCCCAAAAGATTAGCCTTGTACAAAGGGGGGATGACGCAGACCAAATCGAGCCGCCAAAAGTGTCATCACAAGAAAGGCCCCTTAAGGTTCCATCAGAACTTGGTTTAGGAGAGGAATTCACCATTCAAGTAAAAAAGAAGCCAGTAAAAGATCCTGAGATGGATTGGTTTGCTGATATGATCCCAGAAATTAAGCCTTCTGCTGCTTTTCTTATATTACCTGAACTGAGGACAGAAATGGTCCCAAAAAAGGATGATGTCTCCCCAGTGATGCAGTTTTCCTCAAAATTTGCTGCAGCAGAAATTACTGAGGGAGAGGCTGAAGGCTGGGAAGAAGAAGGGGAGCTGAACTGGGAAGATAATAACTGGCAATAGATGTGAGTTAAACTTTAGGAAAAAGGATTCCCTTTTTTTAAAAAAAATCAATACCTCAAAAGCAGGCTTTGGGACAAGAAAACCCCAAAGTGGCCTGCTTTTCCCATCCCAGGAGCTCATTATCCAGTCTGTGCCAACTGAAGTAGGAGACTGACTGTGAGTGCTGGCTAAAAGCCCTGGGTGGTGAGGCTCACAGTACTGGTTTCCAGGAGGAAGAGCCTTTGTGCATTTGACTGAGGCCAGTTTCTATGAAGAGCAAGTAGCTGAGGAGAGGTCGAATTTACTGCTTTTTCCAGGACAATTCTGGAAGTAAAGAAAATGTAATTCAAGCTGGTTAGCTTAATTTTGTGCCATTCTTTAACATAAGAGTAAGCTCTATTATGAAATACAACTTTAAAAAATTTTAGCTATAAATTATATAAATGATTTTAAATTGCTGAGGTTTCCTTAGGCAGCTTATTTATTTGTTTACAGTTAGACTATCTGAGTAAATGGTTCTTTGTGGACCTAGGCAGTTCCTGACTGTTCCACATGTAGTACATTGTACCAAAGTTCTTAATAAGAATATTCCCCACAATCCTGTTCTCTAAATGTCAAATAAAGATTATTTTCACTAGATTCAACTTTACAAAANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN'\n",
            " 'GGCTTTGGCCCTGGAAAGCCTCGCGGACGTGTTCTGACCCAAGGTTTTAGCAGTGGATGTGGCGTTTTCTTCCATTCCTTCTTTCAGTTTTTCTGTACTCGTTGCTTGCAATTAAGTGTAAATACTTTTGCTAGTGGATAATGGGGGAGGCAAGGACTGAGACCTGCGGTATGACGATAGCTCTGGCTCTTAATAGTTTGAGGTAAAGCGAGATACTCTGAGCTTTTGTCTCCCGTAAAAAGGGTGGTGAATATGAATAAGGGCTTTCTTAGCGTTATAAGAATTAAAGGGCATAGTTCTGTGGTGTGAAATCTTTAAAAGATGTTCAGTAAATAAAAATGATTTTCCTCCTTCCCCTCTCAGACCTCTTTTTCTTCTTTCTTTCTTTTTTTTTGACAAGTTCTCACTCCTCTCACCCAGGCTGGAGTCTTTCTGAAAGAGTTCTTCCGCTTGTTGTTGGCTTTCAACTGTTGGATTTGAGGCGCTTAGCGCCTTCTTCGTCCGGGTGCAGCACATTCTTGATTGGTCTCATGCCTTTGTGGTTGTAAATGTGCCTGGAATCCTAGCCTTTCATGGTTTGTTCTGAGTAATGAATACCCTATTACTATGATACTAGTATCTTCCTTAATTATCCTACTCATTGTCTCAACATTCTGACAGTTGGATTGAGCATATTCAAATTTTGAAAATTATTGTAGAAATGTTTTTACCTCATATGAACCACCTGACATTGGAACAGACTTTCTTTTCACAAGTGTTACCAAAGACTGTGAAATTATTCGATGACATGATGTATGAATTAACCAGTCAAGCCAGAGGACTGTCAAGCCAAAATTTGGAAATCCAGACCACTCTAAGGAATATTTTACAAACAATGGTGCAGCTCTTAGGAGCTCTCACAGGATGTGTTCAGCATATCTGTGCCACACAGGAATCCATCATTTTGGAAAATATTCAGAGTCTCCCCTCCTCAGTCCTTCATATAATTAAAAGCACATTTGTGCATTGTAAGAATAGTGAATCTGTGTATTCTGGGTGTTTACACCTAGTTTCAGACCTTCTCCAGGCTCTTTTCAAGGAGGCCTATTCTCTTCAAAAGCAGTTAATGGAACTGCTGGACATGGTTTGCATGGACCCTTTAGTAGATGACAATGATGATATTTTGAATATGGTAATAGTTATTCATTCTTTATTGGATATCTGCTCTGTTATTTCCAGTATGGACCATGCATTTCATGCCAATACTTGGAAGTTTATAATTAAGCAGAGCCTTAAGCACCAGTCCATAATAAAAAGCCAGTTGAAACACAAAGATATAATTACTAGCTTGTGTGAAGACATTCTTTTCTCCTTCCATTCTTGTTTACAGTTAGCTGAGCAGATGACACAGTCAGATGCACAGGATAATGCTGACTACAGATTATTTCAGAAAACACTCAAATTGTGTCGTTTTTTTGCCAACTCCCTTTTGCACTACGCTAAGGAATTTCTTCCTTTCCTCTCTGATTCTTGCTGTACTTTGCACCAACTGTATCTTCAGATACACAGCAAGTTTCCTCCAAGCCTTTATGCTACCAGGATTTCTAAAGCACACCAAGAGGAAATAGCAGGTGCTTTCCTAGTGACACTGGATCCACTTATCAGTCAGCTGCTCACATTTCAGCCTTTCATGCAGGTGGTTTTGGACAGTAAATTAGACCTGCCATGTGAACTGCAGTTTCCACAATGTCTTCTTCTGGTTGTTGTCATGGATAAGCTGCCATCTCAGCCTAAGGAAGTGCAAACCCTGTGGTGCACAGACAGCCAGGTCTCAGAAACGACAACCAGGATATCTCTACTCAAAGCCGTTTTCTACAGTTTTGAGCAGTGTTCTGGTGAACTCTCTCTACCTGTTCATTTACAGGGATTAAAGAGTAAGGGGAAAGCTGAGGTGGCTGTCACCTTGTATCAGCATGTTTGTGTTCATCTGTGTACATTTATTACTTCCTTTCATCCCTCACTGTTTGCTGAACTGGATGCTGCTCTGCTGAATGCTGTACTTAGTGCTAATATGATCACCTCTTTGTTAGCTATGGATGCATGGTGCTTCCTTGCTCGATATGGGACTGCTGAACTGTGTGCACACCATGTCACCATAGTGGCTCATCTGATAAAGTCATGCCCTGGAGAATGTTATCAACTCATCAACCTATCAATACTGTTGAAGCGTCTCTTTTTCTTCATGGCACCACCCCATCAGCTGGAGTTTATCCAGAAATTTTCCCCAAAAGAAGCAGAAAATCTGCCTCTGTGGCAACATATTTCCTTCCAGGCGTTACCTCCTGAGCTTAGGGAACAAACTGTCCATGAGGTCACCACAGTAGGCACTGCAGAATGCAGGAAATGGCTGAGCAGGAGTCGTACTTTGGGAGAACTAGAATCTCTGAACACAGTACTGTCTGCTTTGCTTGCAGTATGTAATTCTGCTGGTGAAGCTTTGGATACAGGAAAACAAACTGCAATTATCGAAGTTGTGAGTCAGCTTTGGGCTTTTTTAAACATTAAACAGGTAGCAGATCAACCTTATGTTCAACAGACATTCAGCCTTTTACTTCCACTGTTGGGATTTTTCATTCAAACTCTAGATCCTAAACTGATACTTCAGGCAGTAACTTTGCAGACCTCGCTACTTAAATTAGAGCTTCCTGACTATGTTCGTTTGGCAATGTTGGATTTTGTATCTTCTTTAGGAAAACTTTTTATACCTGAAGCTATCCAGGACAGAATTCTGCCCAACCTGTCCTGTATGTTTGCCTTACTGCTAGCTGACAGGAGTTGGCTGCTAGAACAACATACCTTGGAGGCGTTTACTCAGTTCGCTGAGGGAACAAATCATGAAGAGATAGTTCCACAGTGTCTCAGTTCTGAAGAAACTAAGAACAAAGTTGTATCCTTTCTGGAGAAGACTGGGTTTGTAGATGAAACTGAAGCTGCCAAAGTGGAACGTGTGAAACAGGAAAAAGGTATTTTCTGGGAACCCTTTGCTAATGTGACTGTAGAAGAAGCAAAGAGGTCATCTTTACAGCCTTATGCAAAAAGAGCTCGTCAGGAGTTCCCCTGGGAAGAAGAGTACAGGTCAGCGCTGCATACAATAGCAGGGGCTTTGGAAGCAACTGAGTCACTACTCCAAAAGGGTCCTGCTCCAGCCTGGCTTTCAATGGAAATGGAGGCGCTCCAAGAAAGGATGGATAAGCTAAAACGTTACATACATACTCTAGGGAACTTATCACTAGGCAGAACTGGGTTTGATGCTTTGTCAACTGAAAATACTTATGTCTGTACATTTTCTAACAGATATAAAACAAATTTTGTAAAGTTGAATCTAGTGAAAATAATCTTTATTTGACATTTAGAGAACAGGATTGTGGGGAATATTCTTATTAAGAACTTTGGTACAATGTACTACATGTGGAACAGTCAGGAACTGCCTAGGTCCACAAAGAACCATTTACTCAGATAGTCTAACTGTAAACAAATAAATAAGCTGCCTAAGGAAACCTCAGCAATTTAAAATCATTTATATAATTTATAGCTAAAATTTTTTAAAGTTGTATTTCATAATAGAGCTTACTCTTATGTTAAAGAATGGCACAAAATTAAGCTAACCAGCTTGAATTACATTTTCTTTACTTCCAGAATTGTCCTGGAAAAAGCAGTAAATTCGACCTCTCCTCAGCTACTTGCTCTTCATAGAAACTGGCCTCAGTCAAATGCACAAAGGCTCTTCCTCCTGGAAACCAGTACTGTGAGCCTCACCACCCAGGGCTTTTAGCCAGCACTCACAGTCAGTCTCCTACTTCAGTTGGCACAGACTGGATAATGAGCTCCTGGGATGGGAAAAGCAGGCCACTTTGGGGTTTTCTTGTCCCAAAGCCTGCTTTTGAGGTATTGATTTTTTTTAAAAAAAGGGAATCCTTTTTCCTAAAGTTTAACTCACATCTATTGTCACCAGTTATTATCTTCCCAGTTCAGCTCCCCTTCTTCTTCCCAGCCTTCAGCCTCTCCCTGCAACAAAATAAAGCACACCAAGAACCCACTGAAACAAATCATATGCAAAAATCATACGCAAATTTGAAAAAGCAGGAATTTAAAATTTATCTTTTGATGCCAGAAACACTACCTCGTACTAAGTAAAATAACTTAGAGCTCTAACAGAAAGTTGAAAAGTAGGATTACAAAACCATGGCACTGGAAAATAGCTTTTCAAAAACCATAAAATCCAGTAAAAATCAGTGTGGATGACACCAAATCCTTATTTTAATCCCTTTTTTTTCTTTTTCAAATAAAAAGGTTACAATAGCTCATTAAACAAANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN'\n",
            " 'GGCTTGGGGCTAGGGCGTGACTGTCTCCCTGCCACCATCACCGCCCGCCGGCCGTGACTGCAATAAGAGAAGTCCGAGGCGGCTTCCTCCTCCCTGCCCAGCAGGGGCGGCGGTCAGAGGCGGGCAGCACCCCAGTTCTCCCCGCACGCCGGCACTCGCGGCTGCTGGAGCCCCGGCTGGCTCACCCCGGGGCCGGGCAGAATTGGGCTCCAGGTCTCTGACCCCTCCCAAGGATCATGCCGCAGCCCCACTGACCCAGGAGTAGGGGCCTAAGGGCAGGGAACCTGGAATGGGCTGTGTGTTCTGCAAGAAATTGGAGCCGGTGGCCACGGCCAAGGAGGATGCTGGCCTGGAAGGGGACTTCAGAAGCTACGGGGCAGCAGACCACTATGGGCCTGACCCCACTAAGGCCCGGCCTGCATCCTCATTTGCCCACATCCCCAACTACAGCAACTTCTCCTCTCAGGCCATCAACCCTGGCTTCCTTGATAGTGGCACCATCAGGGGTGTGTCAGGGATTGGGGTGACCCTGTTCATTGCCCTGTATGACTATGAGGCTCGAACTGAGGATGACCTCACCTTCACCAAGGGCGAGAAGTTCCACATCCTGAACAATACTGAAGGTGACTGGTGGGAGGCTCGGTCTCTCAGCTCCGGAAAAACTGGCTGCATTCCCAGCAACTACGTGGCCCCTGTTGACTCAATCCAAGCTGAAGAGTGGTACTTTGGAAAGATTGGGAGAAAGGATGCAGAGAGGCAGCTGCTTTCACCAGGCAACCCCCAGGGGGCCTTTCTCATTCGGGAAAGCGAGACCACCAAAGGTGCCTACTCCCTGTCCATCCGGGACTGGGATCAGACCAGAGGCGATCATGTGAAGCATTACAAGATCCGCAAACTGGACATGGGCGGCTACTACATCACCACACGGGTTCAGTTCAACTCGGTGCAGGAGCTGGTGCAGCACTACATGGAGGTGAATGACGGGCTGTGCAACCTGCTCATCGCGCCCTGCACCATCATGAAGCCGCAGACGCTGGGCCTGGCCAAGGACGCCTGGGAGATCAGCCGCAGCTCCATCACGCTGGAGCGCCGGCTGGGCACCGGCTGCTTCGGGGATGTGTGGCTGGGCACGTGGAACGGCAGCACTAAGGTGGCGGTGAAGACGCTGAAGCCGGGCACCATGTCCCCGAAGGCCTTCCTGGAGGAGGCGCAGGTCATGAAGCTGCTGCGGCACGACAAGCTGGTGCAGCTGTACGCCGTGGTGTCGGAGGAGCCCATCTACATCGTGACCGAGTTCATGTGTCACGGCAGCTTGCTGGATTTTCTCAAGAACCCAGAGGGCCAGGATTTGAGGCTGCCCCAATTGGTGGACATGGCAGCCCAGGTAGCTGAGGGCATGGCCTACATGGAACGCATGAACTACATTCACCGCGACCTGAGGGCAGCCAACATCCTGGTTGGGGAGCGGCTGGCGTGCAAGATCGCAGACTTTGGCTTGGCGCGTCTCATCAAGGACGATGAGTACAACCCCTGCCAAGGTTCCAAGTTCCCCATCAAGTGGACAGCCCCAGAAGCTGCCCTCTTTGGCAGATTCACCATCAAGTCAGACGTGTGGTCCTTTGGGATCCTGCTCACTGAGCTCATCACCAAGGGCCGAATCCCCTACCCAGGCATGAATAAACGGGAAGTGTTGGAACAGGTGGAGCAGGGCTACCACATGCCGTGCCCTCCAGGCTGCCCAGCATCCCTGTACGAGGCCATGGAACAGACCTGGCGTCTGGACCCGGAGGAGAGGCCTACCTTCGAGTACCTGCAGTCCTTCCTGGAGGACTACTTCACCTCCGCTGAACCACAGTACCAGCCCGGGGATCAGACACCTGTCCGGGCATCAACCCTCTCTGGCGGTGGCCACCAGTCCTTGCCAATCCCCAGAGCTGTTCTTCCAAAGCCCCCAGGCTGGCTTAGAACCCCATAGAGTCCTAGCATCACCGAGGACGTGGCTGCTCTGACACCACCTAGGGCAACCTACTTGTTTTACAGATGGGGCAAAAGGAGGCCCAGAGCTGATCTCTCATCCGCTCTGGCCCCAAGCACTATTTCTTCCTTTTCCACTTAGGCCCCTACATGCCTGTAGCCTTTCTCACTCCATCCCCACCCAAAGTGCTCAGACCTTGTCTAGTTATTTATAAAACTGTATGTACCTCCCTCACTTCTCTCCTATCACTGCTTTCCTACTCTCCTTTTATCTCACTCTAGTCCAGGTGCCAAGAATTTCCCTTCTACCCTCTATTCTCTTGTGTCTGTAAGTTACAAAGTCAGGAAAAGTCTTGGCTGGACCCCTTTCCTGCTGGGTGGATGCAGTGGTCCAGGACTGGGGTCTGGGCCCAGGTTTGAGGGAGAAGGTTGCAGAGCACTTCCCACCTCTCTGAATAGTGTGTATGTGTTGGTTTATTGATTCTGTAAATAAGTAAAATGACAATATGAATCCTCAAACCATGAAATACCCTTGAACCTTCCTTTGGGAGCGGGGGTGGTCAATAGGGGGTGAACGGACAGATATGGCTACAGGCAGCAGCAGGGGAAGCTGGAGAGGGCCCTAATGCCTACCAAGCACGGGGCATCCAAGGTGTGGAGTTTTAGAACACCCAGAGTCCCACTGCTCATCTGCACGTGAGTTTAGAAGACAAGCAGCTGAAGATACATTAAAATGTCCCCTTCGTTGCTGANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN']\n",
            "(13230,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot for track 1-4: the nucleotides \n",
        "\n",
        "one_hot_seqs = np.array([one_hot(seq, neutral_value=0) for seq in fixed_len_seqs])\n",
        "print(one_hot_seqs[0:2])\n",
        "print(one_hot_seqs.shape)\n",
        "rubbish = [fixed_len_seqs]\n",
        "del rubbish"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdyuPKVL_0QT",
        "outputId": "4d2a8600-70d5-4612-cb34-37ffe18066d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[1. 0. 0. 0.]\n",
            "  [0. 0. 1. 0.]\n",
            "  [0. 0. 0. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 1.]\n",
            "  [0. 0. 1. 0.]\n",
            "  [0. 0. 0. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]]]\n",
            "(13230, 10000, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot for track 5: the exon binding sites\n",
        "# dunno why, I guess I still suck at python, but this took me over an hours to code and bugfix\n",
        "# lol this is future me from the next day, this was wrong and I had redo all the training\n",
        "\n",
        "exons = []\n",
        "\n",
        "for i in range(len(sequences)):\n",
        "  onehot = np.repeat(0, repeats = max_len)\n",
        "  if(isinstance(sequences[\"Exon_Junctions_In_Full_Sequence\"][i], str)):\n",
        "    current_exons = list(map(int, sequences[\"Exon_Junctions_In_Full_Sequence\"][i].split(\",\")))\n",
        "    assert len(current_exons) > 0\n",
        "    positions_capped = [x for x in current_exons if x <= 10000] # delete all exon junctions after 10000 since we're capping the sequence there\n",
        "    onehot[positions_capped] = 1\n",
        "    '''\n",
        "    for j in current_exons:\n",
        "      positions = [x+len(sequences['UTR5_seqs'][i]) for x in current_exons] # have to add UTR5 length to indices\n",
        "      positions_capped = [x for x in positions if x <= 10000] # delete all exon junctions after 10000 since we're capping the sequence there\n",
        "      onehot[positions_capped] = 1 #exon junctions are 1 now\n",
        "      \n",
        "  if(isinstance(sequences[\"Exon_Junctions_In_Full_Sequence\"][i], float)):\n",
        "    if(not(math.isnan(sequences[\"Exon_Junctions_In_Full_Sequence\"][i]))):\n",
        "      onehot[int(sequences[\"Exon_Junctions_In_Full_Sequence\"][i])+len(sequences['UTR5_seqs'][i])] = 1\n",
        "    '''\n",
        "  exons.append(onehot)"
      ],
      "metadata": {
        "id": "27vK8R2SAD9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(map(int, sequences[\"Exon_Junctions_In_Full_Sequence\"][1].split(\",\"))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDsHa06rCEQq",
        "outputId": "1efd32db-ccb9-4036-d6e5-c5c9be5892c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[387, 573, 687, 744, 847, 959, 1037, 1177, 1362, 1534, 1696, 2391]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#one hot for track 6: Marking the beginning of each codon with 1\n",
        "starts = []\n",
        "for i in range(len(sequences)):\n",
        "  #assert len(sequences['ORF_seqs'].astype(\"string\")[i]) % 3 == 0 \n",
        "  lst = list(range(len(sequences['ORF_seqs'].astype(\"string\")[i])))\n",
        "  onehot = np.repeat(0, repeats = len(sequences['ORF_seqs'].astype(\"string\")[i]))\n",
        "  onehot[lst[0::3]] = 1\n",
        "  full = np.concatenate((np.repeat([0], repeats = len(sequences['UTR5_seqs'].astype(\"string\")[i])),\n",
        "                         onehot,\n",
        "                         np.repeat([0], repeats = len(sequences['UTR3_seqs'].astype(\"string\")[i]))), axis=None)\n",
        "  if (len(full) > max_len):\n",
        "    full = full[:max_len]\n",
        "  elif (len(full) < max_len):\n",
        "    full = np.concatenate((full, np.repeat(0, repeats = max_len - len(full))),axis = None)\n",
        "  starts.append(full)"
      ],
      "metadata": {
        "id": "pRuNTsz4AHLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rubbish = [fixed_len_seqs]\n",
        "\n",
        "rubbish = [fixed_len_seqs, d, UTR3, ORF, UTR5, UTR5_seqs, UTR3_seqs, ORF_seqs, hl,\n",
        "           UTR3_identifiers, UTR5_identifiers, ORF_identifiers, halflife, max_len]\n",
        "del rubbish"
      ],
      "metadata": {
        "id": "lt-QVEBiP5Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This takes about 18 GB, so beware of that\n",
        "onehot = np.concatenate((one_hot_seqs,np.array(exons)[:, :, None], np.array(starts)[:, :, None]), axis = 2)\n",
        "print(onehot.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZpZoaGuPp3c",
        "outputId": "fa729fb7-c6c2-4e58-d175-e0bc7989ad82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13230, 10000, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del exons\n",
        "del starts"
      ],
      "metadata": {
        "id": "8qpC7Tr5RZae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for The Train-Val-Test split we split as recommended on Chromosomes:"
      ],
      "metadata": {
        "id": "oM3U9VTxSy0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chrom_val = ['2', '3', '4']\n",
        "chrom_test = ['1', '8', '9']"
      ],
      "metadata": {
        "id": "Ih4OOJxISyJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_test = np.where(sequences.Chromosome.isin(chrom_test))[0]\n",
        "idx_val = np.where(sequences.Chromosome.isin(chrom_val))[0]\n",
        "idx_train = np.where(~(sequences.Chromosome.isin(chrom_test)| sequences.Chromosome.isin(chrom_val)))[0]"
      ],
      "metadata": {
        "id": "FQcQCI4BTFSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(array, idx_train, idx_val, idx_test):\n",
        "  return array[idx_train], array[idx_val], array[idx_test]"
      ],
      "metadata": {
        "id": "k7GWuUtuTJxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(idx_test))\n",
        "print(len(idx_val))\n",
        "print(len(idx_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOrm43s0Z8M2",
        "outputId": "0eeb4910-71b4-4c73-872a-57cf582368c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2334\n",
            "2118\n",
            "8778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, X_test = train_test_split(onehot, idx_train, idx_val, idx_test)\n",
        "y_train, y_val, y_test = train_test_split(sequences['zscore'].values, idx_train, idx_val, idx_test)"
      ],
      "metadata": {
        "id": "mmpWExHRTfMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExplainedVariance(keras.callbacks.Callback):\n",
        "    def __init__(self, validation_data=(), interval=10):\n",
        "        super(keras.callbacks.Callback, self).__init__()\n",
        "\n",
        "        self.interval = interval\n",
        "        self.X_val, self.y_val = validation_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.interval == 0:\n",
        "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "            var_score = explained_variance_score(self.y_val, y_pred)\n",
        "            #r2 = r2_score(self.y_val, y_pred)\n",
        "            del y_pred\n",
        "            #print(\" - interval evaluation - epoch: {:d} - explained variance: {:.6f} - R2: {:.6f}\".format(epoch, var_score, r2))\n",
        "            print(\"interval evaluation - epoch: {:d} - explained variance: {:.6f}\".format(epoch, var_score))\n",
        "        gc.collect()"
      ],
      "metadata": {
        "id": "xf5ZZA63U-qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saluki-Type model\n",
        "input = kl.Input((X_train.shape[1:]))\n",
        "\n",
        "x = kl.Conv1D(64, kernel_size=5, activation=None, kernel_regularizer=tf.keras.regularizers.l2(l=0.0015))(input)\n",
        "x = tfa.layers.GroupNormalization(groups = 1, axis = 2)(x)\n",
        "x = kl.Activation(\"relu\")(x)\n",
        "\n",
        "for i in range(6):\n",
        "  x1 = kl.Conv1D(32, kernel_size=5, padding=\"same\", activation=None, kernel_regularizer=tf.keras.regularizers.l2(l=0.0015))(x)\n",
        "  x1 = kl.MaxPooling1D(pool_size=2)(x1)\n",
        "  x1 = tfa.layers.GroupNormalization(groups = 1, axis = 2)(x1)\n",
        "  x1 = kl.Activation(\"relu\")(x1)\n",
        "  x1 = kl.Dropout(0.33)(x1)\n",
        "\n",
        "  x2 = kl.Conv1D(32, kernel_size=3, padding = \"same\", activation=None, kernel_regularizer=tf.keras.regularizers.l2(l=0.0015))(x)\n",
        "  x2 = kl.MaxPooling1D(pool_size=2)(x2)\n",
        "  x2 = tfa.layers.GroupNormalization(groups = 1, axis = 2)(x2)\n",
        "  x2 = kl.Activation(\"relu\")(x2)\n",
        "  x2 = kl.Dropout(0.33)(x2)\n",
        "\n",
        "  x3 = kl.MaxPooling1D(pool_size=2)(x)\n",
        "  x3 = kl.Conv1D(16, kernel_size=1, padding='same', activation=None, kernel_regularizer=tf.keras.regularizers.l2(l=0.0015))(x3)\n",
        "  x3 = tfa.layers.GroupNormalization(groups = 1, axis = 2)(x3)\n",
        "  x3 = kl.Activation(\"relu\")(x3)\n",
        "  x3 = kl.Dropout(0.33)(x3)\n",
        "\n",
        "  x4 = kl.Conv1D(16, kernel_size=1, padding='same', activation=None, kernel_regularizer=tf.keras.regularizers.l2(l=0.0015))(x)\n",
        "  x4 = kl.MaxPooling1D(pool_size=2)(x4)\n",
        "  x4 = tfa.layers.GroupNormalization(groups = 1, axis = 2)(x4)\n",
        "  x4 = kl.Activation(\"relu\")(x4)\n",
        "  x4 = kl.Dropout(0.33)(x4)\n",
        "\n",
        "  x = kl.concatenate([x1, x2, x3, x4], axis = 2)\n",
        "\n",
        "x = kl.GRU(80, go_backwards=True, kernel_regularizer=tf.keras.regularizers.l2(l=0.001))(x)\n",
        "x = kl.Dropout(0.33)(x)\n",
        "x = kl.BatchNormalization()(x)\n",
        "x = kl.Activation(\"relu\")(x)\n",
        "\n",
        "x = kl.Dense(96, kernel_regularizer=tf.keras.regularizers.l2(l=0.001))(x) #backwards so it encounters padding first\n",
        "#x = kl.Dropout(0.3)(x)\n",
        "#x = kl.BatchNormalization()(x) #batch is fine here, no padding to consider anymore\n",
        "x = kl.Activation(\"relu\")(x)\n",
        "\n",
        "output = kl.Dense(units=1)(x)\n",
        "\n",
        "my_saluki = Model(inputs=input, outputs=output)\n",
        "my_saluki.summary()\n",
        "\n",
        "#Saluki: We chose layer normalization over batch normalization because most of the 3′ positions are zero padded and would confuse the batch statistics.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0vIgaHIVpyI",
        "outputId": "e265c3b4-d422-4646-a8f3-b8ca38b80ac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 10000, 6)]   0           []                               \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 9996, 64)     1984        ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " group_normalization (GroupNorm  (None, 9996, 64)    128         ['conv1d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 9996, 64)     0           ['group_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 9996, 32)     10272       ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 9996, 32)     6176        ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 4998, 64)    0           ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 9996, 16)     1040        ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 4998, 32)     0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 4998, 32)    0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 4998, 16)     1040        ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " max_pooling1d_3 (MaxPooling1D)  (None, 4998, 16)    0           ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_1 (GroupNo  (None, 4998, 32)    64          ['max_pooling1d[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " group_normalization_2 (GroupNo  (None, 4998, 32)    64          ['max_pooling1d_1[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " group_normalization_3 (GroupNo  (None, 4998, 16)    32          ['conv1d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " group_normalization_4 (GroupNo  (None, 4998, 16)    32          ['max_pooling1d_3[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 4998, 32)     0           ['group_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 4998, 32)     0           ['group_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 4998, 16)     0           ['group_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 4998, 16)     0           ['group_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 4998, 32)     0           ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 4998, 32)     0           ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 4998, 16)     0           ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 4998, 16)     0           ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 4998, 96)     0           ['dropout[0][0]',                \n",
            "                                                                  'dropout_1[0][0]',              \n",
            "                                                                  'dropout_2[0][0]',              \n",
            "                                                                  'dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 4998, 32)     15392       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 4998, 32)     9248        ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling1d_6 (MaxPooling1D)  (None, 2499, 96)    0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 4998, 16)     1552        ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling1d_4 (MaxPooling1D)  (None, 2499, 32)    0           ['conv1d_5[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_5 (MaxPooling1D)  (None, 2499, 32)    0           ['conv1d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 2499, 16)     1552        ['max_pooling1d_6[0][0]']        \n",
            "                                                                                                  \n",
            " max_pooling1d_7 (MaxPooling1D)  (None, 2499, 16)    0           ['conv1d_8[0][0]']               \n",
            "                                                                                                  \n",
            " group_normalization_5 (GroupNo  (None, 2499, 32)    64          ['max_pooling1d_4[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " group_normalization_6 (GroupNo  (None, 2499, 32)    64          ['max_pooling1d_5[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " group_normalization_7 (GroupNo  (None, 2499, 16)    32          ['conv1d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " group_normalization_8 (GroupNo  (None, 2499, 16)    32          ['max_pooling1d_7[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 2499, 32)     0           ['group_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 2499, 32)     0           ['group_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 2499, 16)     0           ['group_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 2499, 16)     0           ['group_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 2499, 32)     0           ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 2499, 32)     0           ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 2499, 16)     0           ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 2499, 16)     0           ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 2499, 96)     0           ['dropout_4[0][0]',              \n",
            "                                                                  'dropout_5[0][0]',              \n",
            "                                                                  'dropout_6[0][0]',              \n",
            "                                                                  'dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 2499, 32)     15392       ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 2499, 32)     9248        ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_10 (MaxPooling1D  (None, 1249, 96)    0           ['concatenate_1[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 2499, 16)     1552        ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_8 (MaxPooling1D)  (None, 1249, 32)    0           ['conv1d_9[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_9 (MaxPooling1D)  (None, 1249, 32)    0           ['conv1d_10[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 1249, 16)     1552        ['max_pooling1d_10[0][0]']       \n",
            "                                                                                                  \n",
            " max_pooling1d_11 (MaxPooling1D  (None, 1249, 16)    0           ['conv1d_12[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " group_normalization_9 (GroupNo  (None, 1249, 32)    64          ['max_pooling1d_8[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " group_normalization_10 (GroupN  (None, 1249, 32)    64          ['max_pooling1d_9[0][0]']        \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_11 (GroupN  (None, 1249, 16)    32          ['conv1d_11[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_12 (GroupN  (None, 1249, 16)    32          ['max_pooling1d_11[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1249, 32)     0           ['group_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 1249, 32)     0           ['group_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 1249, 16)     0           ['group_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 1249, 16)     0           ['group_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 1249, 32)     0           ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 1249, 32)     0           ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 1249, 16)     0           ['activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 1249, 16)     0           ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 1249, 96)     0           ['dropout_8[0][0]',              \n",
            "                                                                  'dropout_9[0][0]',              \n",
            "                                                                  'dropout_10[0][0]',             \n",
            "                                                                  'dropout_11[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 1249, 32)     15392       ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 1249, 32)     9248        ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_14 (MaxPooling1D  (None, 624, 96)     0           ['concatenate_2[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)             (None, 1249, 16)     1552        ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_12 (MaxPooling1D  (None, 624, 32)     0           ['conv1d_13[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling1d_13 (MaxPooling1D  (None, 624, 32)     0           ['conv1d_14[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 624, 16)      1552        ['max_pooling1d_14[0][0]']       \n",
            "                                                                                                  \n",
            " max_pooling1d_15 (MaxPooling1D  (None, 624, 16)     0           ['conv1d_16[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " group_normalization_13 (GroupN  (None, 624, 32)     64          ['max_pooling1d_12[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_14 (GroupN  (None, 624, 32)     64          ['max_pooling1d_13[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_15 (GroupN  (None, 624, 16)     32          ['conv1d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_16 (GroupN  (None, 624, 16)     32          ['max_pooling1d_15[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 624, 32)      0           ['group_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 624, 32)      0           ['group_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 624, 16)      0           ['group_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 624, 16)      0           ['group_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 624, 32)      0           ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 624, 32)      0           ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)           (None, 624, 16)      0           ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 624, 16)      0           ['activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 624, 96)      0           ['dropout_12[0][0]',             \n",
            "                                                                  'dropout_13[0][0]',             \n",
            "                                                                  'dropout_14[0][0]',             \n",
            "                                                                  'dropout_15[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_17 (Conv1D)             (None, 624, 32)      15392       ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_18 (Conv1D)             (None, 624, 32)      9248        ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_18 (MaxPooling1D  (None, 312, 96)     0           ['concatenate_3[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_20 (Conv1D)             (None, 624, 16)      1552        ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_16 (MaxPooling1D  (None, 312, 32)     0           ['conv1d_17[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling1d_17 (MaxPooling1D  (None, 312, 32)     0           ['conv1d_18[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_19 (Conv1D)             (None, 312, 16)      1552        ['max_pooling1d_18[0][0]']       \n",
            "                                                                                                  \n",
            " max_pooling1d_19 (MaxPooling1D  (None, 312, 16)     0           ['conv1d_20[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " group_normalization_17 (GroupN  (None, 312, 32)     64          ['max_pooling1d_16[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_18 (GroupN  (None, 312, 32)     64          ['max_pooling1d_17[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_19 (GroupN  (None, 312, 16)     32          ['conv1d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_20 (GroupN  (None, 312, 16)     32          ['max_pooling1d_19[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 312, 32)      0           ['group_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 312, 32)      0           ['group_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 312, 16)      0           ['group_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 312, 16)      0           ['group_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 312, 32)      0           ['activation_17[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)           (None, 312, 32)      0           ['activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)           (None, 312, 16)      0           ['activation_19[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_19 (Dropout)           (None, 312, 16)      0           ['activation_20[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 312, 96)      0           ['dropout_16[0][0]',             \n",
            "                                                                  'dropout_17[0][0]',             \n",
            "                                                                  'dropout_18[0][0]',             \n",
            "                                                                  'dropout_19[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_21 (Conv1D)             (None, 312, 32)      15392       ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_22 (Conv1D)             (None, 312, 32)      9248        ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_22 (MaxPooling1D  (None, 156, 96)     0           ['concatenate_4[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_24 (Conv1D)             (None, 312, 16)      1552        ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_20 (MaxPooling1D  (None, 156, 32)     0           ['conv1d_21[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling1d_21 (MaxPooling1D  (None, 156, 32)     0           ['conv1d_22[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_23 (Conv1D)             (None, 156, 16)      1552        ['max_pooling1d_22[0][0]']       \n",
            "                                                                                                  \n",
            " max_pooling1d_23 (MaxPooling1D  (None, 156, 16)     0           ['conv1d_24[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " group_normalization_21 (GroupN  (None, 156, 32)     64          ['max_pooling1d_20[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_22 (GroupN  (None, 156, 32)     64          ['max_pooling1d_21[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_23 (GroupN  (None, 156, 16)     32          ['conv1d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " group_normalization_24 (GroupN  (None, 156, 16)     32          ['max_pooling1d_23[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 156, 32)      0           ['group_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 156, 32)      0           ['group_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 156, 16)      0           ['group_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 156, 16)      0           ['group_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_20 (Dropout)           (None, 156, 32)      0           ['activation_21[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_21 (Dropout)           (None, 156, 32)      0           ['activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_22 (Dropout)           (None, 156, 16)      0           ['activation_23[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_23 (Dropout)           (None, 156, 16)      0           ['activation_24[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 156, 96)      0           ['dropout_20[0][0]',             \n",
            "                                                                  'dropout_21[0][0]',             \n",
            "                                                                  'dropout_22[0][0]',             \n",
            "                                                                  'dropout_23[0][0]']             \n",
            "                                                                                                  \n",
            " gru (GRU)                      (None, 80)           42720       ['concatenate_5[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_24 (Dropout)           (None, 80)           0           ['gru[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 80)          320         ['dropout_24[0][0]']             \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 80)           0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 96)           7776        ['activation_25[0][0]']          \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 96)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            97          ['activation_26[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 211,425\n",
            "Trainable params: 211,265\n",
            "Non-trainable params: 160\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUAbPQFOV3mD",
        "outputId": "a23bf18d-ea37-44f4-88e0-63988a792d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "import datetime"
      ],
      "metadata": {
        "id": "aIP6jd9lV5o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# So my GPU-memory is somewhat unstable and crashes for batchsize 16 or bigger"
      ],
      "metadata": {
        "id": "mo9tczmEapVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"best_weights27\",\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)"
      ],
      "metadata": {
        "id": "GU-VeebSvhTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "my_saluki.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.985, clipnorm=0.5),\n",
        "              loss=\"mse\")\n",
        "\n",
        "logdir = os.path.join(os.path.join(os.getcwd(), \"logs\"), datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1, write_graph=True)\n",
        "\n",
        "# Train the model\n",
        "history = my_saluki.fit(X_train, y_train,\n",
        "                        #X_train[0:200,:], y_train[0:200], \n",
        "                        validation_data=(X_val, y_val),\n",
        "                        #validation_data=(X_val[0:100,:], y_val[0:100]),\n",
        "                        callbacks=[EarlyStopping(patience=25, restore_best_weights=True),   \n",
        "                                   History(),\n",
        "                                   ExplainedVariance(validation_data=(X_val, y_val), interval=4),\n",
        "                                   tensorboard_callback,\n",
        "                                   model_checkpoint_callback],\n",
        "                        batch_size=32,  #they used 64\n",
        "                        epochs=1000)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t13N24J-V8eq",
        "outputId": "27944902-85fa-4127-c11c-c345f338418e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "  6/275 [..............................] - ETA: 26s - loss: 2.8947WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0630s vs `on_train_batch_end` time: 0.0847s). Check your callbacks.\n",
            "275/275 [==============================] - ETA: 0s - loss: 2.2071interval evaluation - epoch: 0 - explained variance: 0.024232\n",
            "275/275 [==============================] - 54s 139ms/step - loss: 2.2071 - val_loss: 1.6416\n",
            "Epoch 2/1000\n",
            "275/275 [==============================] - 34s 122ms/step - loss: 1.4603 - val_loss: 1.1557\n",
            "Epoch 3/1000\n",
            "275/275 [==============================] - 52s 189ms/step - loss: 1.1634 - val_loss: 0.9611\n",
            "Epoch 4/1000\n",
            "275/275 [==============================] - 89s 325ms/step - loss: 1.0505 - val_loss: 0.9114\n",
            "Epoch 5/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.9203interval evaluation - epoch: 4 - explained variance: 0.282598\n",
            "275/275 [==============================] - 106s 385ms/step - loss: 0.9203 - val_loss: 0.7468\n",
            "Epoch 6/1000\n",
            "275/275 [==============================] - 104s 380ms/step - loss: 0.7968 - val_loss: 0.7230\n",
            "Epoch 7/1000\n",
            "275/275 [==============================] - 106s 384ms/step - loss: 0.7449 - val_loss: 0.6679\n",
            "Epoch 8/1000\n",
            "275/275 [==============================] - 106s 387ms/step - loss: 0.7192 - val_loss: 0.7045\n",
            "Epoch 9/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.7190interval evaluation - epoch: 8 - explained variance: 0.308991\n",
            "275/275 [==============================] - 114s 416ms/step - loss: 0.7190 - val_loss: 0.6919\n",
            "Epoch 10/1000\n",
            "275/275 [==============================] - 106s 386ms/step - loss: 0.6996 - val_loss: 0.8206\n",
            "Epoch 11/1000\n",
            "275/275 [==============================] - 106s 386ms/step - loss: 0.6996 - val_loss: 0.6257\n",
            "Epoch 12/1000\n",
            "275/275 [==============================] - 106s 385ms/step - loss: 0.6839 - val_loss: 0.7084\n",
            "Epoch 13/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6883interval evaluation - epoch: 12 - explained variance: 0.342188\n",
            "275/275 [==============================] - 114s 416ms/step - loss: 0.6883 - val_loss: 0.6363\n",
            "Epoch 14/1000\n",
            "275/275 [==============================] - 106s 387ms/step - loss: 0.6766 - val_loss: 0.6439\n",
            "Epoch 15/1000\n",
            "275/275 [==============================] - 108s 391ms/step - loss: 0.6764 - val_loss: 0.6821\n",
            "Epoch 16/1000\n",
            "275/275 [==============================] - 107s 389ms/step - loss: 0.6699 - val_loss: 0.6986\n",
            "Epoch 17/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6671interval evaluation - epoch: 16 - explained variance: 0.359083\n",
            "275/275 [==============================] - 115s 420ms/step - loss: 0.6671 - val_loss: 0.8048\n",
            "Epoch 18/1000\n",
            "275/275 [==============================] - 106s 387ms/step - loss: 0.6591 - val_loss: 0.7803\n",
            "Epoch 19/1000\n",
            "275/275 [==============================] - 106s 386ms/step - loss: 0.6624 - val_loss: 0.6388\n",
            "Epoch 20/1000\n",
            "275/275 [==============================] - 106s 386ms/step - loss: 0.6567 - val_loss: 0.6810\n",
            "Epoch 21/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6479interval evaluation - epoch: 20 - explained variance: 0.291836\n",
            "275/275 [==============================] - 114s 415ms/step - loss: 0.6479 - val_loss: 1.4560\n",
            "Epoch 22/1000\n",
            "275/275 [==============================] - 106s 387ms/step - loss: 0.6434 - val_loss: 0.6821\n",
            "Epoch 23/1000\n",
            "275/275 [==============================] - 107s 388ms/step - loss: 0.6500 - val_loss: 1.3627\n",
            "Epoch 24/1000\n",
            "275/275 [==============================] - 104s 377ms/step - loss: 0.6414 - val_loss: 0.6308\n",
            "Epoch 25/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6417interval evaluation - epoch: 24 - explained variance: 0.362015\n",
            "275/275 [==============================] - 114s 416ms/step - loss: 0.6417 - val_loss: 0.6287\n",
            "Epoch 26/1000\n",
            "275/275 [==============================] - 106s 387ms/step - loss: 0.6443 - val_loss: 0.7005\n",
            "Epoch 27/1000\n",
            "275/275 [==============================] - 107s 388ms/step - loss: 0.6467 - val_loss: 0.5903\n",
            "Epoch 28/1000\n",
            "275/275 [==============================] - 105s 382ms/step - loss: 0.6338 - val_loss: 0.5968\n",
            "Epoch 29/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6334interval evaluation - epoch: 28 - explained variance: 0.399575\n",
            "275/275 [==============================] - 114s 415ms/step - loss: 0.6334 - val_loss: 0.6428\n",
            "Epoch 30/1000\n",
            "275/275 [==============================] - 106s 386ms/step - loss: 0.6370 - val_loss: 0.6916\n",
            "Epoch 31/1000\n",
            "275/275 [==============================] - 106s 387ms/step - loss: 0.6307 - val_loss: 0.7193\n",
            "Epoch 32/1000\n",
            "275/275 [==============================] - 106s 387ms/step - loss: 0.6256 - val_loss: 0.6219\n",
            "Epoch 33/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6174interval evaluation - epoch: 32 - explained variance: 0.383565\n",
            "275/275 [==============================] - 112s 407ms/step - loss: 0.6174 - val_loss: 0.6037\n",
            "Epoch 34/1000\n",
            "275/275 [==============================] - 104s 379ms/step - loss: 0.6310 - val_loss: 0.6190\n",
            "Epoch 35/1000\n",
            "275/275 [==============================] - 105s 382ms/step - loss: 0.6148 - val_loss: 0.7022\n",
            "Epoch 36/1000\n",
            "275/275 [==============================] - 105s 384ms/step - loss: 0.6122 - val_loss: 0.5833\n",
            "Epoch 37/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6151interval evaluation - epoch: 36 - explained variance: 0.391269\n",
            "275/275 [==============================] - 112s 409ms/step - loss: 0.6151 - val_loss: 0.5820\n",
            "Epoch 38/1000\n",
            "275/275 [==============================] - 106s 386ms/step - loss: 0.6227 - val_loss: 0.6845\n",
            "Epoch 39/1000\n",
            "275/275 [==============================] - 106s 387ms/step - loss: 0.6191 - val_loss: 0.6357\n",
            "Epoch 40/1000\n",
            "275/275 [==============================] - 106s 386ms/step - loss: 0.6126 - val_loss: 0.5971\n",
            "Epoch 41/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6146interval evaluation - epoch: 40 - explained variance: 0.352469\n",
            "275/275 [==============================] - 114s 415ms/step - loss: 0.6146 - val_loss: 0.6993\n",
            "Epoch 42/1000\n",
            "275/275 [==============================] - 105s 383ms/step - loss: 0.6054 - val_loss: 0.7613\n",
            "Epoch 43/1000\n",
            "275/275 [==============================] - 106s 388ms/step - loss: 0.6050 - val_loss: 0.5839\n",
            "Epoch 44/1000\n",
            "275/275 [==============================] - 105s 382ms/step - loss: 0.6094 - val_loss: 0.7769\n",
            "Epoch 45/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6041interval evaluation - epoch: 44 - explained variance: 0.422339\n",
            "275/275 [==============================] - 117s 425ms/step - loss: 0.6041 - val_loss: 0.5631\n",
            "Epoch 46/1000\n",
            "275/275 [==============================] - 110s 399ms/step - loss: 0.6152 - val_loss: 0.6651\n",
            "Epoch 47/1000\n",
            "275/275 [==============================] - 112s 406ms/step - loss: 0.6116 - val_loss: 0.5891\n",
            "Epoch 48/1000\n",
            "275/275 [==============================] - 112s 407ms/step - loss: 0.6073 - val_loss: 0.5912\n",
            "Epoch 49/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.6027interval evaluation - epoch: 48 - explained variance: 0.370125\n",
            "275/275 [==============================] - 121s 439ms/step - loss: 0.6027 - val_loss: 0.7567\n",
            "Epoch 50/1000\n",
            "275/275 [==============================] - 110s 399ms/step - loss: 0.6091 - val_loss: 0.6308\n",
            "Epoch 51/1000\n",
            "275/275 [==============================] - 106s 387ms/step - loss: 0.5993 - val_loss: 0.6566\n",
            "Epoch 52/1000\n",
            "275/275 [==============================] - 106s 387ms/step - loss: 0.5973 - val_loss: 0.7971\n",
            "Epoch 53/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.5975interval evaluation - epoch: 52 - explained variance: 0.404172\n",
            "275/275 [==============================] - 114s 415ms/step - loss: 0.5975 - val_loss: 0.5693\n",
            "Epoch 54/1000\n",
            "275/275 [==============================] - 106s 387ms/step - loss: 0.5997 - val_loss: 0.6912\n",
            "Epoch 55/1000\n",
            "275/275 [==============================] - 106s 387ms/step - loss: 0.5922 - val_loss: 0.5928\n",
            "Epoch 56/1000\n",
            "275/275 [==============================] - 106s 387ms/step - loss: 0.5898 - val_loss: 0.9596\n",
            "Epoch 57/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.5978interval evaluation - epoch: 56 - explained variance: 0.343858\n",
            "275/275 [==============================] - 114s 416ms/step - loss: 0.5978 - val_loss: 1.1697\n",
            "Epoch 58/1000\n",
            "275/275 [==============================] - 106s 387ms/step - loss: 0.5944 - val_loss: 0.5614\n",
            "Epoch 59/1000\n",
            "275/275 [==============================] - 106s 387ms/step - loss: 0.5905 - val_loss: 0.5863\n",
            "Epoch 60/1000\n",
            "275/275 [==============================] - 106s 387ms/step - loss: 0.5956 - val_loss: 0.6846\n",
            "Epoch 61/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.5893interval evaluation - epoch: 60 - explained variance: 0.377088\n",
            "275/275 [==============================] - 114s 415ms/step - loss: 0.5893 - val_loss: 0.5990\n",
            "Epoch 62/1000\n",
            "275/275 [==============================] - 106s 387ms/step - loss: 0.5931 - val_loss: 0.9532\n",
            "Epoch 63/1000\n",
            "275/275 [==============================] - 106s 387ms/step - loss: 0.6018 - val_loss: 0.6773\n",
            "Epoch 64/1000\n",
            "275/275 [==============================] - 106s 387ms/step - loss: 0.5920 - val_loss: 0.7429\n",
            "Epoch 65/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.5872interval evaluation - epoch: 64 - explained variance: 0.416535\n",
            "275/275 [==============================] - 115s 417ms/step - loss: 0.5872 - val_loss: 0.6689\n",
            "Epoch 66/1000\n",
            "275/275 [==============================] - 119s 432ms/step - loss: 0.5904 - val_loss: 0.6455\n",
            "Epoch 67/1000\n",
            "275/275 [==============================] - 126s 457ms/step - loss: 0.5886 - val_loss: 0.8503\n",
            "Epoch 68/1000\n",
            "275/275 [==============================] - 119s 432ms/step - loss: 0.5776 - val_loss: 0.6488\n",
            "Epoch 69/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.5812interval evaluation - epoch: 68 - explained variance: 0.357636\n",
            "275/275 [==============================] - 129s 469ms/step - loss: 0.5812 - val_loss: 1.1679\n",
            "Epoch 70/1000\n",
            "275/275 [==============================] - 120s 436ms/step - loss: 0.5839 - val_loss: 0.5945\n",
            "Epoch 71/1000\n",
            "275/275 [==============================] - 123s 448ms/step - loss: 0.5845 - val_loss: 0.6281\n",
            "Epoch 72/1000\n",
            "275/275 [==============================] - 118s 430ms/step - loss: 0.5889 - val_loss: 0.6621\n",
            "Epoch 73/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.5822interval evaluation - epoch: 72 - explained variance: 0.424724\n",
            "275/275 [==============================] - 122s 442ms/step - loss: 0.5822 - val_loss: 0.6071\n",
            "Epoch 74/1000\n",
            "275/275 [==============================] - 111s 404ms/step - loss: 0.5834 - val_loss: 0.6626\n",
            "Epoch 75/1000\n",
            "275/275 [==============================] - 120s 436ms/step - loss: 0.5838 - val_loss: 0.6509\n",
            "Epoch 76/1000\n",
            "275/275 [==============================] - 121s 439ms/step - loss: 0.5843 - val_loss: 0.7569\n",
            "Epoch 77/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.5789interval evaluation - epoch: 76 - explained variance: 0.416830\n",
            "275/275 [==============================] - 127s 461ms/step - loss: 0.5789 - val_loss: 0.6410\n",
            "Epoch 78/1000\n",
            "275/275 [==============================] - 113s 410ms/step - loss: 0.5816 - val_loss: 0.6560\n",
            "Epoch 79/1000\n",
            "275/275 [==============================] - 115s 418ms/step - loss: 0.5652 - val_loss: 1.1128\n",
            "Epoch 80/1000\n",
            "275/275 [==============================] - 121s 442ms/step - loss: 0.5820 - val_loss: 0.6138\n",
            "Epoch 81/1000\n",
            "275/275 [==============================] - ETA: 0s - loss: 0.5862interval evaluation - epoch: 80 - explained variance: 0.320773\n",
            "275/275 [==============================] - 130s 474ms/step - loss: 0.5862 - val_loss: 0.9835\n",
            "Epoch 82/1000\n",
            "275/275 [==============================] - 120s 436ms/step - loss: 0.5803 - val_loss: 0.7890\n",
            "Epoch 83/1000\n",
            "275/275 [==============================] - 120s 436ms/step - loss: 0.5717 - val_loss: 0.9876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = my_saluki.predict(X_val)\n",
        "print(\"Explained Var Score: %.2f\" % explained_variance_score(y_val, y_pred))\n",
        "print(\"R2 Score: %.2f\" % r2_score(y_val, y_pred))"
      ],
      "metadata": {
        "id": "7hCHlQnlWPVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3abb33a8-dc8f-4536-d925-a25385141228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "67/67 [==============================] - 8s 119ms/step\n",
            "Explained Var Score: 0.42\n",
            "R2 Score: 0.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_saluki.save('my_saluki5_27_v2.h5')"
      ],
      "metadata": {
        "id": "62thAYu_LH4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_loss(history):\n",
        "    fig, ax = plt.subplots(figsize = (5,5))\n",
        "    ax.plot(history['loss'][1:])\n",
        "    ax.plot(history['val_loss'][1:])\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('mean squared error')"
      ],
      "metadata": {
        "id": "76EPwtmyWN_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " plot_loss(history.history)"
      ],
      "metadata": {
        "id": "ser84xLLWSPq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "33e5c10d-a6b3-45fe-c733-4fc7cbd9df79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}