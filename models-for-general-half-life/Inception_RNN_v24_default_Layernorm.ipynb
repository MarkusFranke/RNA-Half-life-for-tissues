{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inception RNN v24 - default Layernorm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarkusFranke/RNA-Half-life-for-tissues/blob/main/models-for-general-half-life/Inception_RNN_v24_default_Layernorm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKkQxNHexGtb"
      },
      "outputs": [],
      "source": [
        "from Bio import SeqIO #for parsing Fasta Files\n",
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "from kipoiseq.transforms.functional import one_hot, fixed_len\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from sklearn.metrics import explained_variance_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import backend as k\n",
        "from keras.callbacks import EarlyStopping, History\n",
        "from keras.models import Model\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow as tf\n",
        "import keras.layers as kl\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import subprocess\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we can't run this in google colab due to RAM limitations, I'm importing the data from disk. All the data should be available on google drive though, under the same filenames, either to be downloaded and run with a path to where the user saved them, or to be directly accessible by mounting the google drive (and setting a shortcut to our google drive data path in google drive)\n"
      ],
      "metadata": {
        "id": "ku26rqE7x06J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hl = pd.read_excel(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\kelley_et_al_corrected_hl.xlsx', skiprows=[0, 1])\n",
        "hl['zscore'] = zscore(hl['half-life (PC1)'])\n",
        "halflife = hl[[\"Ensembl Gene Id\", \"zscore\"]]\n",
        "hl"
      ],
      "metadata": {
        "id": "7vRfq7bIxy4J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "5d9c0c32-24ee-4e67-bd25-992ee845e378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Ensembl Gene Id  Gene name  half-life (PC1)  Bazzini_ActD_HEK293_1  \\\n",
              "0      ENSG00000000003     TSPAN6         8.660955               0.763166   \n",
              "1      ENSG00000000419       DPM1         2.241221               0.529938   \n",
              "2      ENSG00000000457      SCYL3        -6.929173              -0.798471   \n",
              "3      ENSG00000000460   C1orf112         0.440909               0.461228   \n",
              "4      ENSG00000000938        FGR        -0.943680               0.164310   \n",
              "...                ...        ...              ...                    ...   \n",
              "13916  ENSG00000284770       TBCE         2.218664               0.100281   \n",
              "13917  ENSG00000285077  ARHGAP11B        -3.262964              -0.733980   \n",
              "13918  ENSG00000288596    C8orf44         2.118850              -0.485957   \n",
              "13919  ENSG00000288701     PRRC2B         0.133147               0.133770   \n",
              "13920  ENSG00000288722       F8A1        -3.485607              -0.607463   \n",
              "\n",
              "       Bazzini_ActD_HeLa_1  Bazzini_ActD_RPE_1  Bazzini_4sU_K562_1  \\\n",
              "0                 0.258448            0.106486            1.019072   \n",
              "1                 0.222678           -0.040666           -0.284952   \n",
              "2                -0.894854           -1.039150           -1.444532   \n",
              "3                 0.195794           -0.739672           -0.123925   \n",
              "4                 0.112064            0.095773            0.045345   \n",
              "...                    ...                 ...                 ...   \n",
              "13916            -0.187624           -0.143422            0.201024   \n",
              "13917            -0.934478           -0.952570           -0.461339   \n",
              "13918            -0.320560           -0.332189            0.582473   \n",
              "13919             0.214899            0.144491            0.090991   \n",
              "13920            -0.419258           -0.378650           -0.775774   \n",
              "\n",
              "       Akimitsu_BrU_HeLa_1  Rinn_ActD_K562_1  Rinn_ActD_K562_2  ...  \\\n",
              "0                 2.022504          1.744745          1.783356  ...   \n",
              "1                 0.145097          0.866919          0.832768  ...   \n",
              "2                -1.287191         -1.006317         -1.062201  ...   \n",
              "3                 0.162538         -0.023056         -0.008479  ...   \n",
              "4                 0.024136         -0.209157         -0.223700  ...   \n",
              "...                    ...               ...               ...  ...   \n",
              "13916             0.933352          0.465403          0.451758  ...   \n",
              "13917            -0.940048          0.225157          0.161858  ...   \n",
              "13918            -0.614775          0.305297          0.336730  ...   \n",
              "13919             0.131956         -0.040831         -0.035576  ...   \n",
              "13920             0.400149         -1.060379         -1.033201  ...   \n",
              "\n",
              "       Gejman_4sU_GM12812_1  Gejman_4sU_GM12814_1  Gejman_4sU_GM12815_1  \\\n",
              "0                  1.037361              0.969166              1.209562   \n",
              "1                 -0.212424             -0.747989              0.371214   \n",
              "2                 -1.155096             -1.421651             -1.568912   \n",
              "3                  1.365208              1.017193             -0.239569   \n",
              "4                  0.722198              1.024313              1.484018   \n",
              "...                     ...                   ...                   ...   \n",
              "13916              1.082013              0.917651              0.897480   \n",
              "13917              0.256969              0.033071             -0.249782   \n",
              "13918              1.023526             -0.155662             -0.950892   \n",
              "13919             -0.091563             -0.021327             -0.017414   \n",
              "13920              0.433892              0.446224              1.344124   \n",
              "\n",
              "       Simon_4sU_K562_1  Simon_4sU_K562_2  Rissland_4sU_HEK293_1  \\\n",
              "0              2.080643          2.095154               0.674058   \n",
              "1              0.772595          0.712843              -0.350914   \n",
              "2             -1.308978         -1.311572              -0.387172   \n",
              "3              0.373929          0.380154              -0.063720   \n",
              "4             -0.375671         -0.384504               0.232924   \n",
              "...                 ...               ...                    ...   \n",
              "13916          0.866792          0.868080              -0.104068   \n",
              "13917         -1.116405         -1.078302              -1.495412   \n",
              "13918         -0.175193         -0.166760               2.505294   \n",
              "13919          0.088581          0.107441               0.345123   \n",
              "13920         -0.631782         -0.629741              -0.413725   \n",
              "\n",
              "       Rissland_4sU_HEK293_2  Rissland_4sU_HEK293_3  Rissland_4sU_HEK293_4  \\\n",
              "0                   1.120375               1.456258               1.791769   \n",
              "1                  -0.879247              -0.825603               0.109861   \n",
              "2                  -1.229226              -1.122749              -0.570002   \n",
              "3                  -0.450610              -0.805719               0.453957   \n",
              "4                   0.347933               0.266619               0.063565   \n",
              "...                      ...                    ...                    ...   \n",
              "13916              -0.169414              -0.404886              -0.165188   \n",
              "13917              -1.006317              -1.240272              -1.440629   \n",
              "13918               1.068162               1.307039               1.563308   \n",
              "13919              -0.188176              -0.119128               0.001050   \n",
              "13920              -0.071227              -0.105036              -0.197549   \n",
              "\n",
              "         zscore  \n",
              "0      1.807620  \n",
              "1      0.467763  \n",
              "2     -1.446182  \n",
              "3      0.092022  \n",
              "4     -0.196955  \n",
              "...         ...  \n",
              "13916  0.463055  \n",
              "13917 -0.681011  \n",
              "13918  0.442223  \n",
              "13919  0.027789  \n",
              "13920 -0.727478  \n",
              "\n",
              "[13921 rows x 58 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ensembl Gene Id</th>\n",
              "      <th>Gene name</th>\n",
              "      <th>half-life (PC1)</th>\n",
              "      <th>Bazzini_ActD_HEK293_1</th>\n",
              "      <th>Bazzini_ActD_HeLa_1</th>\n",
              "      <th>Bazzini_ActD_RPE_1</th>\n",
              "      <th>Bazzini_4sU_K562_1</th>\n",
              "      <th>Akimitsu_BrU_HeLa_1</th>\n",
              "      <th>Rinn_ActD_K562_1</th>\n",
              "      <th>Rinn_ActD_K562_2</th>\n",
              "      <th>...</th>\n",
              "      <th>Gejman_4sU_GM12812_1</th>\n",
              "      <th>Gejman_4sU_GM12814_1</th>\n",
              "      <th>Gejman_4sU_GM12815_1</th>\n",
              "      <th>Simon_4sU_K562_1</th>\n",
              "      <th>Simon_4sU_K562_2</th>\n",
              "      <th>Rissland_4sU_HEK293_1</th>\n",
              "      <th>Rissland_4sU_HEK293_2</th>\n",
              "      <th>Rissland_4sU_HEK293_3</th>\n",
              "      <th>Rissland_4sU_HEK293_4</th>\n",
              "      <th>zscore</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ENSG00000000003</td>\n",
              "      <td>TSPAN6</td>\n",
              "      <td>8.660955</td>\n",
              "      <td>0.763166</td>\n",
              "      <td>0.258448</td>\n",
              "      <td>0.106486</td>\n",
              "      <td>1.019072</td>\n",
              "      <td>2.022504</td>\n",
              "      <td>1.744745</td>\n",
              "      <td>1.783356</td>\n",
              "      <td>...</td>\n",
              "      <td>1.037361</td>\n",
              "      <td>0.969166</td>\n",
              "      <td>1.209562</td>\n",
              "      <td>2.080643</td>\n",
              "      <td>2.095154</td>\n",
              "      <td>0.674058</td>\n",
              "      <td>1.120375</td>\n",
              "      <td>1.456258</td>\n",
              "      <td>1.791769</td>\n",
              "      <td>1.807620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENSG00000000419</td>\n",
              "      <td>DPM1</td>\n",
              "      <td>2.241221</td>\n",
              "      <td>0.529938</td>\n",
              "      <td>0.222678</td>\n",
              "      <td>-0.040666</td>\n",
              "      <td>-0.284952</td>\n",
              "      <td>0.145097</td>\n",
              "      <td>0.866919</td>\n",
              "      <td>0.832768</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.212424</td>\n",
              "      <td>-0.747989</td>\n",
              "      <td>0.371214</td>\n",
              "      <td>0.772595</td>\n",
              "      <td>0.712843</td>\n",
              "      <td>-0.350914</td>\n",
              "      <td>-0.879247</td>\n",
              "      <td>-0.825603</td>\n",
              "      <td>0.109861</td>\n",
              "      <td>0.467763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENSG00000000457</td>\n",
              "      <td>SCYL3</td>\n",
              "      <td>-6.929173</td>\n",
              "      <td>-0.798471</td>\n",
              "      <td>-0.894854</td>\n",
              "      <td>-1.039150</td>\n",
              "      <td>-1.444532</td>\n",
              "      <td>-1.287191</td>\n",
              "      <td>-1.006317</td>\n",
              "      <td>-1.062201</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.155096</td>\n",
              "      <td>-1.421651</td>\n",
              "      <td>-1.568912</td>\n",
              "      <td>-1.308978</td>\n",
              "      <td>-1.311572</td>\n",
              "      <td>-0.387172</td>\n",
              "      <td>-1.229226</td>\n",
              "      <td>-1.122749</td>\n",
              "      <td>-0.570002</td>\n",
              "      <td>-1.446182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ENSG00000000460</td>\n",
              "      <td>C1orf112</td>\n",
              "      <td>0.440909</td>\n",
              "      <td>0.461228</td>\n",
              "      <td>0.195794</td>\n",
              "      <td>-0.739672</td>\n",
              "      <td>-0.123925</td>\n",
              "      <td>0.162538</td>\n",
              "      <td>-0.023056</td>\n",
              "      <td>-0.008479</td>\n",
              "      <td>...</td>\n",
              "      <td>1.365208</td>\n",
              "      <td>1.017193</td>\n",
              "      <td>-0.239569</td>\n",
              "      <td>0.373929</td>\n",
              "      <td>0.380154</td>\n",
              "      <td>-0.063720</td>\n",
              "      <td>-0.450610</td>\n",
              "      <td>-0.805719</td>\n",
              "      <td>0.453957</td>\n",
              "      <td>0.092022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENSG00000000938</td>\n",
              "      <td>FGR</td>\n",
              "      <td>-0.943680</td>\n",
              "      <td>0.164310</td>\n",
              "      <td>0.112064</td>\n",
              "      <td>0.095773</td>\n",
              "      <td>0.045345</td>\n",
              "      <td>0.024136</td>\n",
              "      <td>-0.209157</td>\n",
              "      <td>-0.223700</td>\n",
              "      <td>...</td>\n",
              "      <td>0.722198</td>\n",
              "      <td>1.024313</td>\n",
              "      <td>1.484018</td>\n",
              "      <td>-0.375671</td>\n",
              "      <td>-0.384504</td>\n",
              "      <td>0.232924</td>\n",
              "      <td>0.347933</td>\n",
              "      <td>0.266619</td>\n",
              "      <td>0.063565</td>\n",
              "      <td>-0.196955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13916</th>\n",
              "      <td>ENSG00000284770</td>\n",
              "      <td>TBCE</td>\n",
              "      <td>2.218664</td>\n",
              "      <td>0.100281</td>\n",
              "      <td>-0.187624</td>\n",
              "      <td>-0.143422</td>\n",
              "      <td>0.201024</td>\n",
              "      <td>0.933352</td>\n",
              "      <td>0.465403</td>\n",
              "      <td>0.451758</td>\n",
              "      <td>...</td>\n",
              "      <td>1.082013</td>\n",
              "      <td>0.917651</td>\n",
              "      <td>0.897480</td>\n",
              "      <td>0.866792</td>\n",
              "      <td>0.868080</td>\n",
              "      <td>-0.104068</td>\n",
              "      <td>-0.169414</td>\n",
              "      <td>-0.404886</td>\n",
              "      <td>-0.165188</td>\n",
              "      <td>0.463055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13917</th>\n",
              "      <td>ENSG00000285077</td>\n",
              "      <td>ARHGAP11B</td>\n",
              "      <td>-3.262964</td>\n",
              "      <td>-0.733980</td>\n",
              "      <td>-0.934478</td>\n",
              "      <td>-0.952570</td>\n",
              "      <td>-0.461339</td>\n",
              "      <td>-0.940048</td>\n",
              "      <td>0.225157</td>\n",
              "      <td>0.161858</td>\n",
              "      <td>...</td>\n",
              "      <td>0.256969</td>\n",
              "      <td>0.033071</td>\n",
              "      <td>-0.249782</td>\n",
              "      <td>-1.116405</td>\n",
              "      <td>-1.078302</td>\n",
              "      <td>-1.495412</td>\n",
              "      <td>-1.006317</td>\n",
              "      <td>-1.240272</td>\n",
              "      <td>-1.440629</td>\n",
              "      <td>-0.681011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13918</th>\n",
              "      <td>ENSG00000288596</td>\n",
              "      <td>C8orf44</td>\n",
              "      <td>2.118850</td>\n",
              "      <td>-0.485957</td>\n",
              "      <td>-0.320560</td>\n",
              "      <td>-0.332189</td>\n",
              "      <td>0.582473</td>\n",
              "      <td>-0.614775</td>\n",
              "      <td>0.305297</td>\n",
              "      <td>0.336730</td>\n",
              "      <td>...</td>\n",
              "      <td>1.023526</td>\n",
              "      <td>-0.155662</td>\n",
              "      <td>-0.950892</td>\n",
              "      <td>-0.175193</td>\n",
              "      <td>-0.166760</td>\n",
              "      <td>2.505294</td>\n",
              "      <td>1.068162</td>\n",
              "      <td>1.307039</td>\n",
              "      <td>1.563308</td>\n",
              "      <td>0.442223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13919</th>\n",
              "      <td>ENSG00000288701</td>\n",
              "      <td>PRRC2B</td>\n",
              "      <td>0.133147</td>\n",
              "      <td>0.133770</td>\n",
              "      <td>0.214899</td>\n",
              "      <td>0.144491</td>\n",
              "      <td>0.090991</td>\n",
              "      <td>0.131956</td>\n",
              "      <td>-0.040831</td>\n",
              "      <td>-0.035576</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.091563</td>\n",
              "      <td>-0.021327</td>\n",
              "      <td>-0.017414</td>\n",
              "      <td>0.088581</td>\n",
              "      <td>0.107441</td>\n",
              "      <td>0.345123</td>\n",
              "      <td>-0.188176</td>\n",
              "      <td>-0.119128</td>\n",
              "      <td>0.001050</td>\n",
              "      <td>0.027789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13920</th>\n",
              "      <td>ENSG00000288722</td>\n",
              "      <td>F8A1</td>\n",
              "      <td>-3.485607</td>\n",
              "      <td>-0.607463</td>\n",
              "      <td>-0.419258</td>\n",
              "      <td>-0.378650</td>\n",
              "      <td>-0.775774</td>\n",
              "      <td>0.400149</td>\n",
              "      <td>-1.060379</td>\n",
              "      <td>-1.033201</td>\n",
              "      <td>...</td>\n",
              "      <td>0.433892</td>\n",
              "      <td>0.446224</td>\n",
              "      <td>1.344124</td>\n",
              "      <td>-0.631782</td>\n",
              "      <td>-0.629741</td>\n",
              "      <td>-0.413725</td>\n",
              "      <td>-0.071227</td>\n",
              "      <td>-0.105036</td>\n",
              "      <td>-0.197549</td>\n",
              "      <td>-0.727478</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13921 rows × 58 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\Homo_sapiens.GRCh38.83.chosenTranscript.3pUTRs.fa') as fasta_file:  # Will close handle cleanly\n",
        "    UTR3_identifiers = []\n",
        "    UTR3_seqs = []\n",
        "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
        "        UTR3_identifiers.append(seq_record.id)\n",
        "        UTR3_seqs.append(seq_record.seq)\n",
        "\n",
        "with open(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\Homo_sapiens.GRCh38.83.chosenTranscript.5pUTRs.fa') as fasta_file:  # Will close handle cleanly\n",
        "    UTR5_identifiers = []\n",
        "    UTR5_seqs = []\n",
        "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
        "        UTR5_identifiers.append(seq_record.id)\n",
        "        UTR5_seqs.append(seq_record.seq)\n",
        "\n",
        "with open(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\Homo_sapiens.GRCh38.83.chosenTranscript.ORFs.fa') as fasta_file:  # Will close handle cleanly\n",
        "    ORF_identifiers = []\n",
        "    ORF_seqs = []\n",
        "    for seq_record in SeqIO.parse(fasta_file, 'fasta'):  # (generator)\n",
        "        ORF_identifiers.append(seq_record.id)\n",
        "        ORF_seqs.append(seq_record.seq)\n",
        "\n",
        "print(UTR3_seqs[0])\n",
        "print(UTR5_seqs[0])\n",
        "print(ORF_seqs[0])\n",
        "print(UTR3_identifiers[0])\n",
        "print(UTR5_identifiers[0])\n",
        "print(ORF_identifiers[0])\n",
        "print(len(UTR3_seqs))\n",
        "print(len(UTR5_seqs))\n",
        "print(len(ORF_seqs))"
      ],
      "metadata": {
        "id": "JFU1gCykz-M7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd2653a4-858e-4eac-ef1b-ecccf94b5f44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AATATTATGTATGCAGCAATATTTGAGTAACAAGAAGCAAATATCCAAGTTCCAAAATTATAAAAGAAATTCTTATCCAAATAGTAATGTTCTAATTGATCATATAAGAAAGCAAAGCATAGACATTAGAATTATAAGTCAGCAGTGGTCTGTTCAAGAACAATCAACATTTTTAGAAAATAGTAGGACAAAATTAGGAAATAATTATCACCAAGAGGATCTAGTTCATGACTTTCTATTATCTCAATTAGATTGCTCAATCATCAGCCTTCCTATACTAAACTCTGATTCAGGACCAAGAAAGGCATAGTCTGACTCTGGAAATGCGCTGTTGGAAGCCAAATAACATCAATACTCTTGTTCTATAATTGAATATCAAATAAGACAAATTACCATTAATTTAATGACTGTGGAGTTAATTGTATACCAGCATTTCAGCAAATCATCATCAATAGTATTACATTAGCAATTTATGCAATTAAAAGGGCTTTGTAAAACTTTGAATAGATTTTATTGTCATTAGTAGCTGTTGGAACTTCATTATTATATAATGTTTTTGCAAACTTTAACTTTTTTCTAAATTGTTAAATAAAAGAATAACTATCCTTAATCTAAATAATTTTGGTAGCAAATCCTATAAGGTATTAAACATTTTAAGGTATATTATTACATTGCTATTTTACTGTTTCTCATTAACCCAAACAGTTTAAAGGCAGAATTCCACTTAGAAACAAGTTGCATTTTGAAAGTTTATTTGTAATCCATTTGTTTGGAATTCAGAAATGTATTTCACATAAAAATAATCTTGGAAGTAATAAATTCCAAAATTAACTAACAAAA\n",
            "AGATGAGATTTCATCATGTTGGCCAGCCTGGTCTCAAACTCCTGACCTCAAGTGACCCGCCTGCCTCAGCCTCCCAAAGTGCTGGGATTACAGGAATTTAGTGATTGACA\n",
            "ATGGCAGAAAAAATCCTAGAGAAGTTGGATGTCCTTGATAAGCAAGCAGAGATAATCTTGGCCAGAAGAACAAAGATAAACAGGCTTCAGAGTGAAGGAAGAAAAACAACTATGGCTATACCCCTGACATTTGATTTTCAGTTGGAATTTGAAGAAGCTCTTGCTACATCCGCGTCTAAGGCAATATCAAAGATCAAAGAAGACAAGTCATGCAGCATTACAAAATCAAAAATGCATGTCTCTTTCAAATGTGAGCCTGAACCTAGAAAGAGTAATTTTGAAAAGTCAAATTTAAGACCATTCTTTATTCAAACAAATGTAAAAAATAAAGAAAGTGAGTCAACAGCTCAAATTGAAAAAAAACCTAGGAAACCATTGGATTCTGTTGGTCTCTTAGAAGGTGATAGAAATAAAAGAAAAAAATCTCCACAGATGAACGATTTTAATATAAAAGAAAACAAATCGGTCAGAAATTATCAATTAAGTAAGTATAGGTCAGTAAGAAAGAAAAGCTTGCTCCCGTTGTGCTTTGAGGATGAATTGAAAAATCCACATGCCAAGATAGTCAACGTTAGTCCAACAAAGACAGTAACTTCTCACATGGAACAAAAGGACACAAATCCCATAATTTTCCATGACACAGAATATGTACGAATGTTACTTTTGACAAAAAATAGATTTTCTTCTCATCCTTTGGAAAATGAAAACATTTACCCACATAAAAGAACAAATTTCATTTTAGAAAGAAATTGTGAAATCCTCAAATCTATAATTGGCAATCAATCTATTTCTCTTTTCAAACCCCAAAAAACTATGCCTACAGTACAGAGAAAAGATATACAGATCCCTATGTCTTTTAAAGCGGGCCACACAACTGTAGATGATAAACTAAAGAAGAAAACTAATAAGCAGACACTAGAAAACAGATCTTGGAATACACTCTATAATTTCTCACAGAATTTTTCTAGCCTAACAAAACAATTTGTGGGTTACCTTGATAAAGCTGTTATTCATGAAATGAGTGCCCAAACTGGAAAATTTGAAAGAATGTTTTCTGCAGGAAAACCAACGAGCATACCCACATCCAGTGCCTTACCTGTCAAATGTTACTCAAAGCCTTTTAAATATATATATGAACTAAATAATGTAACGCCACTGGATAATTTGTTAAACTTATCAAATGAAATTTTAAATGCCTCA\n",
            "ENSG00000203963\n",
            "ENSG00000203963\n",
            "ENSG00000203963\n",
            "19094\n",
            "18642\n",
            "20253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exons = pd.read_csv(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\Kelley_et_al_exon_junctions.txt', delimiter = \"\\t\")\n",
        "exons"
      ],
      "metadata": {
        "id": "HrhPuo0w0di6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "15387277-d45f-453b-f365-6f29e639690b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                GeneID  UTR5_len  \\\n",
              "0      ENSG00000000003       112   \n",
              "1      ENSG00000000457       222   \n",
              "2      ENSG00000000460       700   \n",
              "3      ENSG00000000938       289   \n",
              "4      ENSG00000000971       240   \n",
              "...                ...       ...   \n",
              "13225  ENSG00000278615        48   \n",
              "13226  ENSG00000278619       239   \n",
              "13227  ENSG00000278845       161   \n",
              "13228  ENSG00000280789       574   \n",
              "13229  ENSG00000281991       330   \n",
              "\n",
              "                         Exon_Junctions_In_Full_Sequence  \n",
              "0                                199,388,463,562,697,781  \n",
              "1      387,573,687,744,847,959,1037,1177,1362,1534,16...  \n",
              "2      766,871,1012,1178,1263,1402,1483,1548,1697,182...  \n",
              "3           515,618,717,821,971,1127,1307,1384,1538,1670  \n",
              "4      298,484,590,667,859,1030,1204,1399,1576,1759,1...  \n",
              "...                                                  ...  \n",
              "13225                                         87,212,310  \n",
              "13226                                  781,875,1008,1128  \n",
              "13227                        227,405,523,622,671,821,995  \n",
              "13228                                          1056,1139  \n",
              "13229                                                495  \n",
              "\n",
              "[13230 rows x 3 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GeneID</th>\n",
              "      <th>UTR5_len</th>\n",
              "      <th>Exon_Junctions_In_Full_Sequence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ENSG00000000003</td>\n",
              "      <td>112</td>\n",
              "      <td>199,388,463,562,697,781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENSG00000000457</td>\n",
              "      <td>222</td>\n",
              "      <td>387,573,687,744,847,959,1037,1177,1362,1534,16...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENSG00000000460</td>\n",
              "      <td>700</td>\n",
              "      <td>766,871,1012,1178,1263,1402,1483,1548,1697,182...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ENSG00000000938</td>\n",
              "      <td>289</td>\n",
              "      <td>515,618,717,821,971,1127,1307,1384,1538,1670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENSG00000000971</td>\n",
              "      <td>240</td>\n",
              "      <td>298,484,590,667,859,1030,1204,1399,1576,1759,1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13225</th>\n",
              "      <td>ENSG00000278615</td>\n",
              "      <td>48</td>\n",
              "      <td>87,212,310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13226</th>\n",
              "      <td>ENSG00000278619</td>\n",
              "      <td>239</td>\n",
              "      <td>781,875,1008,1128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13227</th>\n",
              "      <td>ENSG00000278845</td>\n",
              "      <td>161</td>\n",
              "      <td>227,405,523,622,671,821,995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13228</th>\n",
              "      <td>ENSG00000280789</td>\n",
              "      <td>574</td>\n",
              "      <td>1056,1139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13229</th>\n",
              "      <td>ENSG00000281991</td>\n",
              "      <td>330</td>\n",
              "      <td>495</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13230 rows × 3 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chromosomes = pd.read_csv(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\Kelley_et_al_chromosomes.txt', delimiter = \"\\t\")\n",
        "chromosomes"
      ],
      "metadata": {
        "id": "N_zoThwx0u0w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "de302c82-a460-4725-e2ae-7e86c49d95b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                GeneID  Chromosome\n",
              "0      ENSG00000186092           1\n",
              "1      ENSG00000279928           1\n",
              "2      ENSG00000279457           1\n",
              "3      ENSG00000278566           1\n",
              "4      ENSG00000273547           1\n",
              "...                ...         ...\n",
              "20290  ENSG00000277856  KI270726.1\n",
              "20291  ENSG00000275063  KI270726.1\n",
              "20292  ENSG00000271254  KI270711.1\n",
              "20293  ENSG00000277475  KI270713.1\n",
              "20294  ENSG00000268674  KI270713.1\n",
              "\n",
              "[20295 rows x 2 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GeneID</th>\n",
              "      <th>Chromosome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ENSG00000186092</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENSG00000279928</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENSG00000279457</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ENSG00000278566</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENSG00000273547</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20290</th>\n",
              "      <td>ENSG00000277856</td>\n",
              "      <td>KI270726.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20291</th>\n",
              "      <td>ENSG00000275063</td>\n",
              "      <td>KI270726.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20292</th>\n",
              "      <td>ENSG00000271254</td>\n",
              "      <td>KI270711.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20293</th>\n",
              "      <td>ENSG00000277475</td>\n",
              "      <td>KI270713.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20294</th>\n",
              "      <td>ENSG00000268674</td>\n",
              "      <td>KI270713.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20295 rows × 2 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chromosomes['Chromosome'].value_counts()"
      ],
      "metadata": {
        "id": "i7fzASja1JI6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d744aac0-6eaf-47c6-ce9b-bb347ef89920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1             2053\n",
              "19            1458\n",
              "11            1316\n",
              "2             1298\n",
              "17            1185\n",
              "3             1070\n",
              "6             1045\n",
              "12            1033\n",
              "7              980\n",
              "5              868\n",
              "16             865\n",
              "X              824\n",
              "14             824\n",
              "9              772\n",
              "4              747\n",
              "10             730\n",
              "8              670\n",
              "15             609\n",
              "20             541\n",
              "22             489\n",
              "13             320\n",
              "18             269\n",
              "21             233\n",
              "Y               54\n",
              "MT              13\n",
              "KI270728.1       6\n",
              "KI270727.1       4\n",
              "KI270734.1       3\n",
              "GL000194.1       2\n",
              "GL000195.1       2\n",
              "KI270726.1       2\n",
              "KI270713.1       2\n",
              "GL000009.2       1\n",
              "GL000205.2       1\n",
              "GL000219.1       1\n",
              "GL000213.1       1\n",
              "GL000218.1       1\n",
              "KI270731.1       1\n",
              "KI270721.1       1\n",
              "KI270711.1       1\n",
              "Name: Chromosome, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XRaMXVFv1YvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rbp_k = np.load(r'C:\\Users\\marku\\Desktop\\ML4RG_shared_with_students\\Saluki_Data\\RBP_k.npy')\n",
        "rbp_k.shape"
      ],
      "metadata": {
        "id": "pNwe4p0T4_u7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d116f55-b7b8-4344-bc8a-5da10d481c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13230, 59)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So not only do we have much more chromosomes, we the data comes from Pedro (df), Saluki (the sequences), from Pauline (chromosomes, exon junctions, via Saluki-chosen transcript), and from Yasmine (RBPs using Deepripe).\n",
        "Thus we should take good care that we merge the tables correctly."
      ],
      "metadata": {
        "id": "ITVKHQAf5c5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d = {'geneID': UTR5_identifiers, 'UTR5_seqs': UTR5_seqs}\n",
        "UTR5 = pd.DataFrame(data=d)\n",
        "d = {'geneID': ORF_identifiers, 'ORF_seqs': ORF_seqs}\n",
        "ORF = pd.DataFrame(data=d)\n",
        "d = {'geneID': UTR3_identifiers, 'UTR3_seqs': UTR3_seqs}\n",
        "UTR3 = pd.DataFrame(data=d)\n",
        "\n",
        "#merge every data frame to sequences\n",
        "halflife = hl[[\"Ensembl Gene Id\", \"zscore\"]] # half-life\n",
        "seqs = pd.merge(pd.merge(UTR5, ORF, on ='geneID'), UTR3, on = 'geneID') # Sequence\n",
        "sequences = pd.merge(halflife, seqs, right_on = 'geneID', left_on = 'Ensembl Gene Id') # half-life + Sequence\n",
        "sequences = sequences.drop(columns=[\"geneID\"])\n",
        "sequences = sequences.rename(columns={\"Ensembl Gene Id\": \"geneID\"})\n",
        "sequences = pd.merge(sequences, chromosomes, left_on='geneID', right_on='GeneID') # halflife + Sequence + chromosomes\n",
        "sequences = sequences.drop(columns=[\"GeneID\"])\n",
        "sequences = pd.merge(sequences, exons, left_on='geneID', right_on='GeneID')# halflife + Sequence + chromosomes + exons\n",
        "sequences = sequences.drop(columns=[\"GeneID\"])\n",
        "\n",
        "#transform seqs into strings:\n",
        "sequences[\"UTR5_seqs\"] = sequences[\"UTR5_seqs\"].apply(str)\n",
        "sequences[\"UTR3_seqs\"] = sequences[\"UTR3_seqs\"].apply(str)\n",
        "sequences[\"ORF_seqs\"] = sequences[\"ORF_seqs\"].apply(str)\n",
        "\n",
        "rubbish = [d, UTR3, ORF, UTR5, UTR5_seqs, UTR3_seqs, ORF_seqs, UTR3_identifiers, UTR5_identifiers, ORF_identifiers, hl, halflife]\n",
        "del rubbish\n",
        "\n",
        "sequences.head()"
      ],
      "metadata": {
        "id": "M7rLipk85dwm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "225cb54a-7648-4786-a96d-c378da5c6fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            geneID    zscore  \\\n",
              "0  ENSG00000000003  1.807620   \n",
              "1  ENSG00000000457 -1.446182   \n",
              "2  ENSG00000000460  0.092022   \n",
              "3  ENSG00000000938 -0.196955   \n",
              "4  ENSG00000000971  1.611324   \n",
              "\n",
              "                                           UTR5_seqs  \\\n",
              "0  AGTTGTGGACGCTCGTAAGTTTTCGGCAGTTTCCGGGGAGACTCGG...   \n",
              "1  TGTCCCGTTTCCGGACCCGTCTCTATGGTGTAGGAGAAACCCGGCC...   \n",
              "2  GGCTTTGGCCCTGGAAAGCCTCGCGGACGTGTTCTGACCCAAGGTT...   \n",
              "3  GGCTTGGGGCTAGGGCGTGACTGTCTCCCTGCCACCATCACCGCCC...   \n",
              "4  ACAGCATTAACATTTAGTGGGAGTGCAGTGAGAATTGGGTTTAACT...   \n",
              "\n",
              "                                            ORF_seqs  \\\n",
              "0  ATGGCGTCCCCGTCTCGGAGACTGCAGACTAAACCAGTCATTACTT...   \n",
              "1  ATGGGATCAGAGAACAGTGCTTTAAAGAGCTATACACTGAGAGAAC...   \n",
              "2  ATGTTTTTACCTCATATGAACCACCTGACATTGGAACAGACTTTCT...   \n",
              "3  ATGGGCTGTGTGTTCTGCAAGAAATTGGAGCCGGTGGCCACGGCCA...   \n",
              "4  ATGAGACTTCTAGCAAAGATTATTTGCCTTATGTTATGGGCTATTT...   \n",
              "\n",
              "                                           UTR3_seqs Chromosome  UTR5_len  \\\n",
              "0  CCCAATGTATCTGTGGGCCTATTCCTCTCTACCTTTAAGGACATTT...          X       112   \n",
              "1  CAATAGATGTGAGTTAAACTTTAGGAAAAAGGATTCCCTTTTTTTA...          1       222   \n",
              "2  AACTTATCACTAGGCAGAACTGGGTTTGATGCTTTGTCAACTGAAA...          1       700   \n",
              "3  CCTGTCCGGGCATCAACCCTCTCTGGCGGTGGCCACCAGTCCTTGC...          1       289   \n",
              "4  AATCAATCATAAAGTGCACACCTTTATTCAGAACTTTAGTATTAAA...          1       240   \n",
              "\n",
              "                     Exon_Junctions_In_Full_Sequence  \n",
              "0                            199,388,463,562,697,781  \n",
              "1  387,573,687,744,847,959,1037,1177,1362,1534,16...  \n",
              "2  766,871,1012,1178,1263,1402,1483,1548,1697,182...  \n",
              "3       515,618,717,821,971,1127,1307,1384,1538,1670  \n",
              "4  298,484,590,667,859,1030,1204,1399,1576,1759,1...  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>geneID</th>\n",
              "      <th>zscore</th>\n",
              "      <th>UTR5_seqs</th>\n",
              "      <th>ORF_seqs</th>\n",
              "      <th>UTR3_seqs</th>\n",
              "      <th>Chromosome</th>\n",
              "      <th>UTR5_len</th>\n",
              "      <th>Exon_Junctions_In_Full_Sequence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ENSG00000000003</td>\n",
              "      <td>1.807620</td>\n",
              "      <td>AGTTGTGGACGCTCGTAAGTTTTCGGCAGTTTCCGGGGAGACTCGG...</td>\n",
              "      <td>ATGGCGTCCCCGTCTCGGAGACTGCAGACTAAACCAGTCATTACTT...</td>\n",
              "      <td>CCCAATGTATCTGTGGGCCTATTCCTCTCTACCTTTAAGGACATTT...</td>\n",
              "      <td>X</td>\n",
              "      <td>112</td>\n",
              "      <td>199,388,463,562,697,781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENSG00000000457</td>\n",
              "      <td>-1.446182</td>\n",
              "      <td>TGTCCCGTTTCCGGACCCGTCTCTATGGTGTAGGAGAAACCCGGCC...</td>\n",
              "      <td>ATGGGATCAGAGAACAGTGCTTTAAAGAGCTATACACTGAGAGAAC...</td>\n",
              "      <td>CAATAGATGTGAGTTAAACTTTAGGAAAAAGGATTCCCTTTTTTTA...</td>\n",
              "      <td>1</td>\n",
              "      <td>222</td>\n",
              "      <td>387,573,687,744,847,959,1037,1177,1362,1534,16...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENSG00000000460</td>\n",
              "      <td>0.092022</td>\n",
              "      <td>GGCTTTGGCCCTGGAAAGCCTCGCGGACGTGTTCTGACCCAAGGTT...</td>\n",
              "      <td>ATGTTTTTACCTCATATGAACCACCTGACATTGGAACAGACTTTCT...</td>\n",
              "      <td>AACTTATCACTAGGCAGAACTGGGTTTGATGCTTTGTCAACTGAAA...</td>\n",
              "      <td>1</td>\n",
              "      <td>700</td>\n",
              "      <td>766,871,1012,1178,1263,1402,1483,1548,1697,182...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ENSG00000000938</td>\n",
              "      <td>-0.196955</td>\n",
              "      <td>GGCTTGGGGCTAGGGCGTGACTGTCTCCCTGCCACCATCACCGCCC...</td>\n",
              "      <td>ATGGGCTGTGTGTTCTGCAAGAAATTGGAGCCGGTGGCCACGGCCA...</td>\n",
              "      <td>CCTGTCCGGGCATCAACCCTCTCTGGCGGTGGCCACCAGTCCTTGC...</td>\n",
              "      <td>1</td>\n",
              "      <td>289</td>\n",
              "      <td>515,618,717,821,971,1127,1307,1384,1538,1670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENSG00000000971</td>\n",
              "      <td>1.611324</td>\n",
              "      <td>ACAGCATTAACATTTAGTGGGAGTGCAGTGAGAATTGGGTTTAACT...</td>\n",
              "      <td>ATGAGACTTCTAGCAAAGATTATTTGCCTTATGTTATGGGCTATTT...</td>\n",
              "      <td>AATCAATCATAAAGTGCACACCTTTATTCAGAACTTTAGTATTAAA...</td>\n",
              "      <td>1</td>\n",
              "      <td>240</td>\n",
              "      <td>298,484,590,667,859,1030,1204,1399,1576,1759,1...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 10000 #this is slightly longer than the 95% quantile, but lower than Saluki's implementation\n",
        "\n",
        "seqs = sequences['UTR5_seqs'] + sequences['ORF_seqs'] + sequences['UTR3_seqs']\n",
        "def pad_sequence(seqs, max_len, anchor='start', value='N'):\n",
        "  padded_seqs = [fixed_len(seq, max_len, anchor=anchor) for seq in seqs.astype(\"string\")]\n",
        "  return padded_seqs\n",
        "fixed_len_seqs = np.array(pad_sequence(seqs, max_len))\n",
        "print(fixed_len_seqs[0:4])\n",
        "del seqs\n",
        "print(fixed_len_seqs.shape)"
      ],
      "metadata": {
        "id": "y8oUTj9w-XQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cbafffb-d0a1-4afb-fa36-775b818c4377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AGTTGTGGACGCTCGTAAGTTTTCGGCAGTTTCCGGGGAGACTCGGGGACTCCGCGTCTCGCTCTCTGTGTTCCAATCGCCCGGTGCGGTGGTGCAGGGTCTCGGGCTAGTCATGGCGTCCCCGTCTCGGAGACTGCAGACTAAACCAGTCATTACTTGTTTCAAGAGCGTTCTGCTAATCTACACTTTTATTTTCTGGATCACTGGCGTTATCCTTCTTGCAGTTGGCATTTGGGGCAAGGTGAGCCTGGAGAATTACTTTTCTCTTTTAAATGAGAAGGCCACCAATGTCCCCTTCGTGCTCATTGCTACTGGTACCGTCATTATTCTTTTGGGCACCTTTGGTTGTTTTGCTACCTGCCGAGCTTCTGCATGGATGCTAAAACTGTATGCAATGTTTCTGACTCTCGTTTTTTTGGTCGAACTGGTCGCTGCCATCGTAGGATTTGTTTTCAGACATGAGATTAAGAACAGCTTTAAGAATAATTATGAGAAGGCTTTGAAGCAGTATAACTCTACAGGAGATTATAGAAGCCATGCAGTAGACAAGATCCAAAATACGTTGCATTGTTGTGGTGTCACCGATTATAGAGATTGGACAGATACTAATTATTACTCAGAAAAAGGATTTCCTAAGAGTTGCTGTAAACTTGAAGATTGTACTCCACAGAGAGATGCAGACAAAGTAAACAATGAAGGTTGTTTTATAAAGGTGATGACCATTATAGAGTCAGAAATGGGAGTCGTTGCAGGAATTTCCTTTGGAGTTGCTTGCTTCCAACTGATTGGAATCTTTCTCGCCTACTGCCTCTCTCGTGCCATAACAAATAACCAGTATGAGATAGTGCCCAATGTATCTGTGGGCCTATTCCTCTCTACCTTTAAGGACATTTAGGGTCCCCCCTGTGAATTAGAAAGTTGCTTGGCTGGAGAACTGACAACACTACTTACTGATAGACCAAAAAACTACACCAGTAGGTTGATTCAATCAAGATGTATGTAGACCTAAAACTACACCAATAGGCTGATTCAATCAAGATCCGTGCTCGCAGTGGGCTGATTCAATCAAGATGTATGTTTGCTATGTTCTAAGTCCACCTTCTATCCCATTCATGTTAGATCGTTGAAACCCTGTATCCCTCTGAAACACTGGAAGAGCTAGTAAATTGTAAATGAAGTAATACTGTGTTCCTCTTGACTGTTATTTTTCTTAGTAGGGGGCCTTTGGAAGGCACTGTGAATTTGCTATTTTGATGTAGTGTTACAAGATGGAAAATTGATTCCTCTGACTTTGCTATTGATGTAGTGTGATAGAAAATTCACCCCTCTGAACTGGCTCCTTCCCAGTCAAGGTTATCTGGTTTGATTGTATAATTTGCACCAAGAAGTTAAAATGTTTTATGACTCTCTGTTCTGCTGACAGGCAGAGAGTCACATTGTGTAATTTAATTTCAGTCAGTCAATAGATGGCATCCCTCATCAGGGTTGCCAGATGGTGATAACAGTGTAAGGCCTTGGGTCTAAGGCATCCACGACTGGAAGGGACTACTGATGTTCTGTGATACATCAGGTTTCAGCACACAACTTACATTTCTTTGCCTCCAAATTGAGGCATTTATTATGATGTTCATACTTTCCCTCTTGTTTGAAAGTTTCTAATTATTAAATGGTGTCGGAATTGTTGTATTTTCCTTAGGAATTCAGTGGAACTTATCTTCATTAAATTTAGCTGGTACCAGGTTGATATGACTTGTCAATATTATGGTCAACTTTAAGTCTTAGTTTTCGTTTGTGCCTTTGATTAATAAGTATAACTCTTATACAATAAATACTGCTTTCCTCTAAAAAGATCGTGTTTAAATTAACTTGTAGAAAATCTGCTGGAATGGTTGTTGTTTTCCACTGAGAAAGCTAAGCCCTACATTTCTATTCAGAGTACTGTTTTTAGATGTGAAATATAAGCCTGCGGCCTTAACTCTGTATTAAAAAAAATGTTTTTGTTTAAAAAAAACTGTTCCCATAGGTGCAGCAAACCACCATGGCACATGTATACCTATGTAACAAACCTGCACATTCTGCACATGTATCCCAGAACTTAATGTAAACAAAAAAATCTTAAAGTGCAAATATTAAAAAAAACTGTTCTCTGTGAAAAAAATTATATTCCATGTTATAAAGTAGCATATGACTAGTGTTCTCCTAGNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN'\n",
            " 'TGTCCCGTTTCCGGACCCGTCTCTATGGTGTAGGAGAAACCCGGCCCCCAGAAGATTGTGGGTGTAGTGGCCACAGCCTTACAGGCAGGCAGGGGTGGTTGGTGTCAACAGGGGGGCCAACAGGGTACCAGAGCCAAGACCCTCGGCCTCCTCCCCCGCCGCCTTCCTGCAGATCTGCTTGGCTTTGAGGAAGAGTGGCAGTACTGCCTCACTGCATAAGGGATGGGATCAGAGAACAGTGCTTTAAAGAGCTATACACTGAGAGAACCACCATTTACCTTACCCTCTGGACTTGCTGTTTATCCCGCTGTACTGCAAGATGGCAAATTTGCTTCAGTTTTTGTGTATAAGAGAGAAAATGAAGACAAGGTTAATAAAGCTGCCAAGCATTTGAAGACACTTCGTCACCCTTGCTTGCTAAGATTTTTATCTTGTACTGTGGAAGCGGATGGCATTCATCTTGTCACTGAGCGAGTACAGCCCCTGGAAGTGGCTTTGGAAACATTGTCTTCTGCAGAGGTCTGTGCTGGGATCTATGACATATTGCTGGCTCTTATCTTCCTTCATGACAGAGGACACCTAACACACAATAATGTCTGTTTATCATCTGTGTTTGTGAGTGAAGATGGACACTGGAAGCTAGGAGGAATGGAAACTGTTTGTAAAGTTTCTCAGGCCACACCAGAGTTTCTGAGGAGTATTCAGTCAATAAGAGACCCAGCATCTATCCCTCCTGAAGAGATGTCTCCAGAATTCACAACTCTCCCAGAGTGTCATGGACATGCCCGGGATGCCTTTTCATTTGGAACATTGGTGGAAAGTTTGCTCACAATCTTAAATGAACAGGTTTCAGCGGATGTTCTCTCCAGCTTTCAACAGACCTTGCACTCAACTTTGCTGAATCCCATTCCAAAATGTCGGCCAGCGCTCTGCACCTTACTATCTCATGACTTCTTCAGAAATGATTTTCTGGAAGTTGTGAATTTCTTGAAAAGTTTAACATTGAAGAGTGAAGAGGAGAAAACGGAATTCTTTAAATTTCTGCTGGACAGAGTCAGCTGCTTGTCAGAGGAATTGATAGCTTCAAGGTTGGTGCCTCTTCTGCTTAATCAGTTGGTGTTTGCAGAGCCAGTGGCTGTTAAGAGTTTTCTTCCTTATCTGCTTGGCCCCAAAAAAGATCATGCGCAGGGAGAAACTCCTTGCTTGCTCTCACCAGCCCTGTTCCAGTCACGGGTGATCCCCGTGCTTCTCCAGTTGTTTGAAGTTCATGAAGAGCATGTGCGGATGGTGCTGCTGTCTCACATCGAGGCCTACGTGGAGCACTTCACTCAGGAGCAGCTGAAGAAAGTCATCTTGCCACAGGTTTTGCTGGGCCTGCGTGATACTAGCGATTCCATTGTGGCAATTACTCTGCATAGCCTAGCAGTGCTGGTCTCTCTGCTTGGACCAGAGGTGGTTGTGGGAGGAGAACGAACCAAGATCTTCAAACGCACTGCCCCAAGTTTTACTAAAAATACTGACCTTTCTCTAGAAGATTCTCCTATGTGTGTCGTCTGCAGCCATCACAGTCAGATCTCGCCAATCTTGGAGAACCCCTTCTCTAGCATATTCCCTAAATGTTTCTTTTCTGGCAGCACGCCCATCAACAGCAAGAAGCACATACAGCGAGATTACTACAATACTCTTTTACAGACAGGCGATCCATTTTCTCAGCCTATTAAATTTCCCATAAATGGACTCTCAGATGTAAAAAATACTTCGGAGGACAGTGAAAACTTCCCATCAAGTTCTAAAAAGTCTGAGGAGTGGCCTGACTGGAGTGAACCTGAGGAGCCTGAAAATCAAACTGTCAACATACAGATTTGGCCTAGAGAACCTTGTGATGATGTCAAGTCCCAGTGCACTACCTTGGATGTGGAAGAGTCATCTTGGGATGACTGCGAGCCCAGCAGCTTAGATACTAAAGTAAACCCAGGAGGTGGAATCACTGCTACAAAACCTGTTACCTCAGGGGAGCAGAAGCCTATTCCTGCTTTGCTTTCACTCACTGAAGAGTCTATGCCTTGGAAATCAAGCTTACCCCAAAAGATTAGCCTTGTACAAAGGGGGGATGACGCAGACCAAATCGAGCCGCCAAAAGTGTCATCACAAGAAAGGCCCCTTAAGGTTCCATCAGAACTTGGTTTAGGAGAGGAATTCACCATTCAAGTAAAAAAGAAGCCAGTAAAAGATCCTGAGATGGATTGGTTTGCTGATATGATCCCAGAAATTAAGCCTTCTGCTGCTTTTCTTATATTACCTGAACTGAGGACAGAAATGGTCCCAAAAAAGGATGATGTCTCCCCAGTGATGCAGTTTTCCTCAAAATTTGCTGCAGCAGAAATTACTGAGGGAGAGGCTGAAGGCTGGGAAGAAGAAGGGGAGCTGAACTGGGAAGATAATAACTGGCAATAGATGTGAGTTAAACTTTAGGAAAAAGGATTCCCTTTTTTTAAAAAAAATCAATACCTCAAAAGCAGGCTTTGGGACAAGAAAACCCCAAAGTGGCCTGCTTTTCCCATCCCAGGAGCTCATTATCCAGTCTGTGCCAACTGAAGTAGGAGACTGACTGTGAGTGCTGGCTAAAAGCCCTGGGTGGTGAGGCTCACAGTACTGGTTTCCAGGAGGAAGAGCCTTTGTGCATTTGACTGAGGCCAGTTTCTATGAAGAGCAAGTAGCTGAGGAGAGGTCGAATTTACTGCTTTTTCCAGGACAATTCTGGAAGTAAAGAAAATGTAATTCAAGCTGGTTAGCTTAATTTTGTGCCATTCTTTAACATAAGAGTAAGCTCTATTATGAAATACAACTTTAAAAAATTTTAGCTATAAATTATATAAATGATTTTAAATTGCTGAGGTTTCCTTAGGCAGCTTATTTATTTGTTTACAGTTAGACTATCTGAGTAAATGGTTCTTTGTGGACCTAGGCAGTTCCTGACTGTTCCACATGTAGTACATTGTACCAAAGTTCTTAATAAGAATATTCCCCACAATCCTGTTCTCTAAATGTCAAATAAAGATTATTTTCACTAGATTCAACTTTACAAAANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN'\n",
            " 'GGCTTTGGCCCTGGAAAGCCTCGCGGACGTGTTCTGACCCAAGGTTTTAGCAGTGGATGTGGCGTTTTCTTCCATTCCTTCTTTCAGTTTTTCTGTACTCGTTGCTTGCAATTAAGTGTAAATACTTTTGCTAGTGGATAATGGGGGAGGCAAGGACTGAGACCTGCGGTATGACGATAGCTCTGGCTCTTAATAGTTTGAGGTAAAGCGAGATACTCTGAGCTTTTGTCTCCCGTAAAAAGGGTGGTGAATATGAATAAGGGCTTTCTTAGCGTTATAAGAATTAAAGGGCATAGTTCTGTGGTGTGAAATCTTTAAAAGATGTTCAGTAAATAAAAATGATTTTCCTCCTTCCCCTCTCAGACCTCTTTTTCTTCTTTCTTTCTTTTTTTTTGACAAGTTCTCACTCCTCTCACCCAGGCTGGAGTCTTTCTGAAAGAGTTCTTCCGCTTGTTGTTGGCTTTCAACTGTTGGATTTGAGGCGCTTAGCGCCTTCTTCGTCCGGGTGCAGCACATTCTTGATTGGTCTCATGCCTTTGTGGTTGTAAATGTGCCTGGAATCCTAGCCTTTCATGGTTTGTTCTGAGTAATGAATACCCTATTACTATGATACTAGTATCTTCCTTAATTATCCTACTCATTGTCTCAACATTCTGACAGTTGGATTGAGCATATTCAAATTTTGAAAATTATTGTAGAAATGTTTTTACCTCATATGAACCACCTGACATTGGAACAGACTTTCTTTTCACAAGTGTTACCAAAGACTGTGAAATTATTCGATGACATGATGTATGAATTAACCAGTCAAGCCAGAGGACTGTCAAGCCAAAATTTGGAAATCCAGACCACTCTAAGGAATATTTTACAAACAATGGTGCAGCTCTTAGGAGCTCTCACAGGATGTGTTCAGCATATCTGTGCCACACAGGAATCCATCATTTTGGAAAATATTCAGAGTCTCCCCTCCTCAGTCCTTCATATAATTAAAAGCACATTTGTGCATTGTAAGAATAGTGAATCTGTGTATTCTGGGTGTTTACACCTAGTTTCAGACCTTCTCCAGGCTCTTTTCAAGGAGGCCTATTCTCTTCAAAAGCAGTTAATGGAACTGCTGGACATGGTTTGCATGGACCCTTTAGTAGATGACAATGATGATATTTTGAATATGGTAATAGTTATTCATTCTTTATTGGATATCTGCTCTGTTATTTCCAGTATGGACCATGCATTTCATGCCAATACTTGGAAGTTTATAATTAAGCAGAGCCTTAAGCACCAGTCCATAATAAAAAGCCAGTTGAAACACAAAGATATAATTACTAGCTTGTGTGAAGACATTCTTTTCTCCTTCCATTCTTGTTTACAGTTAGCTGAGCAGATGACACAGTCAGATGCACAGGATAATGCTGACTACAGATTATTTCAGAAAACACTCAAATTGTGTCGTTTTTTTGCCAACTCCCTTTTGCACTACGCTAAGGAATTTCTTCCTTTCCTCTCTGATTCTTGCTGTACTTTGCACCAACTGTATCTTCAGATACACAGCAAGTTTCCTCCAAGCCTTTATGCTACCAGGATTTCTAAAGCACACCAAGAGGAAATAGCAGGTGCTTTCCTAGTGACACTGGATCCACTTATCAGTCAGCTGCTCACATTTCAGCCTTTCATGCAGGTGGTTTTGGACAGTAAATTAGACCTGCCATGTGAACTGCAGTTTCCACAATGTCTTCTTCTGGTTGTTGTCATGGATAAGCTGCCATCTCAGCCTAAGGAAGTGCAAACCCTGTGGTGCACAGACAGCCAGGTCTCAGAAACGACAACCAGGATATCTCTACTCAAAGCCGTTTTCTACAGTTTTGAGCAGTGTTCTGGTGAACTCTCTCTACCTGTTCATTTACAGGGATTAAAGAGTAAGGGGAAAGCTGAGGTGGCTGTCACCTTGTATCAGCATGTTTGTGTTCATCTGTGTACATTTATTACTTCCTTTCATCCCTCACTGTTTGCTGAACTGGATGCTGCTCTGCTGAATGCTGTACTTAGTGCTAATATGATCACCTCTTTGTTAGCTATGGATGCATGGTGCTTCCTTGCTCGATATGGGACTGCTGAACTGTGTGCACACCATGTCACCATAGTGGCTCATCTGATAAAGTCATGCCCTGGAGAATGTTATCAACTCATCAACCTATCAATACTGTTGAAGCGTCTCTTTTTCTTCATGGCACCACCCCATCAGCTGGAGTTTATCCAGAAATTTTCCCCAAAAGAAGCAGAAAATCTGCCTCTGTGGCAACATATTTCCTTCCAGGCGTTACCTCCTGAGCTTAGGGAACAAACTGTCCATGAGGTCACCACAGTAGGCACTGCAGAATGCAGGAAATGGCTGAGCAGGAGTCGTACTTTGGGAGAACTAGAATCTCTGAACACAGTACTGTCTGCTTTGCTTGCAGTATGTAATTCTGCTGGTGAAGCTTTGGATACAGGAAAACAAACTGCAATTATCGAAGTTGTGAGTCAGCTTTGGGCTTTTTTAAACATTAAACAGGTAGCAGATCAACCTTATGTTCAACAGACATTCAGCCTTTTACTTCCACTGTTGGGATTTTTCATTCAAACTCTAGATCCTAAACTGATACTTCAGGCAGTAACTTTGCAGACCTCGCTACTTAAATTAGAGCTTCCTGACTATGTTCGTTTGGCAATGTTGGATTTTGTATCTTCTTTAGGAAAACTTTTTATACCTGAAGCTATCCAGGACAGAATTCTGCCCAACCTGTCCTGTATGTTTGCCTTACTGCTAGCTGACAGGAGTTGGCTGCTAGAACAACATACCTTGGAGGCGTTTACTCAGTTCGCTGAGGGAACAAATCATGAAGAGATAGTTCCACAGTGTCTCAGTTCTGAAGAAACTAAGAACAAAGTTGTATCCTTTCTGGAGAAGACTGGGTTTGTAGATGAAACTGAAGCTGCCAAAGTGGAACGTGTGAAACAGGAAAAAGGTATTTTCTGGGAACCCTTTGCTAATGTGACTGTAGAAGAAGCAAAGAGGTCATCTTTACAGCCTTATGCAAAAAGAGCTCGTCAGGAGTTCCCCTGGGAAGAAGAGTACAGGTCAGCGCTGCATACAATAGCAGGGGCTTTGGAAGCAACTGAGTCACTACTCCAAAAGGGTCCTGCTCCAGCCTGGCTTTCAATGGAAATGGAGGCGCTCCAAGAAAGGATGGATAAGCTAAAACGTTACATACATACTCTAGGGAACTTATCACTAGGCAGAACTGGGTTTGATGCTTTGTCAACTGAAAATACTTATGTCTGTACATTTTCTAACAGATATAAAACAAATTTTGTAAAGTTGAATCTAGTGAAAATAATCTTTATTTGACATTTAGAGAACAGGATTGTGGGGAATATTCTTATTAAGAACTTTGGTACAATGTACTACATGTGGAACAGTCAGGAACTGCCTAGGTCCACAAAGAACCATTTACTCAGATAGTCTAACTGTAAACAAATAAATAAGCTGCCTAAGGAAACCTCAGCAATTTAAAATCATTTATATAATTTATAGCTAAAATTTTTTAAAGTTGTATTTCATAATAGAGCTTACTCTTATGTTAAAGAATGGCACAAAATTAAGCTAACCAGCTTGAATTACATTTTCTTTACTTCCAGAATTGTCCTGGAAAAAGCAGTAAATTCGACCTCTCCTCAGCTACTTGCTCTTCATAGAAACTGGCCTCAGTCAAATGCACAAAGGCTCTTCCTCCTGGAAACCAGTACTGTGAGCCTCACCACCCAGGGCTTTTAGCCAGCACTCACAGTCAGTCTCCTACTTCAGTTGGCACAGACTGGATAATGAGCTCCTGGGATGGGAAAAGCAGGCCACTTTGGGGTTTTCTTGTCCCAAAGCCTGCTTTTGAGGTATTGATTTTTTTTAAAAAAAGGGAATCCTTTTTCCTAAAGTTTAACTCACATCTATTGTCACCAGTTATTATCTTCCCAGTTCAGCTCCCCTTCTTCTTCCCAGCCTTCAGCCTCTCCCTGCAACAAAATAAAGCACACCAAGAACCCACTGAAACAAATCATATGCAAAAATCATACGCAAATTTGAAAAAGCAGGAATTTAAAATTTATCTTTTGATGCCAGAAACACTACCTCGTACTAAGTAAAATAACTTAGAGCTCTAACAGAAAGTTGAAAAGTAGGATTACAAAACCATGGCACTGGAAAATAGCTTTTCAAAAACCATAAAATCCAGTAAAAATCAGTGTGGATGACACCAAATCCTTATTTTAATCCCTTTTTTTTCTTTTTCAAATAAAAAGGTTACAATAGCTCATTAAACAAANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN'\n",
            " 'GGCTTGGGGCTAGGGCGTGACTGTCTCCCTGCCACCATCACCGCCCGCCGGCCGTGACTGCAATAAGAGAAGTCCGAGGCGGCTTCCTCCTCCCTGCCCAGCAGGGGCGGCGGTCAGAGGCGGGCAGCACCCCAGTTCTCCCCGCACGCCGGCACTCGCGGCTGCTGGAGCCCCGGCTGGCTCACCCCGGGGCCGGGCAGAATTGGGCTCCAGGTCTCTGACCCCTCCCAAGGATCATGCCGCAGCCCCACTGACCCAGGAGTAGGGGCCTAAGGGCAGGGAACCTGGAATGGGCTGTGTGTTCTGCAAGAAATTGGAGCCGGTGGCCACGGCCAAGGAGGATGCTGGCCTGGAAGGGGACTTCAGAAGCTACGGGGCAGCAGACCACTATGGGCCTGACCCCACTAAGGCCCGGCCTGCATCCTCATTTGCCCACATCCCCAACTACAGCAACTTCTCCTCTCAGGCCATCAACCCTGGCTTCCTTGATAGTGGCACCATCAGGGGTGTGTCAGGGATTGGGGTGACCCTGTTCATTGCCCTGTATGACTATGAGGCTCGAACTGAGGATGACCTCACCTTCACCAAGGGCGAGAAGTTCCACATCCTGAACAATACTGAAGGTGACTGGTGGGAGGCTCGGTCTCTCAGCTCCGGAAAAACTGGCTGCATTCCCAGCAACTACGTGGCCCCTGTTGACTCAATCCAAGCTGAAGAGTGGTACTTTGGAAAGATTGGGAGAAAGGATGCAGAGAGGCAGCTGCTTTCACCAGGCAACCCCCAGGGGGCCTTTCTCATTCGGGAAAGCGAGACCACCAAAGGTGCCTACTCCCTGTCCATCCGGGACTGGGATCAGACCAGAGGCGATCATGTGAAGCATTACAAGATCCGCAAACTGGACATGGGCGGCTACTACATCACCACACGGGTTCAGTTCAACTCGGTGCAGGAGCTGGTGCAGCACTACATGGAGGTGAATGACGGGCTGTGCAACCTGCTCATCGCGCCCTGCACCATCATGAAGCCGCAGACGCTGGGCCTGGCCAAGGACGCCTGGGAGATCAGCCGCAGCTCCATCACGCTGGAGCGCCGGCTGGGCACCGGCTGCTTCGGGGATGTGTGGCTGGGCACGTGGAACGGCAGCACTAAGGTGGCGGTGAAGACGCTGAAGCCGGGCACCATGTCCCCGAAGGCCTTCCTGGAGGAGGCGCAGGTCATGAAGCTGCTGCGGCACGACAAGCTGGTGCAGCTGTACGCCGTGGTGTCGGAGGAGCCCATCTACATCGTGACCGAGTTCATGTGTCACGGCAGCTTGCTGGATTTTCTCAAGAACCCAGAGGGCCAGGATTTGAGGCTGCCCCAATTGGTGGACATGGCAGCCCAGGTAGCTGAGGGCATGGCCTACATGGAACGCATGAACTACATTCACCGCGACCTGAGGGCAGCCAACATCCTGGTTGGGGAGCGGCTGGCGTGCAAGATCGCAGACTTTGGCTTGGCGCGTCTCATCAAGGACGATGAGTACAACCCCTGCCAAGGTTCCAAGTTCCCCATCAAGTGGACAGCCCCAGAAGCTGCCCTCTTTGGCAGATTCACCATCAAGTCAGACGTGTGGTCCTTTGGGATCCTGCTCACTGAGCTCATCACCAAGGGCCGAATCCCCTACCCAGGCATGAATAAACGGGAAGTGTTGGAACAGGTGGAGCAGGGCTACCACATGCCGTGCCCTCCAGGCTGCCCAGCATCCCTGTACGAGGCCATGGAACAGACCTGGCGTCTGGACCCGGAGGAGAGGCCTACCTTCGAGTACCTGCAGTCCTTCCTGGAGGACTACTTCACCTCCGCTGAACCACAGTACCAGCCCGGGGATCAGACACCTGTCCGGGCATCAACCCTCTCTGGCGGTGGCCACCAGTCCTTGCCAATCCCCAGAGCTGTTCTTCCAAAGCCCCCAGGCTGGCTTAGAACCCCATAGAGTCCTAGCATCACCGAGGACGTGGCTGCTCTGACACCACCTAGGGCAACCTACTTGTTTTACAGATGGGGCAAAAGGAGGCCCAGAGCTGATCTCTCATCCGCTCTGGCCCCAAGCACTATTTCTTCCTTTTCCACTTAGGCCCCTACATGCCTGTAGCCTTTCTCACTCCATCCCCACCCAAAGTGCTCAGACCTTGTCTAGTTATTTATAAAACTGTATGTACCTCCCTCACTTCTCTCCTATCACTGCTTTCCTACTCTCCTTTTATCTCACTCTAGTCCAGGTGCCAAGAATTTCCCTTCTACCCTCTATTCTCTTGTGTCTGTAAGTTACAAAGTCAGGAAAAGTCTTGGCTGGACCCCTTTCCTGCTGGGTGGATGCAGTGGTCCAGGACTGGGGTCTGGGCCCAGGTTTGAGGGAGAAGGTTGCAGAGCACTTCCCACCTCTCTGAATAGTGTGTATGTGTTGGTTTATTGATTCTGTAAATAAGTAAAATGACAATATGAATCCTCAAACCATGAAATACCCTTGAACCTTCCTTTGGGAGCGGGGGTGGTCAATAGGGGGTGAACGGACAGATATGGCTACAGGCAGCAGCAGGGGAAGCTGGAGAGGGCCCTAATGCCTACCAAGCACGGGGCATCCAAGGTGTGGAGTTTTAGAACACCCAGAGTCCCACTGCTCATCTGCACGTGAGTTTAGAAGACAAGCAGCTGAAGATACATTAAAATGTCCCCTTCGTTGCTGANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN']\n",
            "(13230,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot for track 1-4: the nucleotides \n",
        "\n",
        "one_hot_seqs = np.array([one_hot(seq, neutral_value=0) for seq in fixed_len_seqs])\n",
        "print(one_hot_seqs[0:2])\n",
        "print(one_hot_seqs.shape)\n",
        "rubbish = [fixed_len_seqs] \n",
        "del rubbish"
      ],
      "metadata": {
        "id": "PdyuPKVL_0QT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6e6f3d7-5d37-458f-b8ad-f24444e1dd24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[1. 0. 0. 0.]\n",
            "  [0. 0. 1. 0.]\n",
            "  [0. 0. 0. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 1.]\n",
            "  [0. 0. 1. 0.]\n",
            "  [0. 0. 0. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0.]]]\n",
            "(13230, 10000, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot for track 5: the exon binding sites\n",
        "# dunno why, I guess I still suck at python, but this took me over an hour to code and bugfix\n",
        "# lol this is future me from the next day, this was wrong and I had redo all the training\n",
        "\n",
        "exons = []\n",
        "\n",
        "for i in range(len(sequences)):\n",
        "  onehot = np.repeat(0, repeats = max_len)\n",
        "  if(isinstance(sequences[\"Exon_Junctions_In_Full_Sequence\"][i], str)):\n",
        "    current_exons = list(map(int, sequences[\"Exon_Junctions_In_Full_Sequence\"][i].split(\",\")))\n",
        "    assert len(current_exons) > 0\n",
        "    positions_capped = [x for x in current_exons if x <= 10000] # delete all exon junctions after 10000 since we're capping the sequence there\n",
        "    onehot[positions_capped] = 1\n",
        "    '''\n",
        "    for j in current_exons:\n",
        "      positions = [x+len(sequences['UTR5_seqs'][i]) for x in current_exons] # have to add UTR5 length to indices\n",
        "      positions_capped = [x for x in positions if x <= 10000] # delete all exon junctions after 10000 since we're capping the sequence there\n",
        "      onehot[positions_capped] = 1 #exon junctions are 1 now\n",
        "      \n",
        "  if(isinstance(sequences[\"Exon_Junctions_In_Full_Sequence\"][i], float)):\n",
        "    if(not(math.isnan(sequences[\"Exon_Junctions_In_Full_Sequence\"][i]))):\n",
        "      onehot[int(sequences[\"Exon_Junctions_In_Full_Sequence\"][i])+len(sequences['UTR5_seqs'][i])] = 1\n",
        "    '''\n",
        "  exons.append(onehot)"
      ],
      "metadata": {
        "id": "27vK8R2SAD9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(map(int, sequences[\"Exon_Junctions_In_Full_Sequence\"][5].split(\",\"))))"
      ],
      "metadata": {
        "id": "VDsHa06rCEQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9a9a5c7-8bbf-47a2-8ab7-c28b9b813b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[280, 468, 808, 1019, 1210, 1319]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#one hot for track 6: Marking the beginning of each codon with 1\n",
        "starts = []\n",
        "for i in range(len(sequences)):\n",
        "  #assert len(sequences['ORF_seqs'].astype(\"string\")[i]) % 3 == 0 \n",
        "  lst = list(range(len(sequences['ORF_seqs'].astype(\"string\")[i])))\n",
        "  onehot = np.repeat(0, repeats = len(sequences['ORF_seqs'].astype(\"string\")[i]))\n",
        "  onehot[lst[0::3]] = 1\n",
        "  full = np.concatenate((np.repeat([0], repeats = len(sequences['UTR5_seqs'].astype(\"string\")[i])),\n",
        "                         onehot,\n",
        "                         np.repeat([0], repeats = len(sequences['UTR3_seqs'].astype(\"string\")[i]))), axis=None)\n",
        "  if (len(full) > max_len):\n",
        "    full = full[:max_len]\n",
        "  elif (len(full) < max_len):\n",
        "    full = np.concatenate((full, np.repeat(0, repeats = max_len - len(full))),axis = None)\n",
        "  starts.append(full)"
      ],
      "metadata": {
        "id": "pRuNTsz4AHLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rubbish = [fixed_len_seqs]\n",
        "\n",
        "rubbish = [fixed_len_seqs, d, UTR3, ORF, UTR5, UTR5_seqs, UTR3_seqs, ORF_seqs, hl,\n",
        "           UTR3_identifiers, UTR5_identifiers, ORF_identifiers, halflife, max_len]\n",
        "del rubbish"
      ],
      "metadata": {
        "id": "lt-QVEBiP5Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This takes about 18 GB, so beware of that\n",
        "onehot = np.concatenate((one_hot_seqs,np.array(exons)[:, :, None], np.array(starts)[:, :, None]), axis = 2)\n",
        "print(onehot.shape)"
      ],
      "metadata": {
        "id": "6ZpZoaGuPp3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fff877b9-5116-4798-8ae5-11d2f7a58dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13230, 10000, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del exons\n",
        "del starts"
      ],
      "metadata": {
        "id": "8qpC7Tr5RZae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for The Train-Val-Test split we split as recommended on Chromosomes:"
      ],
      "metadata": {
        "id": "oM3U9VTxSy0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chrom_val = ['2', '3', '4']\n",
        "chrom_test = ['1', '8', '9']"
      ],
      "metadata": {
        "id": "Ih4OOJxISyJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_test = np.where(sequences.Chromosome.isin(chrom_test))[0]\n",
        "idx_val = np.where(sequences.Chromosome.isin(chrom_val))[0]\n",
        "idx_train = np.where(~(sequences.Chromosome.isin(chrom_test)| sequences.Chromosome.isin(chrom_val)))[0]"
      ],
      "metadata": {
        "id": "FQcQCI4BTFSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(array, idx_train, idx_val, idx_test):\n",
        "  return array[idx_train], array[idx_val], array[idx_test]"
      ],
      "metadata": {
        "id": "k7GWuUtuTJxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(idx_test))\n",
        "print(len(idx_val))\n",
        "print(len(idx_train))"
      ],
      "metadata": {
        "id": "aOrm43s0Z8M2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a21a9766-fac2-4567-d157-9c9043e2954f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2334\n",
            "2118\n",
            "8778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, X_test = train_test_split(onehot, idx_train, idx_val, idx_test)\n",
        "y_train, y_val, y_test = train_test_split(sequences['zscore'].values, idx_train, idx_val, idx_test)"
      ],
      "metadata": {
        "id": "mmpWExHRTfMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExplainedVariance(keras.callbacks.Callback):\n",
        "    def __init__(self, validation_data=(), interval=10):\n",
        "        super(keras.callbacks.Callback, self).__init__()\n",
        "\n",
        "        self.interval = interval\n",
        "        self.X_val, self.y_val = validation_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.interval == 0:\n",
        "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "            var_score = explained_variance_score(self.y_val, y_pred)\n",
        "            #r2 = r2_score(self.y_val, y_pred)\n",
        "            del y_pred\n",
        "            #print(\" - interval evaluation - epoch: {:d} - explained variance: {:.6f} - R2: {:.6f}\".format(epoch, var_score, r2))\n",
        "            print(\"interval evaluation - epoch: {:d} - explained variance: {:.6f}\".format(epoch, var_score))\n",
        "        gc.collect()"
      ],
      "metadata": {
        "id": "xf5ZZA63U-qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saluki-Type model\n",
        "input = kl.Input((X_train.shape[1:]))\n",
        "\n",
        "x = kl.Conv1D(64, kernel_size=5, activation=None, kernel_regularizer=tf.keras.regularizers.l2(l=0.0015))(input)\n",
        "x = kl.LayerNormalization(axis = [1,2])(x)\n",
        "x = kl.Activation(\"relu\")(x)\n",
        "\n",
        "for i in range(6):\n",
        "  x1 = kl.Conv1D(32, kernel_size=5, padding=\"same\", activation=None, kernel_regularizer=tf.keras.regularizers.l2(l=0.0015))(x)\n",
        "  x1 = kl.MaxPooling1D(pool_size=2)(x1)\n",
        "  x1 = kl.LayerNormalization(axis = [1,2])(x1)\n",
        "  x1 = kl.Activation(\"relu\")(x1)\n",
        "  x1 = kl.Dropout(0.33)(x1)\n",
        "\n",
        "  x2 = kl.Conv1D(32, kernel_size=3, padding = \"same\", activation=None, kernel_regularizer=tf.keras.regularizers.l2(l=0.0015))(x)\n",
        "  x2 = kl.MaxPooling1D(pool_size=2)(x2)\n",
        "  x2 = kl.LayerNormalization(axis = [1,2])(x2)\n",
        "  x2 = kl.Activation(\"relu\")(x2)\n",
        "  x2 = kl.Dropout(0.33)(x2)\n",
        "\n",
        "  x3 = kl.MaxPooling1D(pool_size=2)(x)\n",
        "  x3 = kl.Conv1D(16, kernel_size=1, padding='same', activation=None, kernel_regularizer=tf.keras.regularizers.l2(l=0.0015))(x3)\n",
        "  x3 = kl.LayerNormalization(axis = [1,2])(x3)\n",
        "  x3 = kl.Activation(\"relu\")(x3)\n",
        "  x3 = kl.Dropout(0.33)(x3)\n",
        "\n",
        "  x = kl.concatenate([x1, x2, x3], axis = 2)\n",
        "\n",
        "x = kl.GRU(80, go_backwards=True, kernel_regularizer=tf.keras.regularizers.l2(l=0.001))(x)\n",
        "x = kl.Dropout(0.33)(x)\n",
        "x = kl.BatchNormalization()(x)\n",
        "x = kl.Activation(\"relu\")(x)\n",
        "\n",
        "x = kl.Dense(96, kernel_regularizer=tf.keras.regularizers.l2(l=0.001))(x) #backwards so it encounters padding first\n",
        "#x = kl.Dropout(0.3)(x)\n",
        "#x = kl.BatchNormalization()(x) #batch is fine here, no padding to consider anymore\n",
        "x = kl.Activation(\"relu\")(x)\n",
        "\n",
        "output = kl.Dense(units=1)(x)\n",
        "\n",
        "my_saluki = Model(inputs=input, outputs=output)\n",
        "my_saluki.summary()\n",
        "\n",
        "#Saluki: We chose layer normalization over batch normalization because most of the 3′ positions are zero padded and would confuse the batch statistics.\n"
      ],
      "metadata": {
        "id": "X0vIgaHIVpyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f82ead7-3c70-43ce-9c8a-25bd8b76df11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 10000, 6)]   0           []                               \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 9996, 64)     1984        ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 9996, 64)    1279488     ['conv1d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 9996, 64)     0           ['layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 9996, 32)     10272       ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 9996, 32)     6176        ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 4998, 64)    0           ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 4998, 32)     0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 4998, 32)    0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 4998, 16)     1040        ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 4998, 32)    319872      ['max_pooling1d[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 4998, 32)    319872      ['max_pooling1d_1[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 4998, 16)    159936      ['conv1d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 4998, 32)     0           ['layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 4998, 32)     0           ['layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 4998, 16)     0           ['layer_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 4998, 32)     0           ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 4998, 32)     0           ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 4998, 16)     0           ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 4998, 80)     0           ['dropout[0][0]',                \n",
            "                                                                  'dropout_1[0][0]',              \n",
            "                                                                  'dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 4998, 32)     12832       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 4998, 32)     7712        ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling1d_5 (MaxPooling1D)  (None, 2499, 80)    0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling1d_3 (MaxPooling1D)  (None, 2499, 32)    0           ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_4 (MaxPooling1D)  (None, 2499, 32)    0           ['conv1d_5[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 2499, 16)     1296        ['max_pooling1d_5[0][0]']        \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 2499, 32)    159936      ['max_pooling1d_3[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " layer_normalization_5 (LayerNo  (None, 2499, 32)    159936      ['max_pooling1d_4[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " layer_normalization_6 (LayerNo  (None, 2499, 16)    79968       ['conv1d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 2499, 32)     0           ['layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 2499, 32)     0           ['layer_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 2499, 16)     0           ['layer_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 2499, 32)     0           ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 2499, 32)     0           ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 2499, 16)     0           ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 2499, 80)     0           ['dropout_3[0][0]',              \n",
            "                                                                  'dropout_4[0][0]',              \n",
            "                                                                  'dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 2499, 32)     12832       ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 2499, 32)     7712        ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_8 (MaxPooling1D)  (None, 1249, 80)    0           ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_6 (MaxPooling1D)  (None, 1249, 32)    0           ['conv1d_7[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_7 (MaxPooling1D)  (None, 1249, 32)    0           ['conv1d_8[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 1249, 16)     1296        ['max_pooling1d_8[0][0]']        \n",
            "                                                                                                  \n",
            " layer_normalization_7 (LayerNo  (None, 1249, 32)    79936       ['max_pooling1d_6[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " layer_normalization_8 (LayerNo  (None, 1249, 32)    79936       ['max_pooling1d_7[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " layer_normalization_9 (LayerNo  (None, 1249, 16)    39968       ['conv1d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 1249, 32)     0           ['layer_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 1249, 32)     0           ['layer_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 1249, 16)     0           ['layer_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 1249, 32)     0           ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 1249, 32)     0           ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 1249, 16)     0           ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 1249, 80)     0           ['dropout_6[0][0]',              \n",
            "                                                                  'dropout_7[0][0]',              \n",
            "                                                                  'dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 1249, 32)     12832       ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 1249, 32)     7712        ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_11 (MaxPooling1D  (None, 624, 80)     0           ['concatenate_2[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling1d_9 (MaxPooling1D)  (None, 624, 32)     0           ['conv1d_10[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_10 (MaxPooling1D  (None, 624, 32)     0           ['conv1d_11[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 624, 16)      1296        ['max_pooling1d_11[0][0]']       \n",
            "                                                                                                  \n",
            " layer_normalization_10 (LayerN  (None, 624, 32)     39936       ['max_pooling1d_9[0][0]']        \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_11 (LayerN  (None, 624, 32)     39936       ['max_pooling1d_10[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_12 (LayerN  (None, 624, 16)     19968       ['conv1d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 624, 32)      0           ['layer_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 624, 32)      0           ['layer_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 624, 16)      0           ['layer_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 624, 32)      0           ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 624, 32)      0           ['activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 624, 16)      0           ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 624, 80)      0           ['dropout_9[0][0]',              \n",
            "                                                                  'dropout_10[0][0]',             \n",
            "                                                                  'dropout_11[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 624, 32)      12832       ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 624, 32)      7712        ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_14 (MaxPooling1D  (None, 312, 80)     0           ['concatenate_3[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling1d_12 (MaxPooling1D  (None, 312, 32)     0           ['conv1d_13[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling1d_13 (MaxPooling1D  (None, 312, 32)     0           ['conv1d_14[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 312, 16)      1296        ['max_pooling1d_14[0][0]']       \n",
            "                                                                                                  \n",
            " layer_normalization_13 (LayerN  (None, 312, 32)     19968       ['max_pooling1d_12[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_14 (LayerN  (None, 312, 32)     19968       ['max_pooling1d_13[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_15 (LayerN  (None, 312, 16)     9984        ['conv1d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 312, 32)      0           ['layer_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 312, 32)      0           ['layer_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 312, 16)      0           ['layer_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 312, 32)      0           ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 312, 32)      0           ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)           (None, 312, 16)      0           ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 312, 80)      0           ['dropout_12[0][0]',             \n",
            "                                                                  'dropout_13[0][0]',             \n",
            "                                                                  'dropout_14[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)             (None, 312, 32)      12832       ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_17 (Conv1D)             (None, 312, 32)      7712        ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_17 (MaxPooling1D  (None, 156, 80)     0           ['concatenate_4[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling1d_15 (MaxPooling1D  (None, 156, 32)     0           ['conv1d_16[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling1d_16 (MaxPooling1D  (None, 156, 32)     0           ['conv1d_17[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv1d_18 (Conv1D)             (None, 156, 16)      1296        ['max_pooling1d_17[0][0]']       \n",
            "                                                                                                  \n",
            " layer_normalization_16 (LayerN  (None, 156, 32)     9984        ['max_pooling1d_15[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_17 (LayerN  (None, 156, 32)     9984        ['max_pooling1d_16[0][0]']       \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " layer_normalization_18 (LayerN  (None, 156, 16)     4992        ['conv1d_18[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 156, 32)      0           ['layer_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 156, 32)      0           ['layer_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 156, 16)      0           ['layer_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 156, 32)      0           ['activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 156, 32)      0           ['activation_17[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)           (None, 156, 16)      0           ['activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 156, 80)      0           ['dropout_15[0][0]',             \n",
            "                                                                  'dropout_16[0][0]',             \n",
            "                                                                  'dropout_17[0][0]']             \n",
            "                                                                                                  \n",
            " gru (GRU)                      (None, 80)           38880       ['concatenate_5[0][0]']          \n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)           (None, 80)           0           ['gru[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 80)          320         ['dropout_18[0][0]']             \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 80)           0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 96)           7776        ['activation_19[0][0]']          \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 96)           0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            97          ['activation_20[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,029,313\n",
            "Trainable params: 3,029,153\n",
            "Non-trainable params: 160\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "hUAbPQFOV3mD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a482818-8493-4411-ae8c-86610c03cfd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "import datetime"
      ],
      "metadata": {
        "id": "aIP6jd9lV5o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# So my GPU-memory is somewhat unstable and prone to error for batchsize bigger than 16, yet\n",
        "# batch size 16 works fine if I restart everything for every model.fit"
      ],
      "metadata": {
        "id": "mo9tczmEapVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "my_saluki.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.98, clipnorm=0.5),\n",
        "              loss=\"mse\")\n",
        "\n",
        "logdir = os.path.join(os.path.join(os.getcwd(), \"logs\"), datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1, write_graph=True)\n",
        "\n",
        "# Train the model\n",
        "history = my_saluki.fit(X_train, y_train,\n",
        "                        #X_train[0:200,:], y_train[0:200], \n",
        "                        validation_data=(X_val, y_val),  \n",
        "                        #validation_data=(X_val[0:100,:], y_val[0:100]),\n",
        "                        callbacks=[EarlyStopping(patience=25, restore_best_weights=True),   \n",
        "                                   History(),\n",
        "                                   ExplainedVariance(validation_data=(X_val, y_val), interval=1),\n",
        "                                   tensorboard_callback],\n",
        "                        batch_size=16,  #they used 64\n",
        "                        shuffle = True,\n",
        "                        epochs=1000)  "
      ],
      "metadata": {
        "id": "t13N24J-V8eq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6a3d0b3-d35e-464d-ba46-9232d206cf39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "  6/549 [..............................] - ETA: 28s - loss: 2.9984WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0414s vs `on_train_batch_end` time: 0.0433s). Check your callbacks.\n",
            "549/549 [==============================] - ETA: 0s - loss: 1.7231interval evaluation - epoch: 0 - explained variance: 0.063361\n",
            "549/549 [==============================] - 50s 70ms/step - loss: 1.7231 - val_loss: 1.2111\n",
            "Epoch 2/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 1.1537interval evaluation - epoch: 1 - explained variance: 0.102921\n",
            "549/549 [==============================] - 38s 69ms/step - loss: 1.1537 - val_loss: 0.9325\n",
            "Epoch 3/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 1.0189interval evaluation - epoch: 2 - explained variance: 0.132715\n",
            "549/549 [==============================] - 41s 74ms/step - loss: 1.0189 - val_loss: 0.9977\n",
            "Epoch 4/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.8626interval evaluation - epoch: 3 - explained variance: 0.316762\n",
            "549/549 [==============================] - 52s 94ms/step - loss: 0.8626 - val_loss: 0.7176\n",
            "Epoch 5/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.7492interval evaluation - epoch: 4 - explained variance: 0.327840\n",
            "549/549 [==============================] - 58s 106ms/step - loss: 0.7492 - val_loss: 0.8348\n",
            "Epoch 6/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.7120interval evaluation - epoch: 5 - explained variance: 0.274929\n",
            "549/549 [==============================] - 62s 113ms/step - loss: 0.7120 - val_loss: 0.7073\n",
            "Epoch 7/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.6899interval evaluation - epoch: 6 - explained variance: 0.268774\n",
            "549/549 [==============================] - 65s 119ms/step - loss: 0.6899 - val_loss: 1.1816\n",
            "Epoch 8/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.6758interval evaluation - epoch: 7 - explained variance: 0.238664\n",
            "549/549 [==============================] - 63s 115ms/step - loss: 0.6758 - val_loss: 1.3284\n",
            "Epoch 9/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.6594interval evaluation - epoch: 8 - explained variance: 0.277970\n",
            "549/549 [==============================] - 70s 127ms/step - loss: 0.6594 - val_loss: 1.1933\n",
            "Epoch 10/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.6297interval evaluation - epoch: 9 - explained variance: 0.288219\n",
            "549/549 [==============================] - 70s 128ms/step - loss: 0.6297 - val_loss: 0.7225\n",
            "Epoch 11/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.6308interval evaluation - epoch: 10 - explained variance: 0.349080\n",
            "549/549 [==============================] - 66s 121ms/step - loss: 0.6308 - val_loss: 2.2107\n",
            "Epoch 12/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.6052interval evaluation - epoch: 11 - explained variance: 0.349174\n",
            "549/549 [==============================] - 72s 132ms/step - loss: 0.6052 - val_loss: 0.6740\n",
            "Epoch 13/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.5753interval evaluation - epoch: 12 - explained variance: 0.302678\n",
            "549/549 [==============================] - 64s 117ms/step - loss: 0.5753 - val_loss: 0.7231\n",
            "Epoch 14/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.5439interval evaluation - epoch: 13 - explained variance: 0.294356\n",
            "549/549 [==============================] - 70s 128ms/step - loss: 0.5439 - val_loss: 0.8513\n",
            "Epoch 15/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.5195interval evaluation - epoch: 14 - explained variance: 0.299801\n",
            "549/549 [==============================] - 62s 113ms/step - loss: 0.5195 - val_loss: 0.8619\n",
            "Epoch 16/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.4963interval evaluation - epoch: 15 - explained variance: 0.293112\n",
            "549/549 [==============================] - 60s 110ms/step - loss: 0.4963 - val_loss: 1.0564\n",
            "Epoch 17/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.4827interval evaluation - epoch: 16 - explained variance: 0.319386\n",
            "549/549 [==============================] - 65s 119ms/step - loss: 0.4827 - val_loss: 0.6762\n",
            "Epoch 18/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.4574interval evaluation - epoch: 17 - explained variance: 0.237439\n",
            "549/549 [==============================] - 63s 116ms/step - loss: 0.4574 - val_loss: 1.7397\n",
            "Epoch 19/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.4378interval evaluation - epoch: 18 - explained variance: 0.263975\n",
            "549/549 [==============================] - 67s 122ms/step - loss: 0.4378 - val_loss: 0.7813\n",
            "Epoch 20/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.4149interval evaluation - epoch: 19 - explained variance: 0.237705\n",
            "549/549 [==============================] - 60s 109ms/step - loss: 0.4149 - val_loss: 0.7621\n",
            "Epoch 21/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.3981interval evaluation - epoch: 20 - explained variance: 0.295770\n",
            "549/549 [==============================] - 62s 113ms/step - loss: 0.3981 - val_loss: 0.8710\n",
            "Epoch 22/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.3837interval evaluation - epoch: 21 - explained variance: 0.239566\n",
            "549/549 [==============================] - 64s 116ms/step - loss: 0.3837 - val_loss: 0.8230\n",
            "Epoch 23/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.3606interval evaluation - epoch: 22 - explained variance: 0.245154\n",
            "549/549 [==============================] - 60s 110ms/step - loss: 0.3606 - val_loss: 0.8209\n",
            "Epoch 24/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.3575interval evaluation - epoch: 23 - explained variance: 0.263633\n",
            "549/549 [==============================] - 67s 122ms/step - loss: 0.3575 - val_loss: 1.1135\n",
            "Epoch 25/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.3336interval evaluation - epoch: 24 - explained variance: 0.161096\n",
            "549/549 [==============================] - 63s 115ms/step - loss: 0.3336 - val_loss: 1.0538\n",
            "Epoch 26/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.3218interval evaluation - epoch: 25 - explained variance: 0.199514\n",
            "549/549 [==============================] - 65s 118ms/step - loss: 0.3218 - val_loss: 0.8923\n",
            "Epoch 27/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.3111interval evaluation - epoch: 26 - explained variance: 0.230071\n",
            "549/549 [==============================] - 61s 112ms/step - loss: 0.3111 - val_loss: 1.1603\n",
            "Epoch 28/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.2998interval evaluation - epoch: 27 - explained variance: 0.221049\n",
            "549/549 [==============================] - 62s 112ms/step - loss: 0.2998 - val_loss: 1.5330\n",
            "Epoch 29/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.2897interval evaluation - epoch: 28 - explained variance: 0.252749\n",
            "549/549 [==============================] - 68s 124ms/step - loss: 0.2897 - val_loss: 0.9001\n",
            "Epoch 30/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.2846interval evaluation - epoch: 29 - explained variance: 0.218013\n",
            "549/549 [==============================] - 61s 111ms/step - loss: 0.2846 - val_loss: 0.9189\n",
            "Epoch 31/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.2747interval evaluation - epoch: 30 - explained variance: 0.251460\n",
            "549/549 [==============================] - 65s 118ms/step - loss: 0.2747 - val_loss: 0.8481\n",
            "Epoch 32/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.2615interval evaluation - epoch: 31 - explained variance: 0.192196\n",
            "549/549 [==============================] - 60s 110ms/step - loss: 0.2615 - val_loss: 0.9613\n",
            "Epoch 33/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.2588interval evaluation - epoch: 32 - explained variance: 0.236275\n",
            "549/549 [==============================] - 67s 123ms/step - loss: 0.2588 - val_loss: 0.7598\n",
            "Epoch 34/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.2521interval evaluation - epoch: 33 - explained variance: 0.242017\n",
            "549/549 [==============================] - 61s 112ms/step - loss: 0.2521 - val_loss: 0.8495\n",
            "Epoch 35/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.2495interval evaluation - epoch: 34 - explained variance: 0.233613\n",
            "549/549 [==============================] - 68s 123ms/step - loss: 0.2495 - val_loss: 0.7422\n",
            "Epoch 36/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.2444interval evaluation - epoch: 35 - explained variance: 0.050026\n",
            "549/549 [==============================] - 64s 116ms/step - loss: 0.2444 - val_loss: 1.1165\n",
            "Epoch 37/1000\n",
            "549/549 [==============================] - ETA: 0s - loss: 0.2336interval evaluation - epoch: 36 - explained variance: 0.349174\n",
            "549/549 [==============================] - 68s 124ms/step - loss: 0.2336 - val_loss: 0.7326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = my_saluki.predict(X_val)\n",
        "print(\"Explained Var Score: %.2f\" % explained_variance_score(y_val, y_pred))\n",
        "print(\"R2 Score: %.2f\" % r2_score(y_val, y_pred))"
      ],
      "metadata": {
        "id": "7hCHlQnlWPVn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc8dab79-b688-4aaa-ba95-fe365f2dbd5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "67/67 [==============================] - 3s 46ms/step\n",
            "Explained Var Score: 0.35\n",
            "R2 Score: 0.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_saluki.save('my_saluki5_24.h5')"
      ],
      "metadata": {
        "id": "RDHswe1X_W34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_loss(history):\n",
        "    fig, ax = plt.subplots(figsize = (5,5))\n",
        "    ax.plot(history['loss'][1:])\n",
        "    ax.plot(history['val_loss'][1:])\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('mean squared error')"
      ],
      "metadata": {
        "id": "5_Wi1QFKWEVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "62gaB6rwHDyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss(history.history)"
      ],
      "metadata": {
        "id": "KDZw8D75goko",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "outputId": "bede6f54-afe0-401d-f418-5190658d54f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplot_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m\n",
            "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36mplot_loss\u001b[1;34m(history)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_loss\u001b[39m(history):\n\u001b[0;32m      3\u001b[0m     fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m----> 4\u001b[0m     ax\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m      5\u001b[0m     ax\u001b[38;5;241m.\u001b[39mplot(history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'loss'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUQAAAEzCAYAAABJzXq/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAANjElEQVR4nO3bf6jdd33H8eerzTpZV+uwV5AmtZWlq5kO7C6lQ5gdupF2kPzhJgmUzVEadFYGyqDD0Un9y8kcCNlcYFIVbI3+MS4YKehaCmJqb6nWJqVyjW69Vdao1X9Ea9l7f5y38/Sa9H6TfM85venzAYHz/Z7PPef96UmfOT/uSVUhSYILFj2AJL1YGERJagZRkppBlKRmECWpGURJapsGMcnHkzyd5LHTXJ8kH02yluTRJNeOP6Ykzd6QZ4h3Abtf4PobgZ395wDwr+c+liTN36ZBrKoHgB++wJK9wCdr4ijwiiSvHmtASZqXMd5DvBx4cup4vc9J0paybZ53luQAk5fVXHzxxb9/zTXXzPPuJb0EPPzww9+vqqWz+dkxgvgUsGPqeHuf+xVVdQg4BLC8vFyrq6sj3L0k/VKS/zrbnx3jJfMK8Bf9afP1wI+r6nsj3K4kzdWmzxCT3A3cAFyWZB34B+DXAKrqY8AR4CZgDfgJ8FezGlaSZmnTIFbV/k2uL+Ddo00kSQviN1UkqRlESWoGUZKaQZSkZhAlqRlESWoGUZKaQZSkZhAlqRlESWoGUZKaQZSkZhAlqRlESWoGUZKaQZSkZhAlqRlESWoGUZKaQZSkZhAlqRlESWoGUZKaQZSkZhAlqRlESWoGUZKaQZSkZhAlqRlESWoGUZKaQZSkZhAlqRlESWoGUZKaQZSkZhAlqRlESWoGUZKaQZSkZhAlqRlESWoGUZKaQZSkZhAlqRlESWqDgphkd5Inkqwluf0U11+R5L4kjyR5NMlN448qSbO1aRCTXAgcBG4EdgH7k+zasOzvgcNV9UZgH/AvYw8qSbM25BnidcBaVZ2oqmeBe4C9G9YU8PK+fCnw3fFGlKT5GBLEy4Enp47X+9y0DwA3J1kHjgDvOdUNJTmQZDXJ6smTJ89iXEmanbE+VNkP3FVV24GbgE8l+ZXbrqpDVbVcVctLS0sj3bUkjWNIEJ8Cdkwdb+9z024BDgNU1VeAlwGXjTGgJM3LkCA+BOxMclWSi5h8aLKyYc1/A28BSPI6JkH0NbGkLWXTIFbVc8BtwL3A40w+TT6W5M4ke3rZ+4Bbk3wduBt4R1XVrIaWpFnYNmRRVR1h8mHJ9Lk7pi4fB9407miSNF9+U0WSmkGUpGYQJakZRElqBlGSmkGUpGYQJakZRElqBlGSmkGUpGYQJakZRElqBlGSmkGUpGYQJakZRElqBlGSmkGUpGYQJakZRElqBlGSmkGUpGYQJakZRElqBlGSmkGUpGYQJakZRElqBlGSmkGUpGYQJakZRElqBlGSmkGUpGYQJakZRElqBlGSmkGUpGYQJakZRElqBlGSmkGUpGYQJakZRElqBlGS2qAgJtmd5Ikka0luP82atyc5nuRYkk+PO6Ykzd62zRYkuRA4CPwxsA48lGSlqo5PrdkJ/B3wpqp6JsmrZjWwJM3KkGeI1wFrVXWiqp4F7gH2blhzK3Cwqp4BqKqnxx1TkmZvSBAvB56cOl7vc9OuBq5O8uUkR5PsHmtASZqXTV8yn8Ht7ARuALYDDyR5Q1X9aHpRkgPAAYArrrhipLuWpHEMeYb4FLBj6nh7n5u2DqxU1c+r6tvAN5kE8nmq6lBVLVfV8tLS0tnOLEkzMSSIDwE7k1yV5CJgH7CyYc1/MHl2SJLLmLyEPjHemJI0e5sGsaqeA24D7gUeBw5X1bEkdybZ08vuBX6Q5DhwH/C3VfWDWQ0tSbOQqlrIHS8vL9fq6upC7lvS+SvJw1W1fDY/6zdVJKkZRElqBlGSmkGUpGYQJakZRElqBlGSmkGUpGYQJakZRElqBlGSmkGUpGYQJakZRElqBlGSmkGUpGYQJakZRElqBlGSmkGUpGYQJakZRElqBlGSmkGUpGYQJakZRElqBlGSmkGUpGYQJakZRElqBlGSmkGUpGYQJakZRElqBlGSmkGUpGYQJakZRElqBlGSmkGUpGYQJakZRElqBlGSmkGUpGYQJakNCmKS3UmeSLKW5PYXWPe2JJVkebwRJWk+Ng1ikguBg8CNwC5gf5Jdp1h3CfA3wINjDylJ8zDkGeJ1wFpVnaiqZ4F7gL2nWPdB4EPAT0ecT5LmZkgQLweenDpe73P/L8m1wI6q+vyIs0nSXJ3zhypJLgA+ArxvwNoDSVaTrJ48efJc71qSRjUkiE8BO6aOt/e5X7gEeD1wf5LvANcDK6f6YKWqDlXVclUtLy0tnf3UkjQDQ4L4ELAzyVVJLgL2ASu/uLKqflxVl1XVlVV1JXAU2FNVqzOZWJJmZNMgVtVzwG3AvcDjwOGqOpbkziR7Zj2gJM3LtiGLquoIcGTDuTtOs/aGcx9LkubPb6pIUjOIktQMoiQ1gyhJzSBKUjOIktQMoiQ1gyhJzSBKUjOIktQMoiQ1gyhJzSBKUjOIktQMoiQ1gyhJzSBKUjOIktQMoiQ1gyhJzSBKUjOIktQMoiQ1gyhJzSBKUjOIktQMoiQ1gyhJzSBKUjOIktQMoiQ1gyhJzSBKUjOIktQMoiQ1gyhJzSBKUjOIktQMoiQ1gyhJzSBKUjOIktQMoiQ1gyhJzSBKUjOIktQGBTHJ7iRPJFlLcvsprn9vkuNJHk3ypSSvGX9USZqtTYOY5ELgIHAjsAvYn2TXhmWPAMtV9XvA54B/HHtQSZq1Ic8QrwPWqupEVT0L3APsnV5QVfdV1U/68CiwfdwxJWn2hgTxcuDJqeP1Pnc6twBfONUVSQ4kWU2yevLkyeFTStIcjPqhSpKbgWXgw6e6vqoOVdVyVS0vLS2NedeSdM62DVjzFLBj6nh7n3ueJG8F3g+8uap+Ns54kjQ/Q54hPgTsTHJVkouAfcDK9IIkbwT+DdhTVU+PP6Ykzd6mQayq54DbgHuBx4HDVXUsyZ1J9vSyDwO/CXw2ydeSrJzm5iTpRWvIS2aq6ghwZMO5O6Yuv3XkuSRp7vymiiQ1gyhJzSBKUjOIktQMoiQ1gyhJzSBKUjOIktQMoiQ1gyhJzSBKUjOIktQMoiQ1gyhJzSBKUjOIktQMoiQ1gyhJzSBKUjOIktQMoiQ1gyhJzSBKUjOIktQMoiQ1gyhJzSBKUjOIktQMoiQ1gyhJzSBKUjOIktQMoiQ1gyhJzSBKUjOIktQMoiQ1gyhJzSBKUjOIktQMoiQ1gyhJzSBKUjOIktQMoiS1QUFMsjvJE0nWktx+iut/Pcln+voHk1w5+qSSNGObBjHJhcBB4EZgF7A/ya4Ny24Bnqmq3wb+GfjQ2INK0qwNeYZ4HbBWVSeq6lngHmDvhjV7gU/05c8Bb0mS8caUpNkbEsTLgSenjtf73CnXVNVzwI+BV44xoCTNy7Z53lmSA8CBPvxZksfmef9zdhnw/UUPMUPn8/7O573B+b+/3znbHxwSxKeAHVPH2/vcqdasJ9kGXAr8YOMNVdUh4BBAktWqWj6bobcC97d1nc97g5fG/s72Z4e8ZH4I2JnkqiQXAfuAlQ1rVoC/7Mt/BvxnVdXZDiVJi7DpM8Sqei7JbcC9wIXAx6vqWJI7gdWqWgH+HfhUkjXgh0yiKUlbyqD3EKvqCHBkw7k7pi7/FPjzM7zvQ2e4fqtxf1vX+bw3cH+nFV/ZStKEX92TpDbzIJ7vX/sbsL/3Jjme5NEkX0rymkXMeTY229vUurclqSRb6pPLIftL8vZ+/I4l+fS8ZzwXA/5uXpHkviSP9N/PmxYx59lI8vEkT5/uV/cy8dHe+6NJrh10w1U1sz9MPoT5FvBa4CLg68CuDWv+GvhYX94HfGaWMy1gf38E/EZfftdW2d+QvfW6S4AHgKPA8qLnHvmx2wk8AvxWH79q0XOPvL9DwLv68i7gO4ue+wz294fAtcBjp7n+JuALQIDrgQeH3O6snyGe71/723R/VXVfVf2kD48y+T3OrWDIYwfwQSbfXf/pPIcbwZD93QocrKpnAKrq6TnPeC6G7K+Al/flS4HvznG+c1JVDzD5jZbT2Qt8siaOAq9I8urNbnfWQTzfv/Y3ZH/TbmHyr9ZWsOne+mXIjqr6/DwHG8mQx+5q4OokX05yNMnuuU137obs7wPAzUnWmfwWyXvmM9pcnOn/m8Ccv7r3UpbkZmAZePOiZxlDkguAjwDvWPAos7SNycvmG5g8s38gyRuq6keLHGpE+4G7quqfkvwBk98lfn1V/e+iB1uUWT9DPJOv/fFCX/t7kRqyP5K8FXg/sKeqfjan2c7VZnu7BHg9cH+S7zB5n2ZlC32wMuSxWwdWqurnVfVt4JtMArkVDNnfLcBhgKr6CvAyJt9zPh8M+n/zV8z4jc9twAngKn75xu7vbljzbp7/ocrhRb9hO/L+3sjkze2di5537L1tWH8/W+tDlSGP3W7gE335MiYvwV656NlH3N8XgHf05dcxeQ8xi579DPZ4Jaf/UOVPef6HKl8ddJtzGPomJv+yfgt4f5+7k8mzJZj8q/RZYA34KvDaRf+HHnl/XwT+B/ha/1lZ9Mxj7W3D2i0VxIGPXZi8LXAc+Aawb9Ezj7y/XcCXO5ZfA/5k0TOfwd7uBr4H/JzJM/lbgHcC75x67A723r8x9O+m31SRpOY3VSSpGURJagZRkppBlKRmECWpGURJagZRkppBlKT2f9OeiaxfHSuTAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "A5VzLjuOWfnD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}